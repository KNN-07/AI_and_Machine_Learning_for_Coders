{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOVi0/7s96hDpVaGHGPet+6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/Chapter12_An_Introduction_to_TensorFlow_Lite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 12: An Introduction to Tensorflow Lite"
      ],
      "metadata": {
        "id": "95fWfSPaUU4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![overvire](https://static1.makeuseofimages.com/wordpress/wp-content/uploads/2021/05/General-Work-Flow-for-TensorFlow-Lite-Micro.jpg)"
      ],
      "metadata": {
        "id": "KmJ3SNNhURgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hi, chào mọi người, tụi mình lại gặp nhau rồi. 😊\n",
        "\n",
        "Nếu như ở các chương trước tụi mình đã học về cách xây dựng, huấn luyện mô hình thì ở chương này cũng như các chương sau, tụi mình sẽ tìm hiểu thêm về cách để ứng dụng mô hình vào các ứng dụng thực tế như di động **(iOS, Android)** hay các thiết bị ngoại vi khác như **Raspberry Pi (Linux)** nha. Tụi mình sẽ học cách ứng dụng, nhúng chúng vào các thiết bị, phần mềm để chạy thực tế. 📱💻\n",
        "\n",
        "Công cụ mà tụi mình sẽ sử dụng là **TensorFlow Lite** nha. Workflow cơ bản sẽ là mọi người huấn luyện mô hình **TensorFlow**, sau đó sẽ chuyển nó về định dạng **TensorFlow Lite** và nhúng vào các thiết bị hoặc cung cấp API nha. ⚙️\n",
        "\n",
        "> **Ơ việc chuyển định dạng này mô hình có bị gì không?**\n",
        "\n",
        "Câu trả lời là có nha, chúng sẽ có ảnh hưởng đến hiệu suất, độ chính xác gì đó nhưng sẽ giảm thiểu về mức thấp nhất có thể nha. Từ đó làm cho mô hình có phần nhẹ hơn cũng như hoạt động được realtime tốt hơn nha. 🚀\n",
        "\n",
        "*Lưu ý: mô hình sau khi chuyển sang dạng **TensorFlow Lite** sẽ không thể huấn luyện lại nữa.* ❗"
      ],
      "metadata": {
        "id": "FinbDfDRUaXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TensorFlow Lite là gì? 😊\n",
        "\n",
        "Mọi người có thể hiểu đơn giản nó là một phiên bản của TensorFlow được phát triển cho các nhà lập trình **Android và iOS** nha. 📱\n",
        "\n",
        "Thông thường, khi mọi người huấn luyện và sử dụng mô hình trên các máy tính hay dịch vụ đám mây thì không cần bận tâm nhiều lắm. Tuy nhiên, trên các thiết bị di động hay thiết bị ngoại vi thì lại khác. Chúng gặp vô vàn hạn chế khiến chúng ta phải đắn đo suy nghĩ. Dưới đây là vài sự đánh đổi mà chúng ta cần phải xem xét nha:  \n",
        "\n",
        "1. **Độ \"nặng\"** của mô hình hay framework:  \n",
        "   Tại sao lại nói vậy? Thường thì các thiết bị di động có bộ nhớ và tài nguyên hạn chế hơn rất nhiều so với máy tính. Do đó, ta cần phải tính toán vô cùng kỹ lưỡng về nguồn tài nguyên tiêu thụ của một mô hình hay framework bên trong ứng dụng. Chúng ta cần đảm bảo chúng thật \"nhẹ nhàng\", phù hợp với hệ thống của đại đa số người dùng. Hầu như mọi người khi tải một ứng dụng nào đó đều quan tâm đến khả năng chiếm bộ nhớ và độ \"nặng\" của ứng dụng. 🏋️‍♀️\n",
        "\n",
        "2. **Độ trễ** của mô hình:  \n",
        "   Cái này thì dễ hiểu rồi ha. Một mô hình hay framework có độ trễ thấp sẽ giúp ứng dụng chạy mượt mà hơn rất nhiều. Ngược lại, nếu chúng hoạt động quá lâu, người dùng sẽ phải đợi, làm giảm trải nghiệm và khiến người dùng ít quay lại sử dụng. Theo thống kê, số người chỉ sử dụng ứng dụng một lần chiếm tỷ lệ rất cao, vì vậy chúng ta càng phải tìm cách níu chân người dùng bằng cách giảm thiểu độ trễ thấp nhất có thể. 🚀\n",
        "\n",
        "3. **Định dạng mô hình**:  \n",
        "   Điều này có thể không ảnh hưởng quá lớn, nhưng vẫn đáng xem xét. Khi hoạt động trên máy tính, chúng ta thường cần các phép toán chi tiết bên trong mô hình. Tuy nhiên, khi hoạt động trên di động, chúng ta cần tập trung vào sự thân thiện, tốc độ và tính nhẹ nhàng của mô hình, ứng dụng hoặc framework. Việc thay đổi định dạng để thực hiện điều này là cần thiết, dù nó có thể ảnh hưởng một phần đến hiệu suất. Điều quan trọng là cân nhắc kỹ lưỡng để đạt được sự cân bằng. ⚙️\n",
        "\n",
        "TensorFlow Lite giúp chúng ta vượt qua những thách thức này để xây dựng các ứng dụng AI hiệu quả trên các thiết bị có tài nguyên hạn chế. 🎯"
      ],
      "metadata": {
        "id": "AY7iZrP6Yu8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Vậy ưu điểm của việc chuyển đổi và sử dụng trên thiết bị này là gì?**  \n",
        "\n",
        "Điều đơn giản mà chúng ta có thể thấy chính là khả năng **bảo mật dữ liệu riêng tư** của người dùng. Mọi người sẽ không còn cần phải tải dữ liệu lên mạng nữa. 🌐🚫 Đồng thời, mọi người có thể sử dụng linh hoạt **offline**, không cần bật dữ liệu di động hay kết nối Wi-Fi nữa. 📶❌  \n",
        "\n",
        "Với tất cả những tiêu chí và mục tiêu sử dụng cho các thiết bị di động, ngoại vi như trên, **TensorFlow Lite** đã được tạo ra. 🎯"
      ],
      "metadata": {
        "id": "4BSbf8ZJ2ZNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Về quy trình tổng quát, sẽ có 2 công việc chính:  \n",
        "\n",
        "1. **Bộ chuyển đổi** sẽ tiếp nhận mô hình TensorFlow, sau đó tiến hành chuyển nó sang định dạng **TFLite**, thu nhỏ và tối ưu hóa mô hình.  \n",
        "2. Kết hợp với một **bộ thông dịch** (interpreter) để giúp mô hình hoạt động hiệu quả trên các môi trường thực thi. 🛠️"
      ],
      "metadata": {
        "id": "Uy8TcqsQ4BFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![flow_convert](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_12/flow_convert.png?raw=true)"
      ],
      "metadata": {
        "id": "NZN2bS3FCVH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Lưu ý:* Không phải mọi thao tác trong TensorFlow đều được hỗ trợ trong **TensorFlow Lite**. Đôi khi, việc chuyển đổi mô hình có thể gặp một vài vấn đề. Bạn có thể kiểm tra thông tin chi tiết trên tài liệu nha. 📚  \n",
        "\n",
        "Ở các phần sau, tụi mình sẽ tìm hiểu thêm về các mô hình **thân thiện với thiết bị di động** và áp dụng phương pháp **học chuyển tiếp** để giải quyết một số vấn đề. Mọi người cũng có thể tham khảo các mô hình đã được tối ưu hóa sẵn để làm việc với **TensorFlow Lite** trên trang **TensorFlow** hoặc **TensorFlow Hub** nha. 🌐"
      ],
      "metadata": {
        "id": "QVqkeUHT3_WZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Walkthrough: Creating and Converting a Model to TensorFlow Lite  \n",
        "\n",
        "Tụi mình sẽ thực hành từng bước một để chuyển đổi mô hình sang định dạng **TensorFlow Lite** nha. Sau đó, tụi mình sẽ sử dụng mô hình đó trên trình thông dịch của **Linux**, đơn giản vì nó có sẵn trên Colab, và nền server của Colab chạy trên **Linux** nha. 🖥️  \n",
        "\n",
        "Ở **chương 13**, tụi mình sẽ tìm hiểu cách chạy mô hình trên **Android**, và ở **chương 14** sẽ là trên **iOS**. 📱  \n",
        "\n",
        "Dưới đây, tụi mình sẽ bắt đầu với một mô hình tuyến tính đơn giản đã làm ở **chương 1** nha. 🚀"
      ],
      "metadata": {
        "id": "FjAWrvuk59pQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cài đặt các thư viện\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time as timer\n",
        "\n",
        "import tf_keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from datetime import timedelta"
      ],
      "metadata": {
        "id": "BXZVeMZu68bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vigHOMV1RpG7"
      },
      "outputs": [],
      "source": [
        "# Định nghĩa lớp tuyến tính\n",
        "linear = Dense(units=1, input_shape=[1])\n",
        "\n",
        "# Tạo mô hình\n",
        "model = Sequential([linear])\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "# Khởi tạo dữ liệu cho mô hình\n",
        "xs = np.arange(-1, 5, 1, dtype='float')\n",
        "ys = np.arange(-3, 9, 2, dtype='float')\n",
        "\n",
        "# Huấn luyện mô hình\n",
        "model.fit(xs, ys, epochs=500, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In ra kết quả dự đoán\n",
        "print(model.predict(np.array([10.0]))) # Dự đoán với x = 10\n",
        "print(f\"Trọng số của lớp tuyến tính: {linear.get_weights()}\")"
      ],
      "metadata": {
        "id": "xDV7iC1B83M8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kết quả ở lần chạy của mình:\n",
        "> [[18.986002]]   \n",
        "Trọng số của lớp tuyến tính: [array([[1.9979712]], dtype=float32), array([-0.9937101], dtype=float32)]"
      ],
      "metadata": {
        "id": "sR_PHOqv8cxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Save model.\n",
        "\n",
        "Bước đầu tiên tụi mình sẽ tiến hành lưu mô hình lại trước nha."
      ],
      "metadata": {
        "id": "Ur4hC7Np9j4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export_dir = 'saved_model/1' # Chỉ định thư mục lưu mô hình lại\n",
        "tf.saved_model.save(model, export_dir)"
      ],
      "metadata": {
        "id": "Gt7f4wO392cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Như mọi người thấy, thư mục lưu mô hình sẽ được hiện ra như sau."
      ],
      "metadata": {
        "id": "xQJtF5tw-Ej5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![tree_folder](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_12/tree_folder.png?raw=true)"
      ],
      "metadata": {
        "id": "atX0ZwH7CaRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Lưu ý:* Sau khi thực hiện lưu mô hình theo cách trên và tiến hành chuyển đổi sang **TFLite**, mình nhận thấy server Colab tự động bị lỗi và khởi động lại. 🚨  \n",
        "\n",
        "Do đó, mình đề xuất một cách khác là lưu mô hình dưới định dạng **.h5** để quá trình chuyển đổi không bị lỗi nha. ✅"
      ],
      "metadata": {
        "id": "8p2cufSlMxN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = 'saved_model/model_1.h5'\n",
        "model.save(model_path)"
      ],
      "metadata": {
        "id": "Q-JYly-3NSQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Convert and Save the model.\n",
        "\n",
        "Đây là đoạn code để tụi mình chuyển đổi mô hình sang định dạng **.tflite** từ thư mục đã lưu trước đó nha.\n",
        "\n",
        "\n",
        "```python\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "tflite_model = converter.convert()\n",
        "```\n",
        "\n",
        " Tuy nhiên như mình nói trước nó sẽ bị lỗi nên mình sẽ không chạy mà thay vào đó sử dụng một đoạn code khác chuyển đổi mô hình từ **file.h5** ha."
      ],
      "metadata": {
        "id": "rW8kwdKD-JsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load lại mô hình từ file.h5\n",
        "keras_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Tạo converter\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
        "\n",
        "# Tùy chọn: Bật tối ưu hóa để giảm kích thước mô hình\n",
        "# converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Chuyển đổi mô hình\n",
        "tflite_model = converter.convert()"
      ],
      "metadata": {
        "id": "FY96umGlKLRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sau khi tiến hành chuyển đổi xong, tụi mình sẽ lưu lại mô hình định dạng tflite đó nha."
      ],
      "metadata": {
        "id": "bbAWezyqOUqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "tflite_model_path = 'model.tflite'\n",
        "tflite_model_file = pathlib.Path(tflite_model_path)\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "metadata": {
        "id": "jYjUMtjZOf4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vậy là tụi mình đã có được một file mô hình định dạng **.tflite** để có thể sử dụng trong bất kỳ môi trường thông dịch nào rồi ha. 🎉  \n",
        "\n",
        "Sau này, tụi mình cũng có thể sử dụng nó cho **Android** và **iOS** nha. 📱  \n",
        "\n",
        "Còn bây giờ, tụi mình sẽ chạy thử với **trình thông dịch Python cơ bản** trên Colab trước. Cách này tương tự với trình thông dịch được sử dụng trong môi trường nhúng **Linux**, chẳng hạn như trên **Raspberry Pi**. 🐧"
      ],
      "metadata": {
        "id": "KtkY-_4dO8lj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Load the TFLite Model and Allocate Tensors  \n",
        "\n",
        "Ở bước này, tụi mình sẽ:  \n",
        "1. **Tải mô hình** vào trình thông dịch.  \n",
        "2. **Cấp phát các tensor**, đại diện cho dữ liệu đầu vào của mô hình để thực hiện dự đoán.  \n",
        "3. **Đọc và xử lý kết quả đầu ra.**  \n",
        "\n",
        "Khác với TensorFlow thông thường, bạn chỉ cần gọi `model.predict(x)` để dự đoán. Với TensorFlow Lite, bạn cần làm việc ở mức **low-level** hơn, cụ thể là:  \n",
        "- Xử lý các tensor đầu vào và đầu ra.  \n",
        "- Định dạng dữ liệu phù hợp.  \n",
        "- Phân tích dữ liệu đầu ra để tương thích với thiết bị.  \n",
        "\n",
        "Điều này tuy có vẻ hơi **rườm rà**, nhưng nó giúp tối ưu hóa hiệu suất và phù hợp hơn với các thiết bị hạn chế tài nguyên. 🚀"
      ],
      "metadata": {
        "id": "MKPIXVoxPmlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path) # Nạp mô hình vào\n",
        "# Mọi người cũng có thể load thẳng model tflite với đoạn code dưới đây thay vì file nha.\n",
        "# interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors() # Cấp phát bộ nhớ cho các tensor đầu vào và ra"
      ],
      "metadata": {
        "id": "H-iuO-mAKsjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tụi mình sẽ tiến hành in ra thông tin chi tiết đầu vào và đầu ra của mô hình để hiểu thêm về định dạng yêu cầu cũng như trả về nha."
      ],
      "metadata": {
        "id": "JMGvzhA-SMpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()"
      ],
      "metadata": {
        "id": "-0UpDcbmK35M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_details)\n",
        "print(output_details)"
      ],
      "metadata": {
        "id": "q4CVTAW4K6pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nhìn vào kết quả in ra:  \n",
        "\n",
        "```python\n",
        "[{'name': 'serving_default_input_layer:0', 'index': 0, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([-1,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
        "```  \n",
        "\n",
        "Chúng ta có thể rút ra các thông tin quan trọng về định dạng dữ liệu đầu vào:  \n",
        "- **`'shape': array([1, 1])`**: Đầu vào yêu cầu là một mảng (hay ma trận) **2 chiều**, với kích thước **1x1** (một hàng, một cột).  \n",
        "- Điều này nghĩa là nếu giá trị đầu vào là `x = 10`, thì nó phải được định dạng thành tensor: `[[10.0]]`.  \n",
        "- **Kiểu dữ liệu (`'dtype'`)**: Dữ liệu đầu vào cần có kiểu **`float32`**.  \n",
        "\n",
        "Nhờ thông tin này, tụi mình biết cần chuẩn bị dữ liệu đầu vào sao cho đúng định dạng để mô hình có thể thực hiện dự đoán chính xác. 🚀"
      ],
      "metadata": {
        "id": "lMvpz3kZSbgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_predict = np.array([[10.0]], dtype=np.float32)\n",
        "print(to_predict)"
      ],
      "metadata": {
        "id": "PkSBN7pTLUMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Perform the Prediction  \n",
        "Bây giờ tụi mình tiến hành dự đoán nha. Đầu tiên là chỉ định tensor đầu vào. Mọi người lưu ý, như lúc nãy mình kiểm tra thì chi tiết đầu vào là một mảng với chỉ một phần tử do đó tụi mình thiết lập tensor với tham số `input_details[0]['index']` nha.  \n",
        "\n",
        "Vì ở đây mô hình của tụi mình đơn giản, chỉ có một tùy chọn đầu vào duy nhất, trong một số trường hợp số lượng có lẽ sẽ nhiều hơn nha. 🌟🤖💡  "
      ],
      "metadata": {
        "id": "PWDF-249UBK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter.set_tensor(input_details[0]['index'], to_predict)"
      ],
      "metadata": {
        "id": "C06DlWJJLtou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiến hành dự đoán và lấy tensor kết quả đầu ra."
      ],
      "metadata": {
        "id": "7nJX9i1VXHhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter.invoke() # Tiến hành dự đoán dựa trên dữ liệu đầu vào\n",
        "tflite_results = interpreter.get_tensor(output_details[0]['index']) # Lấy kết quả ra.\n",
        "print(tflite_results)"
      ],
      "metadata": {
        "id": "abw71m4kXEcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Như mọi người thấy, vậy là chúng ta đã có thể dự đoán và tiến hành lấy kết quả ra được rồi.  \n",
        "\n",
        "Đây là kết quả ở lần chạy của mình:  \n",
        "> [[18.986002]]  \n",
        "\n",
        "Vậy là tụi mình đã xong một ví dụ đơn giản về việc chuyển đổi và sử dụng mô hình rồi. Bây giờ tụi mình sẽ đi vào với các ví dụ phức tạp hơn, khi mà sử dụng học chuyển giao (transfer learning) dựa trên một mô hình phân loại hình ảnh có sẵn. Sau đó tiến hành chuyển đổi qua định dạng **TensorFlow Lite** và sử dụng.  \n",
        "\n",
        "Trong quá trình này, chúng mình có thể khám phá cũng như hiểu thêm được tốt hơn về các tác động đến từ quá trình tối ưu hóa và lượng tử hóa **(quantizing)** mô hình nha. 🚀🧠💻✨  \n"
      ],
      "metadata": {
        "id": "YwuPlxGeF5Rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Walkthrough: Transfer Learning an Image Classifier and Converting to TensorFlow Lite  \n",
        "\n",
        "Tụi mình sẽ tiến hành xây dựng một mô hình phân loại ảnh chó mèo mới dựa trên phương pháp học chuyển giao nha. 🐶🐱✨  \n"
      ],
      "metadata": {
        "id": "b7P2Y10fZcQT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Build and Save the Model  \n",
        "\n",
        "Đầu tiên tụi mình sẽ tiến hành tải dữ liệu về phân loại các tập. 📂🔍📊  \n"
      ],
      "metadata": {
        "id": "u9sJaiHga17x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = (224, 224)\n",
        "\n",
        "def format_image(image, label):\n",
        "  image = tf.image.resize(image, IMAGE_SIZE) / 255.0\n",
        "  return  image, label\n",
        "\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
        "    'cats_vs_dogs',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "\n",
        "num_examples = metadata.splits['train'].num_examples\n",
        "num_classes = metadata.features['label'].num_classes\n",
        "print(f\"Số lượng mẫu: {num_examples}\")\n",
        "print(f\"Số lượng nhãn: {num_classes}\")"
      ],
      "metadata": {
        "id": "0uJyCxHBX3VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cập nhật các tập train, validation, test\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "train_batches = raw_train.shuffle(num_examples // 4).map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "validation_batches = raw_validation.map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "test_batches = raw_test.map(format_image).batch(1)"
      ],
      "metadata": {
        "id": "8nUI1ODUbGOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiếp đến tụi mình sẽ tiến hành học chuyển giao với mô hình **MobileNetV2** nha. Đoạn này mình load mô hình khác với tác giả nha do thấy code khá cũ và có sự xung đột với các phiên bản khác nhau của thư viện. 🛠️📚🤖  \n"
      ],
      "metadata": {
        "id": "CejJ8b59fO1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tải MobileNetV2 với trọng số pre-trained trên ImageNet\n",
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "    input_shape=(224, 224, 3),  # Kích thước đầu vào của ảnh\n",
        "    include_top=False,         # Loại bỏ tầng phân loại đầu ra của MobileNetV2\n",
        "    weights='imagenet'         # Sử dụng trọng số pre-trained trên ImageNet\n",
        ")\n",
        "\n",
        "# Đóng băng các tầng của mô hình gốc (không huấn luyện lại)\n",
        "base_model.trainable = False"
      ],
      "metadata": {
        "id": "J9Djc50Sfg4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_transfer_mobilenet = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model_transfer_mobilenet.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "dT_KWIlKf5fJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = timer.time()\n",
        "history = model_transfer_mobilenet.fit(train_batches, epochs=5, validation_data=validation_batches)\n",
        "end_time = timer.time()"
      ],
      "metadata": {
        "id": "TTjakqBMg7FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Thời gian huấn luyện mô hình: {str(timedelta(seconds=end_time - start_time))}\")"
      ],
      "metadata": {
        "id": "jkiMqXdgjY3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lưu mô hình lại\n",
        "model_transfer_mobilenet_path = 'model_transfer_mobilenet.h5'\n",
        "model_transfer_mobilenet.save(model_transfer_mobilenet_path)"
      ],
      "metadata": {
        "id": "4VKj6W5hlK6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Convert the Model to TensorFlow Lite  \n",
        "\n",
        "Tiến hành chuyển đổi mô hình sang dạng TensorFlow Lite. 🔄📱💡  \n"
      ],
      "metadata": {
        "id": "eFvSOmrWlUv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load lại mô hình từ file.h5\n",
        "model_transfer_mobilenet = tf.keras.models.load_model(model_transfer_mobilenet_path)\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_transfer_mobilenet)\n",
        "tflite_model_transfer = converter.convert()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "J3oB7BIylcKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lưu mô hình TFLite\n",
        "tflite_model_transfer_path = 'model_transfer_mobilenet.tflite'\n",
        "with open(tflite_model_transfer_path, \"wb\") as f:\n",
        "    f.write(tflite_model_transfer)"
      ],
      "metadata": {
        "id": "L9MFqEPpnObv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiến hành tạo bộ thông dịch"
      ],
      "metadata": {
        "id": "iI-reX11mcIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_transfer_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]"
      ],
      "metadata": {
        "id": "nqK42uIvmbfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiến hành dự đoán 100 ảnh đầu tiên trong tập test"
      ],
      "metadata": {
        "id": "Ns9LfMJ0oYmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, test_labels, test_imgs = [], [], []\n",
        "start_time = timer.time()\n",
        "for img, label in test_batches.take(100):\n",
        "  interpreter.set_tensor(input_index, img)\n",
        "  interpreter.invoke()\n",
        "  predictions.append(interpreter.get_tensor(output_index))\n",
        "  test_labels.append(label.numpy()[0])\n",
        "  test_imgs.append(img)\n",
        "end_time = timer.time()"
      ],
      "metadata": {
        "id": "c5N_i_iHmVBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tụi mình sẽ kiểm tra kết quả dự đoán nha."
      ],
      "metadata": {
        "id": "8SQIutOppP0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = 0\n",
        "\n",
        "for item in range(100):\n",
        "  prediction = np.argmax(predictions[item])\n",
        "  label = test_labels[item]\n",
        "  if prediction == label:\n",
        "    score += 1\n",
        "\n",
        "print(f\"Tỷ lệ dự đoán đúng là: {score}%\")\n",
        "print(f\"Thời gian dự đoán: {str(timedelta(seconds=end_time - start_time))}\")"
      ],
      "metadata": {
        "id": "h-mP3B4ZpPTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kết quả ở lần chạy của mình:\n",
        "> Tỷ lệ dự đoán đúng là: 100%   \n",
        "Thời gian dự đoán: 0:00:01.197634\n",
        "\n",
        "Ấn tượng nhỉ."
      ],
      "metadata": {
        "id": "muXj_gqcpnzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['cat', 'dog']\n",
        "\n",
        "figure, axs = plt.subplots(nrows=5, ncols=5, figsize=(18, 15))\n",
        "axs = axs.flatten()\n",
        "for item in range(25):\n",
        "  prediction = np.argmax(predictions[item])\n",
        "  axs[item].imshow(test_imgs[item][0])\n",
        "  axs[item].set(xlabel=f'{class_names[prediction]} ({100 * np.max(predictions[item]):.2f}%)')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "zmFsZntdpwq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kết quả ở lần chạy của mình."
      ],
      "metadata": {
        "id": "HwSXzNHOtCJ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![results_predict](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_12/results_predict.png?raw=true)"
      ],
      "metadata": {
        "id": "BmbgpRHlCgz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Optimize the Model  \n",
        "\n",
        "Vậy là tụi mình đã hoàn thành xong một quy trình bao gồm huấn luyện, chuyển đổi mô hình và dự đoán rồi ha. Bây giờ tụi mình sẽ tiến hành tìm hiểu thêm bước tối ưu hóa và lượng tử hóa mô hình nha. 🚀📉🧠  \n"
      ],
      "metadata": {
        "id": "3DcLn9FLt2RU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Để tối ưu hóa mô hình, chúng ta chỉ cần thêm chỉ số optimize vào bộ chuyển đổi tại khâu chuyển đổi mô hình là được nha."
      ],
      "metadata": {
        "id": "ruMGLckGuaGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_transfer_mobilenet)\n",
        "# Tùy chọn: Bật tối ưu hóa để giảm kích thước mô hình\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model_transfer_optimized = converter.convert()\n",
        "# Lưu mô hình\n",
        "optimized_model_path = \"model_transfer_optimized.tflite\"\n",
        "with open(optimized_model_path, 'wb') as f:\n",
        "    f.write(tflite_model_transfer_optimized)"
      ],
      "metadata": {
        "id": "Ix3iWurVtEDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ở đây mình chỉ định tối ưu hóa mặc định tức là chỉ số cân bằng nhất dựa trên **kích thước** và **độ trễ**. Ngoài ra mọi người cũng có thể điều chỉnh thêm nhiều lựa chọn khác nha, chẳng hạn như:  \n",
        "- **OPTIMIZE_FOR_SIZE**: dựa trên kích thước 📏  \n",
        "- **OPTIMIZE_FOR_LATENCY**: dựa trên độ trễ ⏳  \n"
      ],
      "metadata": {
        "id": "sG6DfDQ7vkHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=optimized_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]"
      ],
      "metadata": {
        "id": "62WNyT0kxKTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiến hành dự đoán 100 ảnh đầu tiên trong tập test"
      ],
      "metadata": {
        "id": "e7yrbEE4xKTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, test_labels, test_imgs = [], [], []\n",
        "start_time = timer.time()\n",
        "\n",
        "for img, label in test_batches.take(100):\n",
        "  interpreter.set_tensor(input_index, img)\n",
        "  interpreter.invoke()\n",
        "  predictions.append(interpreter.get_tensor(output_index))\n",
        "  test_labels.append(label.numpy()[0])\n",
        "  test_imgs.append(img)\n",
        "end_time = timer.time()\n"
      ],
      "metadata": {
        "id": "gERwNq9wxKTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tụi mình sẽ kiểm tra kết quả dự đoán nha."
      ],
      "metadata": {
        "id": "utHeVwqrxKTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = 0\n",
        "for item in range(100):\n",
        "  prediction = np.argmax(predictions[item])\n",
        "  label = test_labels[item]\n",
        "  if prediction == label:\n",
        "    score += 1\n",
        "print(f\"Tỷ lệ dự đoán đúng là: {score}%\")\n",
        "print(f\"Thời gian dự đoán: {str(timedelta(seconds=end_time - start_time))}\")"
      ],
      "metadata": {
        "id": "oJ9YGK4axKTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kết quả ở lần chạy của mình:  \n",
        "> Tỷ lệ dự đoán đúng là: 98%   \n",
        "> Thời gian dự đoán: 0:00:02.588320  \n",
        "\n",
        "Uầy vậy là đã có sự khác biệt, độ chính xác chỉ còn 98%. Tốc độ dự đoán tăng lên nhiều. Nhưng đây vẫn là một con số ấn tượng rồi ha, chung quy 100 tấm hình vẫn chỉ mất hơn 2 giây tí. Bây giờ tụi mình ngó qua kích thước của mô hình thử xem nha. 📊⏱️📉  \n"
      ],
      "metadata": {
        "id": "DoZeqgeTxKTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![model_weight](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_12/model_weight.png?raw=true)"
      ],
      "metadata": {
        "id": "qsflLA0GCnCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uầy kích thước của mô hình giảm đi nhiều luôn ấy chứ. Từ gần 9MB chỉ còn hơn 2MB tí. Thật sự vô cùng ấn tượng ha. 🎉\n",
        "\n",
        "Tuy nhiên mọi người vẫn có thể thử cải thiện hơn nữa với phương pháp lượng tử hóa.  \n",
        "\n",
        "> **Vậy lượng tử hóa là gì?** 🤔  \n",
        "\n",
        "Mọi người có thể hiểu đơn giản là chuyển kiểu dữ liệu dạng bit xuống kích thước nhỏ hơn để giảm **độ 'nặng'** của mô hình hoặc tính toán nhanh hơn, giảm **độ trễ**, tiết kiệm năng lượng hơn nha. ⚡  \n",
        "\n",
        "Điển hình là mọi người có thể lượng tử hóa từ dạng số thập phân (float) 32-bit xuống thập phân 16-bit hoặc dạng số nguyên 8-bit nha. Tuy nhiên phương pháp này bởi vì thay đổi dữ liệu nên có thể cũng sẽ ảnh hưởng nhiều đến khả năng dự đoán nha. 📉  \n",
        "\n",
        "Như ở dưới này đi, mình sẽ tiến hành lượng tử hóa xuống dạng số nguyên 8-bit. Nhưng trước khi thực hiện bạn cần phải chỉ định một tập dữ liệu đại diện để cho bộ chuyển đổi biết phạm vi dữ liệu mà nó có thể đoán trước nha. Ở đây mình sử dụng tập test. 🧪📊  \n"
      ],
      "metadata": {
        "id": "zN3gKwHTzGFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_transfer_mobilenet)\n",
        "converter.optimizetions = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Chỉ định vùng dữ liệu cho bộ chuyển đổi\n",
        "def representative_data_gen():\n",
        "  for input_value, _ in test_batches.take(100):\n",
        "    yield [input_value]\n",
        "\n",
        "converter.representative_dataset = representative_data_gen\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "tflite_model_transfer_quant = converter.convert()\n",
        "tflite_model_transfer_quant_path = 'model_transfer_quant.tflite'\n",
        "\n",
        "with open(tflite_model_transfer_quant_path, 'wb') as f:\n",
        "  f.write(tflite_model_transfer_quant)"
      ],
      "metadata": {
        "id": "XBj_uH0O1lVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_transfer_quant_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "output_index = interpreter.get_output_details()[0][\"index\"]"
      ],
      "metadata": {
        "id": "pQD_3oVa3Ina"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiến hành dự đoán 100 ảnh đầu tiên trong tập test"
      ],
      "metadata": {
        "id": "EUMU1Nso3Inb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, test_labels, test_imgs = [], [], []\n",
        "start_time = timer.time()\n",
        "\n",
        "for img, label in test_batches.take(100):\n",
        "  interpreter.set_tensor(input_index, img)\n",
        "  interpreter.invoke()\n",
        "  predictions.append(interpreter.get_tensor(output_index))\n",
        "  test_labels.append(label.numpy()[0])\n",
        "  test_imgs.append(img)\n",
        "end_time = timer.time()\n"
      ],
      "metadata": {
        "id": "iFuILlCM3Inb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tụi mình sẽ kiểm tra kết quả dự đoán nha."
      ],
      "metadata": {
        "id": "HCgspG-D3Inb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = 0\n",
        "for item in range(100):\n",
        "  prediction = np.argmax(predictions[item])\n",
        "  label = test_labels[item]\n",
        "  if prediction == label:\n",
        "    score += 1\n",
        "print(f\"Tỷ lệ dự đoán đúng là: {score}%\")\n",
        "print(f\"Thời gian dự đoán: {str(timedelta(seconds=end_time - start_time))}\")"
      ],
      "metadata": {
        "id": "PcxbCmIW3Inb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kết quả ở lần chạy của mình:\n",
        ">Tỷ lệ dự đoán đúng là: 100%   \n",
        "Thời gian dự đoán: 0:00:01.307339\n",
        "\n",
        "Uầy, có vẻ vì bài toán này khá dễ nên tỷ lệ dự đoán đúng không bị ảnh hưởng quá nhiều. Tuy nhiên thời gian dự đoán lại có phần cải thiện hơn với trước đó. Nó xuống chỉ còn hơn 1 giây tí. điều này thật sự rất ấn tượng nha. Tuy nhiên kích thước mô hình lại có phần giống với mô hình ban đầu, hơn 8MB tí."
      ],
      "metadata": {
        "id": "e8Ba0LjL3Inc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Việc chỉ định trước dữ liệu đại diện cho phép bộ chuyển đổi có thể khám phá dữ liệu khi chạy qua mô hình và tìm ra bộ số tốt nhất để chuyển đổi. 🧠 Sau đó bằng cách thiết lập phần hỗ trợ **INT8** để giúp chỉ định lượng tử hóa đúng sang dạng số nguyên 8-bit trong những phần cụ thể của mô hình. ⚙️  \n",
        "\n",
        "Tuy nhiên việc kết hợp lượng tử hóa này đôi khi có thể làm tăng kích thước mô hình lên hoặc thay đổi khả năng dự đoán của mô hình, chúng ta cần phải xem xét nhiều hơn để vận dụng sao cho hợp lý với chúng ta. 📊  \n",
        "\n",
        "---  \n",
        "\n",
        "Có thể mọi người chưa biết thì **\"TensorFlow Lite for Microcontroller\"** là một phiên bản của TensorFlow Lite được phát triển cho các vi điều khiển có kích thước bộ nhớ vô cùng hạn chế. Đây là một lĩnh vực trong học máy mà mọi người có thể tìm kiếm với từ khóa **TinyML**. 🌐 Phiên bản này được viết dựa trên **C++** và có thể chạy được trên các bo mạch phát triển như Arduino Nano, Sparkfun Edge, ... Mọi người có thể tìm thử cuốn sách **\"TinyML: Machine Learning and TensorFlow Lite on Arduino and Ultra-Low Power Microcontrollers\"** của Pete Warden và Daniel Situnayake (O'Reilly) nha. 📚✨  "
      ],
      "metadata": {
        "id": "USwG-Qs84U4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary  \n",
        "Tổng kết lại, ở chương này tụi mình đã học được:  \n",
        "- Cách chuyển đổi mô hình sang dạng **tflite**. 🔄  \n",
        "- Cách để sử dụng mô hình từ file **.tflite**. 📂  \n",
        "- Cách tối ưu hóa mô hình cũng như khái niệm lượng tử hóa. 🛠️📉  \n",
        "\n",
        "Ngoài ra, tụi mình cũng đã khám phá thêm một số điểm quan trọng khác:  \n",
        "- **Lợi ích của việc tối ưu hóa mô hình**: Giúp giảm kích thước mô hình, tăng tốc độ dự đoán và tiết kiệm tài nguyên tính toán. Điều này đặc biệt hữu ích khi triển khai mô hình trên các thiết bị có tài nguyên hạn chế như điện thoại di động hoặc thiết bị IoT. 📱💡  \n",
        "- **Lượng tử hóa (Quantization)**: Một kỹ thuật mạnh mẽ giúp giảm độ phức tạp của mô hình bằng cách chuyển đổi dữ liệu từ dạng số thập phân (float) sang dạng số nguyên (int). Tuy nhiên, cần cân nhắc kỹ lưỡng vì nó có thể ảnh hưởng đến độ chính xác của mô hình. 🎯  \n",
        "- **TensorFlow Lite for Microcontrollers**: Một phiên bản đặc biệt của TensorFlow Lite dành cho các thiết bị vi điều khiển, mở ra cánh cửa cho việc triển khai AI trên các thiết bị siêu nhỏ gọn và tiết kiệm năng lượng. Đây là nền tảng cho lĩnh vực **TinyML** đang phát triển mạnh mẽ. 🌟  \n",
        "\n",
        "Với những kiến thức này, tụi mình đã có thể tự tin hơn trong việc chuyển đổi, tối ưu hóa và triển khai các mô hình học máy trên nhiều nền tảng khác nhau. Hãy tiếp tục khám phá và thử nghiệm để tạo ra những ứng dụng AI hiệu quả và sáng tạo nhé! 🚀🤖  "
      ],
      "metadata": {
        "id": "bXkfnarXAbC8"
      }
    }
  ]
}