{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/Chapter_8_Using_Tensorflow_to_Create_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yooo41ZlEgpM"
      },
      "source": [
        "# Chapter 8: Using Tensorflow to Create Text  \n",
        "### Hi, chÃ o má»i ngÆ°á»i, tá»¥i mÃ¬nh láº¡i gáº·p nhau rá»“i ğŸŒ».  \n",
        "### Trong chÆ°Æ¡ng nÃ y tá»¥i mÃ¬nh sáº½ tÃ¬m hiá»ƒu vá» cÃ¡ch sá»­ dá»¥ng cÃ¡c mÃ´ hÃ¬nh Ä‘á»ƒ táº¡o ra vÄƒn báº£n. NÃ³i nÃ´m na lÃ  giá»‘ng nhÆ° cÃ¡ch mÃ  cÃ¡c chatbot hiá»‡n táº¡i táº¡o ra cÃ¢u tráº£ lá»i cho má»i ngÆ°á»i Ã¡. NhÆ°ng cÃ¡i nÃ y á»Ÿ cáº¥p Ä‘á»™ sÆ¡ khai hÆ¡n. ğŸ˜Š  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrzNuwTVGAiL"
      },
      "source": [
        "![predictext](https://cgupta.tech/images/rnn_representative.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OpPX-_LILXF"
      },
      "source": [
        "NhÆ° má»i ngÆ°á»i tháº¥y á»Ÿ bá»©c hÃ¬nh trÃªn, Ä‘Ã¢y lÃ  cÆ¡ cháº¿ chÃ­nh cá»§a tÃ­nh nÄƒng táº¡o vÄƒn báº£n trong chÆ°Æ¡ng nÃ y.  \n",
        "\n",
        "TrÆ°á»›c khi Ä‘i vÃ o phÃ¢n tÃ­ch nÃ³ thÃ¬ chÃºng ta cÃ¹ng Ã´n láº¡i cÃ¡c chÆ°Æ¡ng trÆ°á»›c tÃ­ nhÃ¡. ğŸ˜Š  \n",
        "\n",
        "---\n",
        "\n",
        "- á» **chÆ°Æ¡ng 5**, má»i ngÆ°á»i tÃ¬m hiá»ƒu vá» cÆ¡ cháº¿ **Tokenize** Ä‘á»ƒ tÃ¡ch vÄƒn báº£n, táº¡o tá»« Ä‘iá»ƒn, mÃ£ hÃ³a á»Ÿ cáº¥p tá»« hay kÃ½ tá»± rá»“i thÃªm bá»™ Ä‘á»‡m hay padding vÃ o.  \n",
        "\n",
        "- Sang Ä‘áº¿n **chÆ°Æ¡ng 6**, chÃºng ta tÃ¬m hiá»ƒu vá» cÆ¡ cháº¿ **Embedding - Vector biá»ƒu diá»…n** trÃªn cÃ¡c chiá»u khÃ´ng gian cao hÆ¡n.  \n",
        "\n",
        "- VÃ  gáº§n Ä‘Ã¢y nháº¥t lÃ  **chÆ°Æ¡ng 7**, tÃ¬m hiá»ƒu vá» máº¡ng há»“i quy **RNN** vÃ  lá»›p **LSTM** qua Ä‘Ã³ cáº£i thiá»‡n kháº£ nÄƒng hiá»ƒu ngá»¯ nghÄ©a theo trÃ¬nh tá»± cá»§a chuá»—i.  \n",
        "\n",
        "BÃ¢y giá» Ä‘áº¿n vá»›i **chÆ°Æ¡ng 8**, tá»¥i mÃ¬nh sáº½ tÃ¬m hiá»ƒu vá» cÆ¡ cháº¿ **táº¡o/sinh vÄƒn báº£n** cá»§a cÃ¡c mÃ´ hÃ¬nh. Tháº­t ra nÃ³ khÃ´ng khÃ³ nhÆ° má»i ngÆ°á»i nghÄ© mÃ  ráº¥t lÃ  Ä‘Æ¡n giáº£n. ChÃºng Ä‘á»u dá»±a trÃªn nhá»¯ng cÆ¡ cháº¿ trÆ°á»›c giá» chÃºng ta há»c, tuy nhiÃªn khÃ¡c á»Ÿ khÃ¢u xá»­ lÃ½ vÃ  biáº¿n Ä‘á»•i dá»¯ liá»‡u tÃ­.  \n",
        "\n",
        "Váº­y sá»± khÃ¡c nhau nÃ y lÃ  gÃ¬, tá»¥i mÃ¬nh sáº½ cÃ¹ng tÃ¬m hiá»ƒu nhÃ¡. ğŸŒŸ  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYXNgZabNqiA"
      },
      "source": [
        "![process_text](https://i.imgur.com/7kOLuRM.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGLvJdjeOSzl"
      },
      "source": [
        "![padding_text](https://i.imgur.com/QOcbcqC.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVQotDeuLNQc"
      },
      "source": [
        "Trong cÃ¡c bÃ i toÃ¡n trÆ°á»›c Ä‘Ã¢y cá»§a chÃºng ta, tá»¥i mÃ¬nh sáº½ tiáº¿n hÃ nh **xá»­ lÃ½ dá»¯ liá»‡u vÄƒn báº£n sang dáº¡ng sá»‘** vÃ  **phÃ¢n loáº¡i chÃºng** dá»±a trÃªn **nhÃ£n cÃ³ sáºµn** cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n trÆ°á»›c Ä‘Ã³. ğŸ“Š  \n",
        "\n",
        "ThÆ°á»ng thÃ¬ **sá»‘ lÆ°á»£ng nhÃ£n khÃ¡ Ã­t**, vÃ­ dá»¥ nhÆ° chá»‰ cÃ³ hai nhÃ£n **\"tÃ­ch cá»±c\"** vÃ  **\"tiÃªu cá»±c\"** trong bÃ i toÃ¡n phÃ¢n tÃ­ch cáº£m xÃºc, hoáº·c vÃ i nhÃ£n cá»¥ thá»ƒ trong bÃ i toÃ¡n phÃ¢n loáº¡i vÄƒn báº£n. ğŸ·ï¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUEX7M8yOlLt"
      },
      "source": [
        "![classify_sentiment](https://media.mlhive.com/i/max/YbAcdK8b2Q4T5Ed4fIf.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vld5yzwhOtHV"
      },
      "source": [
        "Giá» Ä‘Ã¢y Ä‘áº¿n vá»›i **bÃ i toÃ¡n táº¡o/sinh vÄƒn báº£n**, thay vÃ¬ **phÃ¢n loáº¡i vá»›i cÃ¡c nhÃ£n cá»‘ Ä‘á»‹nh**, chÃºng ta sáº½ **dá»± Ä‘oÃ¡n tá»« tiáº¿p theo** cá»§a vÄƒn báº£n Ä‘Ã³. ğŸ“âœ¨  \n",
        "\n",
        "> **Æ , váº­y nhÃ£n cá»§a cÃ¡c vÄƒn báº£n sáº½ chÃ­nh lÃ  tá»« tiáº¿p theo cá»§a chÃºng Ã ?**  \n",
        "\n",
        "Yeah, chÃ­nh xÃ¡c rá»“i Ä‘Ã³! ğŸŒŸ"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![example](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_8/input_labels.png?raw=true)"
      ],
      "metadata": {
        "id": "AGDKDXvhzsh_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ORPU7SuQe2p"
      },
      "source": [
        "> **Ã”i, tháº¿ sá»‘ lÆ°á»£ng nhÃ£n cá»§a mÃ´ hÃ¬nh cháº³ng pháº£i sáº½ ráº¥t khá»•ng lá»“ hay sao ï¼ˆâŠ™ï½âŠ™ï¼‰?**  \n",
        "\n",
        "Yeah, báº¡n láº¡i Ä‘oÃ¡n Ä‘Ãºng rá»“i. ğŸ‰  \n",
        "\n",
        "Sá»‘ lÆ°á»£ng nhÃ£n chÃ­nh lÃ  **sá»‘ tá»« trong bá»™ tá»« Ä‘iá»ƒn** Ã¡ =))  \n",
        "CÃ³ khi nÃ³ lÃªn Ä‘áº¿n táº­n **10.000** vÃ  thÆ°á»ng sá»‘ lÆ°á»£ng epochs huáº¥n luyá»‡n cÅ©ng ráº¥t lá»›n, cÃ³ thá»ƒ tá»« **1.000** trá»Ÿ lÃªn.  \n",
        "\n",
        "> **Váº­y chÃºng táº¡o ra vÄƒn báº£n nhÆ° tháº¿ nÃ o?**  \n",
        "\n",
        "ÄÆ¡n giáº£n thÃ´i, tá»¥i mÃ¬nh sáº½ tiáº¿n hÃ nh **láº·p Ä‘i láº·p láº¡i quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n tá»« tiáº¿p theo**. ğŸ”„  \n",
        "Khi má»i ngÆ°á»i dá»± Ä‘oÃ¡n Ä‘Æ°á»£c má»™t tá»« má»›i, chÃºng ta sáº½ láº¥y tá»« Ä‘Ã³ **káº¿t há»£p láº¡i vá»›i vÄƒn báº£n Ä‘áº§u vÃ o** vÃ  tiáº¿p tá»¥c dá»± Ä‘oÃ¡n tá»« káº¿ tiáº¿p.  \n",
        "Cá»© tiáº¿n hÃ nh nhÆ° tháº¿ cho Ä‘áº¿n khi háº¿t sá»‘ vÃ²ng láº·p hoáº·c Ä‘áº¡t giá»›i háº¡n sá»‘ lÆ°á»£ng tá»« dá»± Ä‘oÃ¡n. ğŸŒŸ  \n",
        "\n",
        "NÃ³ giá»‘ng há»‡t vá»›i hÃ¬nh mÃ  tá»¥i mÃ¬nh Ä‘Ã£ nhÃ¬n tháº¥y á»Ÿ pháº§n Ä‘áº§u Ä‘Ã³. ğŸ“„âœ¨  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_dLy8wRTkuX"
      },
      "source": [
        "![predictext](https://cgupta.tech/images/rnn_representative.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_znzuVmHTl3J"
      },
      "source": [
        "á» Ä‘Ã¢y, cÃ¢u vÄƒn ban Ä‘áº§u cá»§a chÃºng ta lÃ : **\"the man is walking\"**, pháº§n mÃ u xanh dÆ°Æ¡ng lÃ  **cá»­a sá»• trÆ°á»£t - window data** Ä‘á»ƒ cáº¯t láº¥y dá»¯ liá»‡u Ä‘áº§u vÃ o vá»›i Ä‘á»™ dÃ i tá»‘i Ä‘a hiá»‡n táº¡i lÃ  **4 tá»«**. Do Ä‘Ã³, **dá»¯ liá»‡u Ä‘áº§u vÃ o** cá»§a chÃºng ta sáº½ lÃ  **\"the man is walking\"**.  \n",
        "\n",
        "***Má»i ngÆ°á»i nhá»› kÄ© pháº§n cá»­a sá»• trÆ°á»£t cáº¯t láº¥y dá»¯ liá»‡u Ä‘áº§u vÃ o giÃºp mÃ¬nh nha.*** ğŸ“  \n",
        "\n",
        "QuÃ¡ trÃ¬nh táº¡o cÃ¢u vÄƒn, á»Ÿ Ä‘Ã¢y mÃ¬nh cho sá»‘ lÆ°á»£ng tá»« táº¡o ra lÃ  **4**, nÃªn chÃºng ta sáº½ cÃ³ **4 láº§n láº·p**.  \n",
        "\n",
        "**Step 1**: Nháº­n Ä‘áº§u vÃ o lÃ  **\"the man is walking\"**, tá»¥i mÃ¬nh dá»± Ä‘oÃ¡n ra Ä‘Æ°á»£c tá»« **\"down\"**.  \n",
        "\n",
        "**Step 2**: Tá»¥i mÃ¬nh káº¿t há»£p tá»« **\"down\"** vÃ o chung vá»›i vÄƒn báº£n Ä‘áº§u vÃ o lÃºc trÆ°á»›c lÃ  **\"the man is walking\"** thÃ nh **\"the man is walking down\"**, sau Ä‘Ã³ **cá»­a sá»• trÆ°á»£t vá»›i kÃ­ch thÆ°á»›c lÃ  4** sáº½ cáº¯t láº¡i dá»¯ liá»‡u Ä‘áº§u vÃ o thÃ nh **\"man is walking down\"** vÃ  dá»± Ä‘oÃ¡n ra tá»« **\"the\"**.  \n",
        "\n",
        "**Step 3**: TÆ°Æ¡ng tá»± nhÆ° trÆ°á»›c, dá»¯ liá»‡u Ä‘áº§u vÃ o sau khi cáº¯t lÃ  **\"is walking down the\"**, dá»± Ä‘oÃ¡n ra tá»« **\"street\"**.  \n",
        "\n",
        "**Step 4**: Dá»¯ liá»‡u Ä‘áº§u vÃ o lÃ  **\"walking down the street\"**, dá»± Ä‘oÃ¡n ra dáº¥u **\".\"**.  \n",
        "\n",
        "Váº­y cÃ¢u cuá»‘i cÃ¹ng chÃºng ta dá»± Ä‘oÃ¡n ra Ä‘Æ°á»£c sáº½ lÃ :  \n",
        "> **\"the man is walking down the street .\"** ğŸš¶â€â™‚ï¸âœ¨  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FzYVdCkaLCd"
      },
      "source": [
        "# LÃ½ thuyáº¿t Ä‘á»§ nhiá»u rá»“i, tá»¥i mÃ¬nh sáº½ tiáº¿n hÃ nh Ä‘i vÃ o thá»±c hÃ nh luÃ´n nha. ãƒ¾(â€¢Ï‰â€¢`)o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYCnwYGAumKL"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import kagglehub\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from datetime import timedelta\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTHJS8tl-ySf"
      },
      "outputs": [],
      "source": [
        "# Táº¡o má»™t MirroredStrategy Ä‘á»ƒ sá»­ dá»¥ng táº¥t cáº£ cÃ¡c GPU cÃ³ sáºµn\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxfK_EC-uep1"
      },
      "source": [
        "# Tiáº¿n hÃ nh xá»­ lÃ½ dá»¯ liá»‡u ğŸ› ï¸âœ¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWxk5JKrbCgu"
      },
      "source": [
        "Dá»¯ liá»‡u mÃ  chÃºng ta sáº½ dÃ¹ng lÃ  Ä‘oáº¡n thÆ¡ dÆ°á»›i Ä‘Ã¢y nha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUVBfZv1cDnp"
      },
      "outputs": [],
      "source": [
        "data = \"\"\"In the town of Athy one Jeremy Lanigan\n",
        " Battered away til he hadnt a pound.\n",
        " His father died and made him a man again\n",
        " Left him a farm and ten acres of ground.\n",
        " He gave a grand party for friends and relations\n",
        " Who didnt forget him when come to the wall,\n",
        " And if youll but listen Ill make your eyes glisten\n",
        " Of the rows and the ructions of Laniganâ€™s Ball.\n",
        " Myself to be sure got free invitation,\n",
        " For all the nice girls and boys I might ask,\n",
        " And just in a minute both friends and relations\n",
        " Were dancing round merry as bees round a cask.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yjydxacbc1Wr"
      },
      "outputs": [],
      "source": [
        "# Kiá»ƒm tra cÃº phÃ¡p Ä‘oáº¡n thÆ¡\n",
        "print(repr(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zx47s04-fH_"
      },
      "outputs": [],
      "source": [
        "# Tá»¥i mÃ¬nh sáº½ coi máº«u dÃ²ng trong Ä‘oáº¡n thÆ¡ lÃ  má»™t cÃ¢u vÃ  tÃ¡ch chÃºng ra dá»±a trÃªn dáº¥u xuá»‘ng hÃ ng \\n nha\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "# Táº¡o bá»™ tá»« Ä‘iá»ƒn tá»« bá»™ dá»¯ liá»‡u cá»§a chÃºng ta.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "# Kiá»ƒm tra bá»™ tá»« Ä‘iá»ƒn\n",
        "print(tokenizer.word_index)\n",
        "print(f\"Sá»‘ lÆ°á»£ng tá»« trong bá»™ tá»« Ä‘iá»ƒn lÃ : {len(tokenizer.word_index)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in8T7Aj3fWU6"
      },
      "outputs": [],
      "source": [
        "total_words = len(tokenizer.word_index) + 1 # ThÃªm 1 vÃ´ Ä‘áº¡i diá»‡n cho pháº§n token <padding>: 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SExt7Nae8ff"
      },
      "source": [
        "### BÃ¢y giá» tá»¥i mÃ¬nh sáº½ tá»›i vá»›i pháº§n xá»­ lÃ½ dá»¯ liá»‡u huáº¥n luyá»‡n. ğŸ› ï¸  \n",
        "***Má»i ngÆ°á»i chÃº Ã½ kÄ© pháº§n nÃ y, trÃ¡nh nháº§m láº«n giÃºp mÃ¬nh nha. á» pháº§n thá»±c hÃ nh nÃ y, táº¡m thá»i chÃºng ta sáº½ khÃ´ng sá»­ dá»¥ng cá»­a sá»• trÆ°á»£t!***  \n",
        "\n",
        "Äáº§u tiÃªn, ta tiáº¿n hÃ nh mÃ£ hÃ³a cÃ¢u tá»« dá»¯ liá»‡u vÄƒn báº£n sang chuá»—i dáº¡ng sá»‘. ğŸ”¢"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![encode](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_8/encode.png?raw=true)"
      ],
      "metadata": {
        "id": "Z7BftXSnyT2w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUtSQXAygX7l"
      },
      "source": [
        "Tiáº¿p Ä‘áº¿n, chÃºng ta sáº½ tÃ¡ch chuá»—i thÃ nh nhiá»u chuá»—i con nhá» hÆ¡n vá»›i Ä‘á»™ dÃ i chuá»—i tÄƒng dáº§n. âœ‚ï¸  \n",
        "\n",
        "***ChÃº Ã½: Má»i ngÆ°á»i cáº§n pháº£i phÃ¢n biá»‡t, hiá»‡n táº¡i chÃºng ta khÃ´ng sá»­ dá»¥ng cá»­a sá»• trÆ°á»£t á»Ÿ Ä‘Ã¢y nha!*** ğŸ§  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sub_sequences](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_8/sub_sequences.png?raw=true)"
      ],
      "metadata": {
        "id": "Jl0G0RYSyXgm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5SJT0Nzgtn3"
      },
      "source": [
        "NhÆ° má»i ngÆ°á»i tháº¥y, tá»« má»™t chuá»—i ban Ä‘áº§u chÃºng ta Ä‘Ã£ tÃ¡ch ra thÃ nh 7 chuá»—i con, nhá» Ä‘Ã³ gia tÄƒng Ä‘Æ°á»£c sá»‘ lÆ°á»£ng dá»¯ liá»‡u. ğŸŒ± CÃ¡c chuá»—i con Ä‘á»ƒ huáº¥n luyá»‡n nÃ y cÃ²n cÃ³ tÃªn gá»i khÃ¡c lÃ  **\"seed text - háº¡t giá»‘ng vÄƒn báº£n\"**, khá»Ÿi nguá»“n Ä‘á»ƒ náº£y máº§m, táº¡o nÃªn cÃ¡c vÄƒn báº£n dá»± Ä‘oÃ¡n sau nÃ y.\n",
        "Viá»‡c chia cÃ¡c chuá»—i con nÃ y giÃºp cho mÃ´ hÃ¬nh cÃ³ thá»ƒ há»c Ä‘Æ°á»£c ngá»¯ cáº£nh á»Ÿ nhiá»u Ä‘á»™ dÃ i khÃ¡c nhau.ğŸŒŸ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAxkotOlhKwP"
      },
      "outputs": [],
      "source": [
        "input_sequences = []\n",
        "for line in corpus:\n",
        "  # MÃ£ hÃ³a cÃ¢u (line) thÃ nh chuá»—i sá»‘\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0] # VÃ¬ dá»¯ liá»‡u tráº£ vá» cá»§a hÃ m lÃ  dáº¡ng má»™t list nÃªn Ä‘á»ƒ láº¥y Ä‘Æ°á»£c chuá»—i sá»‘ pháº£i láº¥y pháº§n tá»­ 0 cá»§a list\n",
        "  for i in range(1, len(token_list)):\n",
        "    # Tiáº¿n hÃ nh tÃ¡ch ra thÃ nh tá»«ng chuá»—i con\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Kiá»ƒm tra thá»­ 5 chuá»—i Ä‘áº§u tiÃªn\n",
        "for i in range(5):\n",
        "  print(input_sequences[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUdDON_DiOjd"
      },
      "source": [
        "BÃ¢y giá» chÃºng ta tiáº¿n hÃ nh **padding** cho dá»¯ liá»‡u dá»±a trÃªn Ä‘á»™ dÃ i cá»§a cÃ¢u dÃ i nháº¥t. ğŸ› ï¸\n",
        "\n",
        "LÃ½ do lÃ  bá»Ÿi vÃ¬ khÃ´ng cá»‘ Ä‘á»‹nh kÃ­ch thÆ°á»›c nhÆ° **cá»­a sá»• trÆ°á»£t**, dá»¯ liá»‡u giá»¯a cÃ¡c cÃ¢u cá»§a chÃºng ta cÃ³ kÃ­ch thÆ°á»›c chÃªnh lá»‡ch ráº¥t nhiá»u. ChÃºng ta cáº§n **Ä‘á»“ng dáº¡ng kÃ­ch thÆ°á»›c** Ä‘á»ƒ Ä‘Æ°a vÃ o huáº¥n luyá»‡n mÃ´ hÃ¬nh. ğŸ“\n",
        "\n",
        "*ğŸ’¡ LÆ°u Ã½: Trong trÆ°á»ng há»£p nÃ y, chÃºng ta sáº½ **padding á»Ÿ Ä‘áº§u** (pre-padding) nha, Ä‘á»ƒ tá»« cÃ³ thá»ƒ áº£nh hÆ°á»Ÿng máº¡nh hÆ¡n Ä‘áº¿n tá»« dá»± Ä‘oÃ¡n phÃ­a sau, giá»‘ng vá»›i **kháº£ nÄƒng áº£nh hÆ°á»Ÿng cá»§a trÃ¬nh tá»± cÃ¡c tá»«** mÃ  chÃºng ta Ä‘Ã£ tÃ¬m hiá»ƒu á»Ÿ chÆ°Æ¡ng 7.* ğŸŒŸ"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![padding_sequences](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_8/pad_sequences.png?raw=true)\n"
      ],
      "metadata": {
        "id": "v1hq-eENyj3q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEd3Sg6ukep8"
      },
      "outputs": [],
      "source": [
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences_padded = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding=\"pre\"))\n",
        "\n",
        "# Chá»‰nh láº¡i sá»‘ lÆ°á»£ng token vÃ¬ Ä‘Ã£ thÃªm token padding vÃ o <pad>: 0\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Kiá»ƒm tra chuá»—i sau khi Ä‘á»‡m\n",
        "for i in range(5):\n",
        "  print(input_sequences_padded[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwucCRjQnb2T"
      },
      "source": [
        "Váº­y lÃ  cÃ¡c chuá»—i cá»§a chÃºng ta Ä‘Ã£ Ä‘Æ°á»£c **Ä‘á»‡m Ä‘áº§y Ä‘á»§** vá»›i kÃ­ch thÆ°á»›c lÃ  **10**. ğŸ‰\n",
        "\n",
        "BÃ¢y giá», chÃºng ta sáº½ tiáº¿n hÃ nh **chia dá»¯ liá»‡u** thÃ nh hai pháº§n:\n",
        "\n",
        "1. **Dá»¯ liá»‡u huáº¥n luyá»‡n**: ChÃ­nh lÃ  cÃ¡c tá»« trong chuá»—i **trá»« tá»« cuá»‘i cÃ¹ng**.\n",
        "2. **NhÃ£n**: LÃ  **tá»« cuá»‘i cÃ¹ng** cá»§a má»—i chuá»—i.\n",
        "\n",
        "QuÃ¡ trÃ¬nh nÃ y sáº½ giÃºp chÃºng ta cÃ³ dá»¯ liá»‡u phÃ¹ há»£p Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n tá»« tiáº¿p theo. ğŸš€"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![padding_sequences](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_8/x_labels.png?raw=true)"
      ],
      "metadata": {
        "id": "eLKglTjTyy5K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIKlNHE8qIFj"
      },
      "outputs": [],
      "source": [
        "xs = input_sequences_padded[:,:-1]\n",
        "labels = input_sequences_padded[:,-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RUI2BuFqWqa"
      },
      "source": [
        "á» Ä‘Ã¢y tá»¥i mÃ¬nh sáº½ Ã¡p dá»¥ng má»™t **thá»§ thuáº­t nhá»** nhÃ©! ğŸŒŸ ÄÃ¢y cÅ©ng lÃ  cÆ¡ há»™i Ä‘á»ƒ tiáº¿p cáº­n má»™t loáº¡i **loss function** khÃ¡c.\n",
        "\n",
        "NhÆ° trÆ°á»›c Ä‘Ã¢y mÃ¬nh Ä‘Ã£ Ä‘á» cáº­p, cÃ³ sá»± khÃ¡c biá»‡t giá»¯a **`sparse_categorical_crossentropy`** vÃ  **`categorical_crossentropy`**:\n",
        "\n",
        "- Náº¿u dá»¯ liá»‡u nhÃ£n lÃ  dáº¡ng **sá»‘ index** (vÃ­ dá»¥: `[0, 1, 2]`), ta sáº½ sá»­ dá»¥ng **`sparse_categorical_crossentropy`**.\n",
        "- Trong trÆ°á»ng há»£p nÃ y, tá»¥i mÃ¬nh sáº½ tiáº¿n hÃ nh **mÃ£ hÃ³a nhÃ£n theo one-hot-encoding**, nÃªn cáº§n sá»­ dá»¥ng **`categorical_crossentropy`**.\n",
        "\n",
        "### LÆ°u Ã½ nhá»:\n",
        "- Sá»­ dá»¥ng **index-based labels** (dáº¡ng sá»‘) cÃ³ thá»ƒ **tiáº¿t kiá»‡m bá»™ nhá»› hÆ¡n** vÃ¬ khÃ´ng cáº§n lÆ°u trá»¯ toÃ n bá»™ vector one-hot-encoding.\n",
        "- Tuy nhiÃªn, vÃ¬ sÃ¡ch Ä‘Ã£ hÆ°á»›ng dáº«n nhÆ° váº­y, tá»¥i mÃ¬nh cá»© lÃ m theo Ä‘á»ƒ **lÃ m quen vá»›i cáº£ hai cÃ¡ch** nhÃ©! âœ¨"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![dataset](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_8/dataset.png?raw=true)"
      ],
      "metadata": {
        "id": "xv9yH-kdy5RK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhDwD98vrgMa"
      },
      "outputs": [],
      "source": [
        "# Tiáº¿n hÃ nh mÃ£ hÃ³a one-hot-encoding cho nhÃ£n\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "ys = to_categorical(labels, num_classes = total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCIMJdgIsKLo"
      },
      "outputs": [],
      "source": [
        "# Kiá»ƒm tra láº¡i dá»¯ liá»‡u cá»§a chÃºng ta\n",
        "print(f\"Chuá»—i Ä‘Æ°á»£c mÃ£ hÃ³a: {input_sequences[0]}\")\n",
        "print(f\"Chuá»—i x Ä‘á»ƒ huáº¥n luyá»‡n: {xs[0]}\")\n",
        "print(f\"NhÃ£n cá»§a chuá»—i x, tá»©c tá»« tiáº¿p theo: {labels[0]}\")\n",
        "print(f\"NhÃ£n sau khi one-hot-encoding:\\n {ys[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfHd5Ahluajw"
      },
      "source": [
        "# Tiáº¿n hÃ nh táº¡o vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh ğŸš€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw23SIbMurtm"
      },
      "source": [
        "á» Ä‘Ã¢y tá»¥i mÃ¬nh sáº½ thá»­ khá»Ÿi táº¡o má»™t mÃ´ hÃ¬nh Ä‘Æ¡n giáº£n thÃ´i nha.  \n",
        "\n",
        "Sá»‘ chiá»u **Embedding** tá»¥i mÃ¬nh táº¡m cho lÃ  8 nha vÃ¬ kÃ­ch thÆ°á»›c tá»« Ä‘iá»ƒn, sá»‘ lÆ°á»£ng tá»« cÅ©ng khÃ¡ nhá».  \n",
        "\n",
        "Vá» tham sá»‘ trong lá»›p **BLSTM**, sá»‘ Ä‘Æ¡n vá»‹ áº©n nhÆ° chÆ°Æ¡ng trÆ°á»›c tá»¥i mÃ¬nh Ä‘á»ƒ theo sá»‘ chiá»u Embedding thÃ¬ á»Ÿ Ä‘Ã¢y mÃ¬nh Ä‘á»ƒ tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i Ä‘á»™ dÃ i cá»§a chuá»—i ban Ä‘áº§u trá»« Ä‘i 1 nha (vÃ¬ chÃºng ta Ä‘Ã£ láº¥y token cuá»‘i lÃ m nhÃ£n nÃªn trá»« 1 Ä‘i).  \n",
        "\n",
        "Äáº§u ra lá»›p tuyáº¿n tÃ­nh - **Dense**, thÃ¬ sáº½ báº±ng vá»›i tá»•ng sá»‘ lÆ°á»£ng tokens hay sá»‘ lÆ°á»£ng tá»« vá»›i 1 token padding. âœ¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC_DEmseu36E"
      },
      "outputs": [],
      "source": [
        "# XÃ¢y dá»±ng kiáº¿n trÃºc mÃ´ hÃ¬nh\n",
        "model = Sequential([\n",
        "    Embedding(total_words, 8),\n",
        "    Bidirectional(LSTM(max_sequence_len-1)),\n",
        "    Dense(total_words, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYkUkiIwxEmA"
      },
      "source": [
        "Bá»Ÿi vÃ¬ mÃ´ hÃ¬nh á»Ÿ Ä‘Ã¢y khÃ¡ Ä‘Æ¡n giáº£n vÃ  Ã­t dá»¯ liá»‡u nÃªn chÃºng ta tiáº¿n hÃ nh huáº¥n luyá»‡n lÃ¢u hÆ¡n vá»›i **sá»‘ epoch lÃ  15.000**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO0NF3m4w2xN"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "history = model.fit(xs, ys, epochs=1500)\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzLFU3K6Qg57"
      },
      "outputs": [],
      "source": [
        "print(f\"Thá»i gian huáº¥n luyá»‡n: {timedelta(seconds=end_time - start_time)} giÃ¢y\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsNBEKtuxQAO"
      },
      "outputs": [],
      "source": [
        "# Váº½ biá»ƒu Ä‘á»“ huáº¥n luyá»‡n\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_acc = history.history[\"accuracy\"]\n",
        "train_loss = history.history[\"loss\"]\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize = (12, 4))\n",
        "axs[0].plot(train_acc)\n",
        "axs[0].set_title(\"Training Accuracy\")\n",
        "axs[1].plot(train_loss, color=\"orange\")\n",
        "axs[1].set_title(\"Training Loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzWGVLLsy2Bq"
      },
      "outputs": [],
      "source": [
        "eval = model.evaluate(xs, ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eC4JQ-N6qDQ"
      },
      "outputs": [],
      "source": [
        "model.save(\"model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gY2QpWpFWO8"
      },
      "outputs": [],
      "source": [
        "# Táº£i láº¡i model\n",
        "model = load_model(\"model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O082PMny6Ab"
      },
      "source": [
        "Káº¿t quáº£ cuá»‘i cÃ¹ng tá»¥i mÃ¬nh Ä‘o Ä‘Æ°á»£c sáº½ khoáº£ng cá»¡ 95%, Ä‘iá»u Ä‘Ã³ cÃ³ nghÄ©a lÃ  tá»« dá»± Ä‘oÃ¡n Ä‘Æ°á»£c tiáº¿p theo cÃ³ xÃ¡c suáº¥t 95% giá»‘ng vá»›i tá»« Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n bá»Ÿi chuá»—i trÆ°á»›c Ä‘Ã³.  \n",
        "\n",
        "***Má»i ngÆ°á»i lÆ°u Ã½ giÃºp mÃ¬nh: á» Ä‘Ã¢y sá»­ dá»¥ng Ä‘á»™ chÃ­nh xÃ¡c - accuracy Ä‘á»ƒ Ä‘o khÃ´ng tháº­t sá»± hiá»‡u quáº£ Ä‘á»‘i vá»›i bÃ i toÃ¡n ngá»¯ nghÄ©a táº¡o/sinh vÄƒn báº£n nhÆ° nÃ y. Cho Ä‘áº¿n thá»i Ä‘iá»ƒm hiá»‡n táº¡i tá»¥i mÃ¬nh há»c, váº«n chÆ°a cÃ³ thang Ä‘o nÃ o phÃ¹ há»£p tuyá»‡t Ä‘á»‘i cho bÃ i toÃ¡n.*** ğŸŒŸ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzTqlPpQ3Qq5"
      },
      "source": [
        "### Predicting the Next Word  \n",
        "### BÃ¢y giá» tá»¥i mÃ¬nh sáº½ tiáº¿n hÃ nh dá»± Ä‘oÃ¡n thá»­ má»™t tá»« tiáº¿p theo trÆ°á»›c nha.  \n",
        "\n",
        "VÄƒn báº£n mÃ¬nh chá»n lÃ  **\"in the town of athy\"** náº±m á»Ÿ cÃ¢u Ä‘áº§u tiÃªn cá»§a dá»¯ liá»‡u huáº¥n luyá»‡n nha. ğŸŒŸ  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVW2qQCv3rgx"
      },
      "outputs": [],
      "source": [
        "seed_text =  \"in the town of athy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WebWVHuI3w-_"
      },
      "source": [
        "Äáº§u tiÃªn ta sáº½ tiáº¿n hÃ nh tokenize cho chuá»—i Ä‘Ã³, sau Ä‘Ã³ thÃ¬ padding vÃ  Ä‘Æ°a vÃ o mÃ´ hÃ¬nh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqRF78S_32r3"
      },
      "outputs": [],
      "source": [
        "token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') # -1 vÃ o max_sequence_len bá»Ÿi vÃ¬ chuá»—i nÃ y khÃ´ng cÃ³ nhÃ£n\n",
        "# Tiáº¿n hÃ nh dá»± Ä‘oÃ¡n\n",
        "predicted = model.predict(token_list, verbose=1)\n",
        "predicted_index = np.argmax(predicted, axis = -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfUxFC1q4RuN"
      },
      "outputs": [],
      "source": [
        "# Táº¡o index to word Ä‘á»ƒ chuyá»ƒn tá»« dá»¯ liá»‡u dáº¡ng sá»‘ sang vÄƒn báº£n láº¡i\n",
        "index_words ={}\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    index_words[index] = word\n",
        "\n",
        "# Chuyá»ƒn index dá»± Ä‘oÃ¡n thÃ nh dáº¡ng chá»¯\n",
        "predicted_word = index_words[predicted_index[0]]\n",
        "\n",
        "print(f\"Chuá»—i ban Ä‘áº§u: {seed_text}\")\n",
        "print(f\"Tá»« dá»± Ä‘oÃ¡n: '{predicted_word}' vá»›i sÃ¡c xuáº¥t {predicted[0][predicted_index[0]]*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-gbSdiZ55xL"
      },
      "source": [
        "Uáº§y ra tá»« \"one\" Ä‘Ãºng vá»›i cÃ¢u Ä‘áº§u tiÃªn trong dá»¯ liá»‡u huáº¥n luyá»‡n luÃ´n nÃ y.\n",
        "> In the town of Athy one Jeremy Lanigan<br>\n",
        " Battered away til he hadnt a pound."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O48-oBgP6KdF"
      },
      "source": [
        "### Compounding Predictions to Generate Text  \n",
        "### BÃ¢y giá» tá»¥i mÃ¬nh sáº½ tiáº¿n hÃ nh táº¡o/sinh vÄƒn báº£n dá»±a trÃªn workflow trÆ°á»›c Ä‘Ã³ tá»¥i mÃ¬nh Ä‘á»‹nh nghÄ©a nha. ğŸŒŸâœ¨ğŸŒˆ  \n",
        "\n",
        "***LÆ°u Ã½: á» Ä‘Ã¢y chÃºng ta chá»n padding chuá»—i theo Ä‘á»™ dÃ i cÃ¢u dÃ i nháº¥t vÃ¬ ban Ä‘áº§u khi dá»¯ liá»‡u huáº¥n luyá»‡n cÅ©ng Ä‘Æ°á»£c lÃ m nhÆ° váº­y chá»© chÆ°a sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p cá»­a sá»• trÆ°á»£t nha. Äá»ƒ mÃ  nÃ³i má»i ngÆ°á»i cÅ©ng cÃ³ thá»ƒ hiá»ƒu trÆ°á»ng há»£p nÃ y kÃ­ch thÆ°á»›c cá»­a sá»• trÆ°á»£t lÃ  Ä‘á»™ dÃ i cÃ¢u dÃ i nháº¥t cÅ©ng Ä‘Æ°á»£c. Tá»« Ä‘Ã³ cÃ¡c chuá»—i khÃ´ng Ä‘á»§ kÃ­ch thÆ°á»›c thÃ¬ sáº½ padding thÃªm vÃ o. ğŸ”„ğŸ“  ***\n",
        "\n",
        "ThÃ´ng qua viá»‡c láº·p Ä‘i láº·p láº¡i quÃ¡ trÃ¬nh dá»± Ä‘oÃ¡n tá»« tiáº¿p theo, káº¿t há»£p láº¡i vá»›i dá»¯ liá»‡u Ä‘áº§u vÃ o rá»“i dá»± Ä‘oÃ¡n tiáº¿p ta sáº½ táº¡o/sinh ra Ä‘Æ°á»£c pháº§n tiáº¿p theo cho cÃ¢u vÄƒn. ğŸ“ğŸ”®  \n",
        "\n",
        "á» Ä‘Ã¢y mÃ¬nh sá»­ dá»¥ng dá»¯ liá»‡u Ä‘áº§u vÃ o ban Ä‘áº§u lÃ  **\"sweet jeremy saw dublin\"** nha. Tá»¥i mÃ¬nh sáº½ tiáº¿n hÃ nh dá»± Ä‘oÃ¡n 10 tá»« tiáº¿p theo phÃ­a sau. ğŸ¯ğŸ“–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugHAzgSL616W"
      },
      "outputs": [],
      "source": [
        "seed_text = \"sweet jeremy saw dublin\"\n",
        "predicted_text = seed_text\n",
        "n_words = 10\n",
        "\n",
        "for i in range(n_words):\n",
        "    token_list = tokenizer.texts_to_sequences([predicted_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict(token_list, verbose=1)\n",
        "    predicted_index = np.argmax(predicted, axis=-1)\n",
        "    output_word = index_words[predicted_index[0]]\n",
        "\n",
        "    print(f\"Step {i}:\")\n",
        "    print(f\"VÄƒn báº£n Ä‘áº§u vÃ o: {predicted_text}\")\n",
        "    print(f\"Chuá»—i mÃ£ hÃ³a Ä‘áº§u vÃ o: {token_list}\")\n",
        "    print(f\"Tá»« dá»± Ä‘oÃ¡n: {output_word} vá»›i xÃ¡c suáº¥t {predicted[0][predicted_index[0]]*100:2f}\")\n",
        "    print(\"-\"*50)\n",
        "    predicted_text += \" \" + output_word\n",
        "\n",
        "print(predicted_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD6R16wN3LNS"
      },
      "source": [
        "ÄÃ¢y lÃ  káº¿t quáº£ á»Ÿ láº§n cháº¡y cá»§a mÃ¬nh nha:  \n",
        "\n",
        "> \"sweet jeremy saw dublin all the rows a and of laniganâ€™s ball ask ask\"  \n",
        "\n",
        "Má»i ngÆ°á»i cÃ³ thá»ƒ tháº¥y cÃ ng vá» sau thÃ¬ cÃ¡c tá»« táº¡o ra khiáº¿n cÃ¢u vÄƒn cÃ ng trá»Ÿ nÃªn vÃ´ nghÄ©a hÆ¡n, tháº­m chÃ­ sai cáº£ vá» máº·t ngá»¯ phÃ¡p cá»§a tá»«. ğŸ˜…ğŸ“‰  \n",
        "\n",
        "> **Váº­y lá»—i nÃ y do Ä‘Ã¢u mÃ  ra?**  \n",
        "\n",
        "CÃ³ 2 lÃ½ do chÃ­nh áº£nh hÆ°á»Ÿng Ä‘áº¿n viá»‡c nÃ y lÃ :  \n",
        "\n",
        "1. **Dá»¯ liá»‡u huáº¥n luyá»‡n cá»§a chÃºng ta quÃ¡ nhá»** khiáº¿n mÃ´ hÃ¬nh khÃ´ng thá»ƒ náº¯m báº¯t Ä‘Æ°á»£c nhiá»u ngá»¯ cáº£nh. ğŸ“šâŒ  \n",
        "2. **Hiá»‡u á»©ng dÃ¢y chuyá»n, cÃ¡nh bÆ°á»›m hay hiá»‡u á»©ng Domino.** Khi má»™t viá»‡c gÃ¬ Ä‘Ã³ xáº£y ra thÃ¬ cÃ¡c sá»± viá»‡c sau phá»¥ thuá»™c vÃ o nÃ³ sáº½ chá»‹u áº£nh hÆ°á»Ÿng lá»›n dáº§n lÃªn. á» Ä‘Ã¢y, khi má»™t tá»« Ä‘Æ°á»£c dá»± Ä‘oÃ¡n khÃ´ng tá»‘t, cÃ¡c tá»« phÃ­a sau nÃ³ láº¡i Ä‘Æ°á»£c dá»± Ä‘oÃ¡n dá»±a trÃªn nÃ³ vÃ  cÃ¡c tá»« phÃ­a trÆ°á»›c cÅ©ng sáº½ trá»Ÿ nÃªn tá»‡ hÆ¡n. Dáº§n dáº§n sá»± tá»‡ háº¡i nÃ y ngÃ y má»™t lá»›n dáº§n khiáº¿n cho cÃ¡c tá»« ná»‘i vÃ o cÃ¢u vÄƒn cÃ ng vá» sau trá»Ÿ nÃªn vÃ´ nghÄ©a hÆ¡n ráº¥t lÃ  nhiá»u. ğŸ”—ğŸŒ€  \n",
        "\n",
        "CÃ³ má»™t fact lÃ  vÃ o nÄƒm 2016 Ä‘Ã£ tá»«ng cÃ³ má»™t bá»™ phim viá»…n tÆ°á»Ÿng [Sunspring](https://en.wikipedia.org/wiki/Sunspring) Ä‘Æ°á»£c táº¡o ra bá»Ÿi AI. Bá»™ phim nÃ y khÃ¡ buá»“n cÆ°á»i, ban Ä‘áº§u ná»™i dung váº«n á»•n cho Ä‘áº¿n cÃ ng vá» sau chÃºng láº¡i trá»Ÿ nÃªn khÃ³ hiá»ƒu Ä‘Ãºng nghÄ©a ba cháº¥m luÃ´n Ã½. ğŸ¥ğŸ¤–âœ¨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnbyzGdC-riZ"
      },
      "source": [
        "# Extending the dataset  \n",
        "### BÃ¢y giá» tá»¥i mÃ¬nh sáº½ lÃ m viá»‡c vá»›i mÃ´ hÃ¬nh vá»›i bá»™ dá»¯ liá»‡u lá»›n hÆ¡n nha.  \n",
        "\n",
        "á» Ä‘Ã¢y theo tÃ¡c giáº£ thÃ¬ bá»™ dá»¯ liá»‡u dÆ°á»›i Ä‘Ã¢y bao gá»“m khoáº£ng 1.700 vá» lá»i cÃ¡c bÃ i hÃ¡t. VÃ¬ Ä‘Æ°á»ng dáº«n trong sÃ¡ch khÃ´ng thá»ƒ sá»­ dá»¥ng Ä‘Æ°á»£c ná»¯a nÃªn tá»¥i mÃ¬nh sáº½ táº£i dá»¯ liá»‡u tá»« Kaggle nha. ğŸ¶ğŸ“¥  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS4Qi4i0_CGA"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jamzhu/irishlyricseof\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnMJvtpT_GAc"
      },
      "outputs": [],
      "source": [
        "# # Liá»‡t kÃª cÃ¡c file bÃªn trong thÆ° má»¥c\n",
        "import os\n",
        "for f in os.listdir(path):\n",
        "  print(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exeWRN8l_XlM"
      },
      "outputs": [],
      "source": [
        "# Tiáº¿n hÃ nh táº£i bá»™ dá»¯ liá»‡u\n",
        "data = open(os.path.join(path,\"irish-lyrics-eof.txt\")).read()\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "# In thá»­ 5 hÃ ng Ä‘áº§u tiÃªn ra\n",
        "for i in range(5):\n",
        "  print(repr(corpus[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBR3Ohcf_4HD"
      },
      "outputs": [],
      "source": [
        "# Tiáº¿n hÃ nh tokenize dá»¯ liá»‡u\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1 # ThÃªm má»™t cho token padding sau nÃ y\n",
        "print(f\"Tá»•ng sá»‘ lÆ°á»£ng tá»«: {total_words}\")\n",
        "print(f\"Bá»™ tá»« Ä‘iá»ƒn tá»« vá»±ng: {tokenizer.word_index}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FARlEG1Anbb"
      },
      "outputs": [],
      "source": [
        "# Táº¡o bá»™ tá»« Ä‘iá»ƒn chuyá»ƒn dá»¯ liá»‡u sá»‘ thÃ nh chá»¯ láº¡i\n",
        "index_words ={}\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    index_words[index] = word\n",
        "\n",
        "# Tiáº¿n hÃ nh táº¡o cÃ¡c seed text hay chuá»—i con\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)-1):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Kiá»ƒm tra 5 chuá»—i mÃ£ hÃ³a sá»‘ Ä‘áº§u tiÃªn\n",
        "for i in range(5):\n",
        "  print(input_sequences[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMe2sgSUBlmN"
      },
      "outputs": [],
      "source": [
        "# Tiáº¿n hÃ nh padding cho dá»¯ liá»‡u\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "print(f\"Äá»™ dÃ i chuá»—i dÃ i nháº¥t: {max_sequence_len}\")\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "# In thá»­ 5 chuá»—i Ä‘áº§u tiÃªn\n",
        "for i in range(5):\n",
        "  print(input_sequences[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYB7Zl9vCFph"
      },
      "outputs": [],
      "source": [
        "# Tiáº¿n hÃ nh chia dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  nhÃ£n\n",
        "xs = input_sequences[:,:-1]\n",
        "labels = input_sequences[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSU79wB_CMV7"
      },
      "outputs": [],
      "source": [
        "# Thiáº¿t láº­p mÃ´ hÃ¬nh\n",
        "model1 = Sequential([\n",
        "    Embedding(total_words, 8),\n",
        "    Bidirectional(LSTM(max_sequence_len-1)),\n",
        "    Dense(total_words, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model1.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZO2GYa3CrXk"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "history1 = model1.fit(xs, labels, epochs=1000)\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZaULiFaibUA"
      },
      "outputs": [],
      "source": [
        "print(f\"Thá»i gian huáº¥n luyá»‡n: {timedelta(seconds=end_time-start_time)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeJHizKpCwFH"
      },
      "outputs": [],
      "source": [
        "# Váº½ biá»ƒu Ä‘á»“ quÃ¡ trÃ¬nh huáº¥n luyá»‡n\n",
        "import matplotlib.pyplot as plt\n",
        "fig, axs = plt.subplots(1, 2, figsize = (12, 4))\n",
        "train_acc = history1.history[\"accuracy\"]\n",
        "train_loss = history1.history[\"loss\"]\n",
        "axs[0].plot(train_acc)\n",
        "axs[0].set_title(\"Training Accuracy\")\n",
        "axs[1].plot(train_loss, color=\"orange\")\n",
        "axs[1].set_title(\"Training Loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffBTpv2Qnz5W"
      },
      "outputs": [],
      "source": [
        "model1.evaluate(xs,labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Kz5jVw561Gr"
      },
      "outputs": [],
      "source": [
        "model1.save(\"model1.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIwiMg7aGh-0"
      },
      "outputs": [],
      "source": [
        "# Táº£i láº¡i mÃ´ hÃ¬nh\n",
        "model1 = load_model(\"model1.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdfhGOIflUV3"
      },
      "source": [
        "Uáº§y, váº­y lÃ  vá»›i bá»™ dá»¯ liá»‡u nÃ y, Ä‘á»™ chÃ­nh xÃ¡c trÃªn táº­p huáº¥n luyá»‡n Ä‘áº¡t Ä‘Æ°á»£c lÃ  khoáº£ng 60%. ğŸ‰\n",
        "\n",
        "BÃ¢y giá» tá»¥i mÃ¬nh sáº½ tiáº¿n hÃ nh dá»± Ä‘oÃ¡n thá»­ vá»›i cÃ¡c váº¿ trÆ°á»›c Ä‘Ã³ nha. ğŸ”âœ¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEiQdO-tmmdW"
      },
      "outputs": [],
      "source": [
        "# Dá»± Ä‘oÃ¡n má»™t tá»«\n",
        "seed_text = \"in the town of athy\"\n",
        "token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') # -1 vÃ o max_sequence_len bá»Ÿi vÃ¬ chuá»—i nÃ y khÃ´ng cÃ³ nhÃ£n\n",
        "# Tiáº¿n hÃ nh dá»± Ä‘oÃ¡n\n",
        "predicted = model1.predict(token_list, verbose=1)\n",
        "predicted_index = np.argmax(predicted, axis = -1)\n",
        "predicted_word = index_words[predicted_index[0]]\n",
        "\n",
        "print(f\"Tá»« dá»± Ä‘oÃ¡n lÃ : '{predicted_word}' vá»›i xÃ¡c suáº¥t {predicted[0][predicted_index[0]]*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZZxqUJgoD_b"
      },
      "source": [
        "á» Ä‘Ã¢y káº¿t quáº£ mÃ¬nh ra Ä‘Æ°á»£c lÃ :  \n",
        "> Tá»« dá»± Ä‘oÃ¡n lÃ : **'light'** vá»›i xÃ¡c suáº¥t **20.35%** âœ¨  \n",
        "\n",
        "Má»i ngÆ°á»i cÃ³ thá»ƒ ra káº¿t quáº£ khÃ¡c nha, Ä‘iá»u nÃ y phá»¥ thuá»™c vÃ o trá»ng sá»‘ cá»§a mÃ´ hÃ¬nh mÃ  trá»ng sá»‘ cá»§a má»—i mÃ´ hÃ¬nh khi khá»Ÿi táº¡o lÃ  ngáº«u nhiÃªn, khÃ´ng giá»‘ng nhau nÃªn káº¿t quáº£ cuá»‘i cÅ©ng váº­y. Miá»…n sao Ã½ nghÄ©a cÃ¢u vÄƒn hay cÃ¡c tá»« cÃ³ pháº§n liÃªn káº¿t nhau há»£p lÃ½ lÃ  ok rá»“i. ğŸŒŸ  \n",
        "\n",
        "Tá»¥i mÃ¬nh sáº½ dá»± Ä‘oÃ¡n tiáº¿p vá»›i cá»¥m **â€œsweet jeremy saw dublinâ€**. ğŸš€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sGNrH4a3rA4"
      },
      "outputs": [],
      "source": [
        "# Dá»± Ä‘oÃ¡n má»™t chuá»—i\n",
        "seed_text = \"sweet jeremy saw dublin\"\n",
        "num_words = 10\n",
        "predicted_text = seed_text\n",
        "for i in range(num_words):\n",
        "  token_list = tokenizer.texts_to_sequences([predicted_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "  predicted = model1.predict(token_list, verbose=1)\n",
        "  predicted_index = np.argmax(predicted, axis=-1)\n",
        "  output_word = index_words[predicted_index[0]]\n",
        "\n",
        "  print(f\"Step {i}:\")\n",
        "  print(f\"VÄƒn báº£n Ä‘áº§u vÃ o: {predicted_text}\")\n",
        "  print(f\"Chuá»—i mÃ£ hÃ³a Ä‘áº§u vÃ o: {token_list[0]}\")\n",
        "  print(f\"Tá»« dá»± Ä‘oÃ¡n: {output_word} vá»›i xÃ¡c suáº¥t {predicted[0][predicted_index[0]]*100:2f}%\")\n",
        "  print(\"-\"*50)\n",
        "  predicted_text += \" \" + output_word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n3i2V3Osxlq"
      },
      "outputs": [],
      "source": [
        "print(f\"VÄƒn báº£n sinh ra: '{predicted_text}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NcgUOcUs5gb"
      },
      "source": [
        "ÄÃ¢y lÃ  káº¿t quáº£ á»Ÿ láº§n cháº¡y cá»§a mÃ¬nh:  \n",
        "> VÄƒn báº£n sinh ra: 'sweet jeremy saw dublin ever make a begging i neer could you dead and'âœ¨  \n",
        "\n",
        "CÃ³ váº» lÃ  chÃºng Ä‘Ã£ á»•n hÆ¡n rá»“i ha, sai sÃ³t thÃ¬ táº¥t nhiÃªn váº«n cÃ³ nhÆ°ng ngá»¯ nghÄ©a cáº¥u trÃºc Ä‘Ã£ á»•n hÆ¡n tÃ­ rá»“i. ğŸŒŸ BÃ¢y giá» tá»¥i mÃ¬nh sáº½ Ä‘i thá»­ thÃªm nhiá»u cÃ¡ch ná»¯a Ä‘á»ƒ tÄƒng Ä‘á»™ hiá»‡u quáº£ lÃªn hÆ¡n nha. ğŸš€  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqKd6g6HuzVR"
      },
      "source": [
        "# Changing the Modle Architecture\n",
        "### TrÆ°á»›c máº¯t thÃ¬ tá»¥i mÃ¬nh sáº½ thá»­ thay Ä‘á»•i kiáº¿n trÃºc mÃ´ hÃ¬nh xem sao."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Zj5H1WiuacY"
      },
      "outputs": [],
      "source": [
        "model2= Sequential([\n",
        "    Embedding(total_words, 8),\n",
        "    Bidirectional(LSTM(max_sequence_len-1, return_sequences=True)),\n",
        "    Bidirectional(LSTM(max_sequence_len-1)),\n",
        "    Dense(total_words, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model2.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GntMGZGKvoD7"
      },
      "outputs": [],
      "source": [
        "# Tiáº¿n hÃ nh huáº¥n luyá»‡n\n",
        "start_time = time.time()\n",
        "history2 = model2.fit(xs, labels, epochs = 1000)\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YizHpfmv3TX"
      },
      "outputs": [],
      "source": [
        "print(f\"Thá»i gian huáº¥n luyá»‡n mÃ´ hÃ¬nh: {timedelta(seconds=end_time - start_time)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN-m48Hlv_iw"
      },
      "outputs": [],
      "source": [
        "# Váº½ biá»ƒu Ä‘á»“ quÃ¡ trÃ¬nh huáº¥n luyá»‡n\n",
        "train_acc = history2.history[\"accuracy\"]\n",
        "train_loss = history2.history[\"loss\"]\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize = (12, 4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc)\n",
        "axs[0].set_title(\"Training Accuracy\")\n",
        "axs[1].plot(train_loss, color=\"orange\")\n",
        "axs[1].set_title(\"Training Loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C8952SpwUqb"
      },
      "outputs": [],
      "source": [
        "# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh\n",
        "model2.evaluate(xs, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y40Bspoj7Dll"
      },
      "outputs": [],
      "source": [
        "model2.save(\"model2.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_RapNTYGs-P"
      },
      "outputs": [],
      "source": [
        "# Táº£i láº¡i mÃ´ hÃ¬nh\n",
        "model2 = load_model(\"model2.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLXe5_SSwiVT"
      },
      "source": [
        "Tá»¥i mÃ¬nh sáº½ tiáº¿n hÃ nh dá»± Ä‘oÃ¡n láº¡i vá»›i cÃ¡c cÃ¢u trÆ°á»›c Ä‘Ã³"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYtBRNkTwZv2"
      },
      "outputs": [],
      "source": [
        "# Dá»± Ä‘oÃ¡n má»™t tá»«\n",
        "seed_text = \"in the town of athy\"\n",
        "token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') # -1 vÃ o max_sequence_len bá»Ÿi vÃ¬ chuá»—i nÃ y khÃ´ng cÃ³ nhÃ£n\n",
        "# Tiáº¿n hÃ nh dá»± Ä‘oÃ¡n\n",
        "predicted = model2.predict(token_list, verbose=1)\n",
        "predicted_index = np.argmax(predicted, axis = -1)\n",
        "predicted_word = index_words[predicted_index[0]]\n",
        "\n",
        "print(f\"Tá»« dá»± Ä‘oÃ¡n lÃ : '{predicted_word}' vá»›i xÃ¡c suáº¥t {predicted[0][predicted_index[0]]*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9nkVt5Kwo2o"
      },
      "outputs": [],
      "source": [
        "# Dá»± Ä‘oÃ¡n má»™t tá»«\n",
        "seed_text = \"sweet jeremy saw dublin\"\n",
        "num_words = 10\n",
        "predicted_text = seed_text\n",
        "for i in range(num_words):\n",
        "  token_list = tokenizer.texts_to_sequences([predicted_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "  predicted = model2.predict(token_list, verbose=1)\n",
        "  predicted_index = np.argmax(predicted, axis=-1)\n",
        "  output_word = index_words[predicted_index[0]]\n",
        "\n",
        "  print(f\"Step {i}:\")\n",
        "  print(f\"VÄƒn báº£n Ä‘áº§u vÃ o: {predicted_text}\")\n",
        "  print(f\"Chuá»—i mÃ£ hÃ³a Ä‘áº§u vÃ o: {token_list[0]}\")\n",
        "  print(f\"Tá»« dá»± Ä‘oÃ¡n: {output_word} vá»›i xÃ¡c suáº¥t {predicted[0][predicted_index[0]]*100:2f}%\")\n",
        "  print(\"-\"*50)\n",
        "  predicted_text += \" \" + output_word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBh1yyAOHlZ6"
      },
      "outputs": [],
      "source": [
        "print(f\"VÄƒn báº£n sinh ra: '{predicted_text}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNNKR4SZB9VG"
      },
      "source": [
        "# Improving the Data  \n",
        "### BÃ¢y giá» tá»¥i mÃ¬nh tiáº¿n hÃ nh thá»­ cÃ¡ch cáº£i thiá»‡n dá»¯ liá»‡u Ä‘áº§u vÃ o Ä‘á»ƒ cáº£i tiáº¿n mÃ´ hÃ¬nh nha, tÄƒng Ä‘á»™ hiá»‡u quáº£ nha. âœ¨  \n",
        "\n",
        "Sá»Ÿ dÄ© má»i ngÆ°á»i tháº¥y Ä‘Æ°á»ng **train_accuracy** nhiá»…u váº­y lÃ  vÃ¬ dá»¯ liá»‡u váº«n thiáº¿u Ä‘i pháº§n nÃ o Ä‘Ã³ sá»± liÃªn káº¿t ngá»¯ cáº£nh máº¡ch láº¡c. ğŸŒ  \n",
        "\n",
        "> **Váº­y nguyÃªn nhÃ¢n do Ä‘Ã¢u mÃ  ra?**  \n",
        "\n",
        "ChÃ­nh lÃ  do cÃ¡ch tiáº¿p cáº­n, chiáº¿n lÆ°á»£c chia, xá»­ lÃ½ dá»¯ liá»‡u cá»§a chÃºng ta trÆ°á»›c Ä‘Ã³. Trong má»™t bÃ i hÃ¡t thÃ¬ cÃ¡c cÃ¢u hÃ¡t Ä‘á»u cÃ³ sá»± liÃªn káº¿t ngá»¯ cáº£nh vá»›i nhau, Ä‘Ã³ cÅ©ng lÃ  cÃ¡ch mÃ  con ngÆ°á»i tiáº¿p thu. ğŸ¶ Tuy nhiÃªn khi Ä‘Æ°a dá»¯ liá»‡u vÃ o huáº¥n luyá»‡n cho mÃ´ hÃ¬nh, chÃºng ta Ä‘Ã£ coi **má»—i cÃ¢u hÃ¡t lÃ  má»™t dÃ²ng Ä‘á»™c láº­p** vÃ  tiáº¿n hÃ nh chia chuá»—i con. ChÃ­nh Ä‘iá»u nÃ y Ä‘Ã£ lÃ m cho cÃ¡c cÃ¢u hÃ¡t trá»Ÿ nÃªn máº¥t káº¿t ná»‘i ngá»¯ cáº£nh vá»›i nhau. ğŸ”—  \n",
        "\n",
        "> **Ok, xÃ¡c Ä‘á»‹nh Ä‘Æ°á»£c gá»‘c váº¥n Ä‘á» rá»“i váº­y chÃºng ta sáº½ lÃ m gÃ¬ tiáº¿p theo Ä‘Ã¢y?**  \n",
        "\n",
        "CÃ¢u tráº£ lá»i lÃ  lÃ m sao cho cÃ¡c chuá»—i vÄƒn báº£n huáº¥n luyá»‡n cÃ³ chá»©a má»™t pháº§n cá»§a cÃ¢u trÆ°á»›c vÃ  cÃ¢u sau, tá»« Ä‘Ã³ giá»¯a cÃ¡c cÃ¢u sáº½ cÃ³ sá»± liÃªn káº¿t vá»›i nhau. ğŸ§©  \n",
        "\n",
        "BÃ¢y giá» thay vÃ¬ Ä‘á»ƒ cÃ¡c cÃ¢u trong bÃ i hÃ¡t thÃ nh má»™t hÃ ng Ä‘á»™c láº­p, chÃºng ta sáº½ tiáº¿n hÃ nh **gom táº¥t cáº£ cÃ¡c cÃ¢u Ä‘Ã³ láº¡i vá» chung má»™t hÃ ng. ChÃºng ta sáº½ khÃ´ng chia theo kÃ½ tá»± xuá»‘ng hÃ ng \"\\n\" ná»¯a mÃ  lÃ  theo dáº¥u khoáº£ng cÃ¡ch**  ğŸš€  \n",
        "\n",
        "**Váº­y workflow cá»§a chÃºng ta sáº½ nhÆ° sau:**\n",
        "\n",
        "**1. Chia cÃ¡c cÃ¢u dá»±a trÃªn cá»­a sá»• trÆ°á»£t:** Chia Ä‘oáº¡n vÄƒn thÃ nh danh sÃ¡ch cÃ¡c tá»«, sau Ä‘Ã³ dÃ¹ng cá»­a sá»• trÆ°á»£t Ä‘á»ƒ táº¡o ra cÃ¡c cÃ¢u hay chuá»—i con. Qua Ä‘Ã³ ta cÃ³ thá»ƒ giá»¯ Ä‘Æ°á»£c sá»± liÃªn káº¿t ngá»¯ cáº£nh cá»§a cÃ¡c cÃ¢u vá»›i nhau.\n",
        "\n",
        "**2. Tiáº¿n hÃ nh chia cÃ¡c chuá»—i con tá»« cÃ¡c chuá»—i Ä‘Ã£ chia trÆ°á»›c Ä‘Ã³:** lÃ m giá»‘ng nhÆ° cÃ¡ch ban Ä‘Ã¢u tuy nhiÃªn thay vÃ¬ dá»¯ liá»‡u cá»§a chÃºng ta lÃ  cÃ¡c cÃ¢u Ä‘á»™c láº­p thÃ¬ giá» lÃ  cÃ¡c cÃ¢u Ä‘Ã£ Ä‘Æ°á»£c táº¡o ra á»Ÿ bÆ°á»›c 1.k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otAJ6Gxc5nlv"
      },
      "outputs": [],
      "source": [
        "# Kiá»ƒm tra láº¡i dá»¯ liá»‡u\n",
        "print(repr(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BRk1caBJyXg"
      },
      "source": [
        "Tiáº¿n hÃ nh chia cÃ¡c cÃ¢u, chuá»—i dá»±a trÃªn cá»­a sá»• trÆ°á»£t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWF2E1v958Eh"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "# Chia theo khoáº£ng cÃ¡ch thay vÃ¬ kÃ­ tá»± xuá»‘ng hÃ ng \"\\n\"\n",
        "window_size = 6\n",
        "\n",
        "# Tiáº¿n hÃ nh láº¥y chuÃ´i cÃ¡c tá»« trong vÄƒn báº£n\n",
        "corpus = data.lower()\n",
        "words = corpus.split(\" \")\n",
        "\n",
        "# Tiáº¿n hÃ nh chia cÃ¡c cÃ¢u (chuá»—i) dá»±a trÃªn cá»­a sá»• trÆ°á»£t\n",
        "range_size = len(words) - window_size + 1\n",
        "\n",
        "input_sentences = []\n",
        "for i in range(range_size):\n",
        "  input_sentences.append(\" \".join(words[i:i+window_size]))\n",
        "\n",
        "\n",
        "# Kiá»ƒm tra 5 cÃ¢u (chuá»—i) Ä‘áº§u tiÃªn\n",
        "for i in range(5):\n",
        "  print(repr(input_sentences[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnI46rvLt2Vk"
      },
      "source": [
        "**CÃ¡c cÃ¢u á»Ÿ chiáº¿n lÆ°á»£c chia dá»¯ liá»‡u cÅ©:**\n",
        ">come all ye maidens young and fair   \n",
        "and you that are blooming in your prime   \n",
        "always beware and keep your garden fair   \n",
        "let no man steal away your thyme   \n",
        "for thyme it is a precious thing   \n",
        "\n",
        "\n",
        "**CÃ¡c cÃ¢u chia theo phÆ°Æ¡ng phÃ¡p cá»­a sá»• trÆ°á»£t:**\n",
        ">come all ye maidens young and   \n",
        "all ye maidens young and fair\\nand   \n",
        "ye maidens young and fair\\nand you   \n",
        "maidens young and fair\\nand you that    \n",
        "young and fair\\nand you that are"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmh8M-COJrWT"
      },
      "source": [
        "BÃ¢y giá» tá»¥i mÃ¬nh Ä‘áº¿n vá»›i bÆ°á»›c 2, tiáº¿n hÃ nh mÃ£ hÃ³a vÃ  chia chuá»—i con nha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S6aJ6JGEp6N"
      },
      "outputs": [],
      "source": [
        "# Táº¡o tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(input_sentences)\n",
        "\n",
        "sub_sequences = []\n",
        "# Tiáº¿n hÃ nh mÃ£ hÃ³a vÃ  chia chuá»—i con\n",
        "for sentence in input_sentences:\n",
        "  token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
        "  for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    sub_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Kiá»ƒm tra 5 chuá»—i Ä‘áº§u tiÃªn\n",
        "for i in range(5):\n",
        "  print(sub_sequences[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFgORcrIHB10"
      },
      "outputs": [],
      "source": [
        "# Kiá»ƒm tra kÃ­ch thÆ°á»›c tá»« Ä‘iá»ƒn\n",
        "print(f\"Tá»•ng sá»‘ lÆ°á»£ng tá»«: {len(tokenizer.word_index)}\")\n",
        "print(f\"Bá»™ tá»« Ä‘iá»ƒn tá»« vá»±ng: {tokenizer.word_index}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_sA4k2iAxvj"
      },
      "outputs": [],
      "source": [
        "# Táº¡o tá»« Ä‘iá»ƒn chuyá»ƒn sá»‘ thÃ nh tá»«\n",
        "index_words ={}\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    index_words[index] = word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l6lBGl4HNUp"
      },
      "source": [
        "á» Ä‘Ã¢y cÃ³ má»™t lÆ°u Ã½ nhá»: vÃ¬ báº£n thÃ¢n tokenizer Ä‘Ã£ cÃ³ sáºµn má»™t bá»™ lá»c Ä‘á»ƒ cÃ¡c kÃ­ tá»± Ä‘áº·c biá»‡t nÃªn cÃ¡c tá»« ná»‘i liá»n bá»Ÿi kÃ­ tá»± Ä‘áº·c biá»‡t (vÃ­ dá»¥: \"mother-in-law\" sáº½ bá»‹ chuyá»ƒn thÃ nh \"mother in law\"). Do Ä‘Ã³ sáº½ cÃ³ má»™t sá»‘ chuá»—i cÃ³ kÃ­ch thÆ°á»›c lá»›n hÆ¡n so vá»›i cá»­a sá»• trÆ°á»£t. Ta sáº½ tiáº¿n hÃ nh tÃ­nh láº¡i Ä‘á»™ dÃ i chuá»—i dÃ i nháº¥t."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NXxTxfyHM1w"
      },
      "outputs": [],
      "source": [
        "max_sequence_len = max([len(x) for x in sub_sequences])\n",
        "print(f\"Äá»™ dÃ i chuá»—i dÃ i nháº¥t: {max_sequence_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ExmeFzg9qvd"
      },
      "source": [
        "Káº¿t quáº£ lÃ :\n",
        "> Äá»™ dÃ i chuá»—i dÃ i nháº¥t: 22\n",
        "\n",
        "NhÆ° chÃºng mÃ¬nh Ä‘Ã£ dá»± Ä‘oÃ¡n nhÆ°ng khÃ´ng ngá» kÃ­ch thÆ°á»›c nÃ³ cÃ³ thá»ƒ tÄƒng lÃªn nhiá»u váº­y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3zOa44BIbPr"
      },
      "outputs": [],
      "source": [
        "# Kiá»ƒm tra 5 chuá»—i cÃ³ Ä‘á»™ dÃ i lá»›n hÆ¡n cá»­a sá»• trÆ°á»£t.\n",
        "sequences_higher_than_window = []\n",
        "for i in range(len(sub_sequences)):\n",
        "  if len(sub_sequences[i]) > window_size:\n",
        "    sequences_higher_than_window.append(sub_sequences[i])\n",
        "\n",
        "for i in range(5):\n",
        "  print(sequences_higher_than_window[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGEiY9D1IxOA"
      },
      "source": [
        "BÃ¢y giá» tá»¥i mÃ¬nh sáº½ tiáº¿n hÃ nh padding Ä‘á»ƒ táº¥t cáº£ chuá»—i cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c, Ä‘á»™ dÃ i nha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ0JH5xpI6bS"
      },
      "outputs": [],
      "source": [
        "input_sequences = np.array(pad_sequences(sub_sequences, maxlen=max_sequence_len, padding=\"pre\"))\n",
        "\n",
        "# Chia dá»¯ liá»‡u x vÃ  labels Ä‘á»ƒ huáº¥n luyá»‡n.\n",
        "xs = input_sequences[:,:-1]\n",
        "labels = input_sequences[:,-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH_4wehACKF3"
      },
      "source": [
        "Tiáº¿n hÃ nh táº¡o dataset Ä‘á»ƒ tá»‘i Æ°u hÃ³a dá»¯ liá»‡u Ä‘áº©y vÃ o quÃ¡ trÃ¬nh huáº¥n luyá»‡n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E1MISaHXoem"
      },
      "outputs": [],
      "source": [
        "# Táº¡o datagen Ä‘á»ƒ xá»§ lÃ½ nhanh, tá»‘i Æ°u hÃ³a quÃ¡ trÃ¬nh huáº¥n luyá»‡n hÆ¡n\n",
        "# Táº¡o Ä‘á»‘i tÆ°á»£ng Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((xs, labels))\n",
        "\n",
        "# Tá»‘i Æ°u Dataset vá»›i shuffle, batch vÃ  prefetch\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "dataset = (\n",
        "    dataset\n",
        "    .cache(\"train_cache.tfdata\")    # Thay vÃ¬ lÆ°u dá»¯ liá»‡u vÃ o ram thÃ¬ lÆ°u háº³n vÃ o disk\n",
        "    .shuffle(buffer_size=1000)      # Trá»™n dá»¯ liá»‡u\n",
        "    .batch(32)                      # Chia batch (batch_size=32)\n",
        "    .prefetch(buffer_size=AUTOTUNE) # Táº£i trÆ°á»›c dá»¯ liá»‡u\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPZykQp2nwWX"
      },
      "source": [
        "MÃ¬nh sáº½ tiáº¿n hÃ nh Ä‘á»‹nh nghÄ©a kiáº¿n trÃºc mÃ´ hÃ¬nh theo Ä‘Ãºng láº§n thá»­ cá»§a tÃ¡c giáº£, má»i ngÆ°á»i cÃ³ thá»ƒ linh hoáº¡t thay Ä‘á»•i cÃ¡c siÃªu tham sá»‘ liÃªn tá»¥c Ä‘á»ƒ cáº£i thiá»‡n nha.  \n",
        "\n",
        "TÃ¡c giáº£ sau khi thá»­ nhiá»u láº§n thÃ¬ cÃ³ Ä‘á» xuáº¥t:  \n",
        "- **window_size**: 6  \n",
        "- **sá»‘ chiá»u embedding**: 16  \n",
        "- **sá»‘ unit trong LSTMs**: 32  \n",
        "- **tÄƒng tá»‘c Ä‘á»™ há»c-learning rate lÃªn**: tÃ¡c giáº£ khÃ´ng nÃ³i rÃµ nÃªn mÃ¬nh táº¡m láº¥y **0.005** Ä‘i. Sá»‘ máº·c Ä‘á»‹nh lÃ  **0.001**  \n",
        "- **epochs**: 100 bá»Ÿi sá»‘ lÆ°á»£ng tham sá»‘ tÃ­nh toÃ¡n nÃ y lÃ  ráº¥t lá»›n. ğŸš€  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JS-L_59IaBRY"
      },
      "outputs": [],
      "source": [
        "total_words = len(tokenizer.word_index) + 1 # vÃ¬ tá»« Ä‘iá»ƒn báº¯t Ä‘áº§u tá»« 1, sá»‘ 0 Ä‘Æ°á»£c thÃªm vÃ o nhÆ° padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SBGyebYQuGT"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  model3 = Sequential([\n",
        "      Embedding(total_words, 16),\n",
        "      Bidirectional(LSTM(32, return_sequences=True)),\n",
        "      Bidirectional(LSTM(32)),\n",
        "      Dense(total_words, activation=\"softmax\")\n",
        "  ])\n",
        "  adam = Adam(learning_rate=0.01)\n",
        "  model3.compile(loss = 'sparse_categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4XR-DCiRE17"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "history3 = model3.fit(dataset, epochs = 100)\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17RgwQEFRHft"
      },
      "outputs": [],
      "source": [
        "print(f\"Thá»i gian huáº¥n luyá»‡n mÃ´ hÃ¬nh: {timedelta(seconds=end_time - start_time)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAdcyVxpRI8p"
      },
      "outputs": [],
      "source": [
        "# Váº½ biá»ƒu Ä‘á»“ huáº¥n luyá»‡n\n",
        "train_acc = history3.history[\"accuracy\"]\n",
        "train_loss = history3.history[\"loss\"]\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize = (12, 4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc)\n",
        "axs[0].set_title(\"Training Accuracy\")\n",
        "axs[1].plot(train_loss, color=\"orange\")\n",
        "axs[1].set_title(\"Training Loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABtvvjgqRMzE"
      },
      "outputs": [],
      "source": [
        "# ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh\n",
        "model3.evaluate(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJQQRopD7Qbg"
      },
      "outputs": [],
      "source": [
        "model3.save(\"model3.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT-9aZxQHBSv"
      },
      "outputs": [],
      "source": [
        "# Táº£i láº¡i mÃ´ hÃ¬nh\n",
        "model3 = load_model(\"model3.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eA1ZzADROy_"
      },
      "outputs": [],
      "source": [
        "# Tiáº¿n hÃ nh dá»± Ä‘oÃ¡n\n",
        "seed_text = \"sweet jeremy saw dublin\"\n",
        "num_words = 100\n",
        "predicted_text = seed_text\n",
        "\n",
        "# VÃ¬ kÃ­ch thÆ°á»›c Ä‘áº§u vÃ o Ä‘Ã£ thay Ä‘á»•i theo kÃ­ch thÆ°á»›c cá»­a sá»•\n",
        "# nÃªn ta cÅ©ng thay Ä‘á»•i pháº§n cáº¯t dá»¯ liá»‡u Ä‘Æ°a vÃ o tÃ­\n",
        "for i in range(num_words):\n",
        "  # KÃ­ch thÆ°á»›c cá»­a sá»• cáº¯t ra ban Ä‘áº§u lÃ  6 nhÆ°ng pháº£i chia tá»« cuá»‘i cho nhÃ£n nÃªn Ä‘áº§u vÃ o lÃ  5\n",
        "  token_list = tokenizer.texts_to_sequences([predicted_text])[0]\n",
        "  # Tiáº¿n hÃ nh kiá»ƒm tra luÃ´n náº¿u kÃ­ch thÆ°á»›c khÃ´ng Ä‘á»§ thÃ¬ padding vÃ o hoáº·c dÃ i quÃ¡ thÃ¬ cÃ³ thá»ƒ cáº¯t Ä‘i\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding=\"pre\")\n",
        "  token_list = np.array(token_list)\n",
        "  predicted_result = model3.predict(token_list)\n",
        "  predicted_index = np.argmax(predicted_result, axis=-1)\n",
        "  output_word = index_words.get(predicted_index[0], \"\") # Náº¿u index khÃ´ng cÃ³ trong tá»« Ä‘iá»ƒn, tráº£ vá» kÃ­ tá»± rá»—ng nhÆ° padding\n",
        "\n",
        "  print(f\"Step {i}:\")\n",
        "  print(f\"Tá»« dá»± Ä‘oÃ¡n: {output_word} vá»›i xÃ¡c suáº¥t {predicted_result[0][predicted_index[0]]*100:2f}%\")\n",
        "  print(\"-\"*50)\n",
        "  predicted_text += \" \" + output_word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PykF8WrdTJ05"
      },
      "outputs": [],
      "source": [
        "print(f\"VÄƒn báº£n sinh ra: \\n{predicted_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkZ0nLZ8diuv"
      },
      "source": [
        "# Character-Based Encoding  \n",
        "### Tá»¥i mÃ¬nh sáº½ tiáº¿p cáº­n vá»›i viá»‡c mÃ£ hÃ³a á»Ÿ cáº¥p kÃ­ tá»± nha.  \n",
        "\n",
        "Náº¿u nhÆ° á»Ÿ cÃ¡c chÆ°Æ¡ng trÆ°á»›c chÃºng ta thÆ°á»ng hay sá»­ dá»¥ng mÃ£ hÃ³a á»Ÿ cáº¥p tá»« thÃ¬ á»Ÿ chÆ°Æ¡ng nÃ y chÃºng ta cáº§n pháº£i xem xÃ©t láº¡i. Vá»›i má»™t lÆ°á»£ng vÄƒn báº£n khá»•ng lá»“ nhÆ° nÃ y, kÃ­ch thÆ°á»›c bá»™ tá»« Ä‘iá»ƒn tá»« cÅ©ng sáº½ ráº¥t lÃ  lá»›n. Khi Ä‘áº¿n vá»›i tÃ¡c vá»¥ táº¡o sinh vÄƒn báº£n, cháº³ng pháº£i Ä‘áº§u cá»§a mÃ´ hÃ¬nh báº±ng vá»›i kÃ­ch thÆ°á»›c bá»™ tá»« Ä‘iá»ƒn vÃ  nÃ³ tháº­t sá»± lÃ  má»™t con sá»‘ choÃ¡ng ngá»£p.  \n",
        "\n",
        "Trong khi Ä‘Ã³ náº¿u ta sá»­ dá»¥ng mÃ£ hÃ³a á»Ÿ cáº¥p kÃ­ tá»± thÃ¬ kÃ­ch thÆ°á»›c bá»™ tá»« Ä‘iá»ƒn sáº½ nhá» hÆ¡n ráº¥t nhiá»u, Ä‘áº§u ra mÃ  mÃ´ hÃ¬nh cÅ©ng Ã­t hÆ¡n. Tháº­m chÃ­ tháº¥p hÆ¡n cáº£ trÄƒm láº§n. âœ¨  \n",
        "\n",
        "**VD**: ChÃºng ta cÃ³ má»™t bá»™ dá»¯ liá»‡u vÄƒn báº£n:  \n",
        "- Khi mÃ£ hÃ³a á»Ÿ cáº¥p tá»«, bá»™ tá»« Ä‘iá»ƒn cá»§a chÃºng ta cÃ³ kÃ­ch thÆ°á»›c lÃ  **2700** tÆ°Æ¡ng á»©ng vá»›i 2700 tá»« riÃªng biá»‡t.  \n",
        "- Tuy nhiÃªn khi mÃ£ hÃ³a á»Ÿ cáº¥p kÃ­ tá»± thÃ¬ kÃ­ch thÆ°á»›c tá»« Ä‘iá»ƒn chá»‰ cÃ²n **65**, nhá» hÆ¡n ráº¥t lÃ  nhiá»u, mÃ´ hÃ¬nh cÅ©ng chá»‰ cáº§n dá»± Ä‘oÃ¡n Ä‘áº§u ra lÃ  **65 nhÃ£n** thay vÃ¬ **2700** nhÆ° trÆ°á»›c.  \n",
        "\n",
        "Qua cÃ¡ch mÃ£ hÃ³a dá»¯ liá»‡u trÃªn thÃ¬ mÃ´ hÃ¬nh cá»§a chÃºng ta sáº½ Ä‘Æ¡n giáº£n hÆ¡n ráº¥t nhiá»u, ngoÃ i ra cÅ©ng cÃ³ thá»ƒ chá»©a cÃ¡c kÃ­ tá»± Ä‘áº·c biá»‡t. ğŸš€  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnwJB1vtLunZ"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"kewagbln/shakespeareonline\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdZ1qbPmMRSH"
      },
      "outputs": [],
      "source": [
        "# Liá»‡t kÃª danh sÃ¡ch file\n",
        "for f in os.listdir(path):\n",
        "  print(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REwsNOH6MIac"
      },
      "outputs": [],
      "source": [
        "# Load dá»¯ liá»‡u vÃ o\n",
        "data = open(os.path.join(path,\"t8.shakespeare.txt\"), \"r\").read()\n",
        "# Kiá»ƒm tra dá»¯ liá»‡u\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iMBBlmNMfQ1"
      },
      "outputs": [],
      "source": [
        "# Tiáº¿n hÃ nh chia cÃ¢u dá»±a trÃªn cá»­a sá»• trÆ°á»£t\n",
        "window_size = 17\n",
        "corpus = data.lower()\n",
        "\n",
        "# Láº¥y danh sÃ¡ch kÃ­ tá»±\n",
        "characters = list(corpus)\n",
        "# print(characters)\n",
        "\n",
        "# Chia cÃ¢u (chuá»—i) dá»±a trÃªn cá»­a sá»• trÆ°á»£t\n",
        "range_size = len(characters) - window_size + 1\n",
        "input_sentences = []\n",
        "for i in range(range_size):\n",
        "  input_sentences.append(\"\".join(characters[i:i+window_size]))\n",
        "\n",
        "# Kiá»ƒm tra 5 cÃ¢u (chuá»—i) Ä‘áº§u tiÃªn:\n",
        "for i in range(5):\n",
        "  print(repr(input_sentences[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BunmodZQNtEY"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Tiáº¿n hÃ nh táº¡o bá»™ mÃ£ hÃ³a cáº¥p kÃ­ tá»± vÃ  chia chuá»—i con\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(input_sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PB_cfGIUNj7"
      },
      "outputs": [],
      "source": [
        "print(f\"Tá»•ng sá»‘ lÆ°á»£ng kÃ­ tá»±: {len(tokenizer.word_index)}\")\n",
        "print(f\"Bá»™ tá»« Ä‘iá»ƒn tá»« vá»±ng: {tokenizer.word_index}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BItmUKP7sRqC"
      },
      "outputs": [],
      "source": [
        "# Táº¡o bá»™ tá»« Ä‘iá»ƒn chuyá»ƒn Ä‘á»•i\n",
        "index_words = {}\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  index_words[index] = word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWMH-WSeYNXk"
      },
      "source": [
        "VÃ¬ váº¥n Ä‘á» ram khÃ´ng Ä‘á»§, lÆ°á»£ng dá»¯ liá»‡u quÃ¡ lá»›n nÃªn mÃ¬nh tiáº¿n hÃ nh lÆ°u tá»«ng dá»¯ liá»‡u mÃ£ hÃ³a vÃ o má»™t file pickle nha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKcsonfbUPcw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Táº¡o file táº¡m Ä‘á»ƒ lÆ°u dá»¯ liá»‡u\n",
        "temp_file = \"sub_sequences_temp.pkl\"\n",
        "max_sequence_len = 0\n",
        "\n",
        "with open(temp_file, \"wb\") as f:\n",
        "  for sentence in input_sentences:\n",
        "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
        "    if len(token_list) > max_sequence_len:\n",
        "      max_sequence_len = len(token_list)\n",
        "    for i in range(1, len(token_list)):\n",
        "      n_gram_sequence = token_list[:i+1]\n",
        "      pickle.dump(n_gram_sequence, f)\n",
        "\n",
        "print(f\"Äá»™ dÃ i chuá»—i dÃ i nháº¥t: {max_sequence_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVhsqWwaeTGI"
      },
      "source": [
        " Sau khi tiáº¿n hÃ nh lÆ°u má»™t file pickle xong, ta sáº½ tiáº¿n hÃ nh xá»­ lÃ½ theo tá»«ng batch, tuy lÃ  viá»‡c nÃ y sáº½ tá»‘n thá»i gian hÆ¡n ráº¥t nhiá»u nhÆ°ng Ä‘Ã¢y lÃ  cÃ¡ch duy nháº¥t vÃ¬ lÆ°á»£ng dá»¯ liá»‡u quÃ¡ lá»›n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxHjXDFme2Gw"
      },
      "outputs": [],
      "source": [
        "temp_file = \"sub_sequences_temp.pkl\"\n",
        "def read_sequences_in_batches(file_path, batch_size):\n",
        "    \"\"\"\n",
        "    HÃ m generator Ä‘á»c dá»¯ liá»‡u tá»« file Pickle theo tá»«ng batch.\n",
        "    \"\"\"\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        batch = []\n",
        "        while True:\n",
        "            try:\n",
        "                # Äá»c tá»«ng chuá»—i\n",
        "                n_gram_sequence = pickle.load(f)\n",
        "                batch.append(n_gram_sequence)\n",
        "                # Náº¿u Ä‘á»§ batch size, yield batch vÃ  khá»Ÿi táº¡o láº¡i\n",
        "                if len(batch) == batch_size:\n",
        "                    yield batch\n",
        "                    batch = []\n",
        "            except EOFError:\n",
        "                # Tráº£ vá» batch cuá»‘i cÃ¹ng náº¿u cÃ²n dá»¯ liá»‡u\n",
        "                if batch:\n",
        "                    yield batch\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkcQWUNPe9MR"
      },
      "outputs": [],
      "source": [
        "def process_batch(batch, max_seq_length):\n",
        "    \"\"\"\n",
        "    Pad cÃ¡c chuá»—i vÃ  tÃ¡ch X (input), labels (output).\n",
        "    \"\"\"\n",
        "    # Pad cÃ¡c chuá»—i Ä‘áº¿n chiá»u dÃ i tá»‘i Ä‘a\n",
        "    padded_batch = pad_sequences(batch, maxlen=max_seq_length, padding='pre')\n",
        "\n",
        "    # TÃ¡ch X vÃ  labels\n",
        "    X = padded_batch[:, :-1]  # Táº¥t cáº£ trá»« pháº§n tá»­ cuá»‘i\n",
        "    labels = padded_batch[:, -1]  # Pháº§n tá»­ cuá»‘i cÃ¹ng lÃ m nhÃ£n\n",
        "    return X, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NzkFZtefsmT"
      },
      "outputs": [],
      "source": [
        "def data_generator(file_path, batch_size, max_seq_length):\n",
        "    for batch in read_sequences_in_batches(file_path, batch_size):\n",
        "        X, labels = process_batch(batch, max_seq_length)\n",
        "        yield X, labels\n",
        "\n",
        "batch_size = 16\n",
        "dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(temp_file, batch_size, max_sequence_len),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, max_sequence_len - 1), dtype=tf.int32),  # X\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.int32)  # labels\n",
        "    )\n",
        ")\n",
        "\n",
        "# Prefetch Ä‘á»ƒ tÄƒng hiá»‡u suáº¥t\n",
        "dataset = dataset.cache(\"train_cache.tfdata\").prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Z6XxkOd7NCM"
      },
      "outputs": [],
      "source": [
        "# Kiá»ƒm tra sá»‘ lÆ°á»£ng kÃ­ tá»± tá»‘i Ä‘a\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(f\"Sá»‘ lÆ°á»£ng kÃ­ tá»±: {total_words}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgB-oFNmiFwK"
      },
      "source": [
        "Uáº§y váº­y lÃ  sá»‘ lÆ°á»£ng nhÃ£n Ä‘áº§u ra mÃ  mÃ´ hÃ¬nh pháº£i dá»± Ä‘oÃ¡n Ä‘Ã£ tháº¥p hÆ¡n ráº¥t nhiá»u rá»“i, chá»‰ 66 kÃ­ tá»±. BÃ¢y giá» tá»¥i mÃ¬nh tiáº¿n hÃ nh Ä‘á»‹nh nghÄ©a vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh thá»­ nha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQfaeQaz72L-"
      },
      "outputs": [],
      "source": [
        "# Äá»‹nh nghÄ©a mÃ´ hÃ¬nh\n",
        "with strategy.scope():\n",
        "  model4 = Sequential([\n",
        "      Embedding(total_words, 16),\n",
        "      Bidirectional(LSTM(32, return_sequences=True)),\n",
        "      Bidirectional(LSTM(32)),\n",
        "      Dense(total_words, activation=\"softmax\")\n",
        "  ])\n",
        "  adam = Adam(learning_rate=0.01)\n",
        "  model4.compile(loss = 'sparse_categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0egK2V0mA5ts"
      },
      "outputs": [],
      "source": [
        "model4.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I7MPVbZS8AMA"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "history4 = model4.fit(dataset, epochs = 100)\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al72VLFa8SME"
      },
      "outputs": [],
      "source": [
        "print(f\"Thá»i gian huáº¥n luyá»‡n mÃ´ hÃ¬nh: {timedelta(seconds=end_time - start_time)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5xK6oNW8UUS"
      },
      "outputs": [],
      "source": [
        "# Váº½ biá»ƒu Ä‘á»“ quÃ¡ trÃ¬nh huáº¥n luyá»‡n\n",
        "train_acc = history4.history[\"accuracy\"]\n",
        "train_loss = history4.history[\"loss\"]\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize = (12, 4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc)\n",
        "axs[0].set_title(\"Training Accuracy\")\n",
        "axs[1].plot(train_loss, color=\"orange\")\n",
        "axs[1].set_title(\"Training Loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSSahWnHA4yt"
      },
      "outputs": [],
      "source": [
        "model4.eval(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkqHVuCl7Wgt"
      },
      "outputs": [],
      "source": [
        "model4.save(\"model4.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y6lTIs2AQ2U"
      },
      "outputs": [],
      "source": [
        "# Cáº­p nháº­t trá»ng sá»‘ tá»« mÃ´ hÃ¬nh Ä‘Ã£ lÆ°u\n",
        "model4 = tf.keras.models.load_model('model4.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcJsdx5WE1JU"
      },
      "outputs": [],
      "source": [
        "# Tiáº¿n hÃ nh dá»± Ä‘oÃ¡n\n",
        "seed_text = \"sweet jeremy saw dublin\"\n",
        "num_words = 500\n",
        "predicted_text = seed_text\n",
        "\n",
        "# VÃ¬ kÃ­ch thÆ°á»›c Ä‘áº§u vÃ o Ä‘Ã£ thay Ä‘á»•i theo kÃ­ch thÆ°á»›c cá»­a sá»•\n",
        "# nÃªn ta cÅ©ng thay Ä‘á»•i pháº§n cáº¯t dá»¯ liá»‡u Ä‘Æ°a vÃ o tÃ­\n",
        "for i in range(num_words):\n",
        "  # KÃ­ch thÆ°á»›c cá»­a sá»• cáº¯t ra ban Ä‘áº§u lÃ  6 nhÆ°ng pháº£i chia tá»« cuá»‘i cho nhÃ£n nÃªn Ä‘áº§u vÃ o lÃ  5\n",
        "  token_list = tokenizer.texts_to_sequences([predicted_text])[0]\n",
        "  # Tiáº¿n hÃ nh kiá»ƒm tra luÃ´n náº¿u kÃ­ch thÆ°á»›c khÃ´ng Ä‘á»§ thÃ¬ padding vÃ o hoáº·c dÃ i quÃ¡ thÃ¬ cÃ³ thá»ƒ cáº¯t Ä‘i\n",
        "  token_list = pad_sequences([token_list], maxlen=window_size-1, padding=\"pre\")\n",
        "  token_list = np.array(token_list)\n",
        "  predicted_result = model4.predict(token_list)\n",
        "  predicted_index = np.argmax(predicted_result, axis=-1)\n",
        "  output_char = index_words[predicted_index[0]]\n",
        "\n",
        "  print(f\"Step {i}:\")\n",
        "  print(f\"Tá»« dá»± Ä‘oÃ¡n: {output_char} vá»›i xÃ¡c suáº¥t {predicted_result[0][predicted_index[0]]*100:2f}%\")\n",
        "  print(\"-\"*50)\n",
        "  predicted_text += \" \" + output_char\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7UaPsBUc5vo"
      },
      "outputs": [],
      "source": [
        "print(\"VÄƒn báº£n sinh ra:\")\n",
        "print(predicted_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 115588,
          "sourceId": 276428,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6378565,
          "sourceId": 10304562,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 2489101,
          "sourceId": 4222990,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30822,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}