{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/Chapter_8_Using_Tensorflow_to_Create_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yooo41ZlEgpM"
      },
      "source": [
        "# Chapter 8: Using Tensorflow to Create Text  \n",
        "### Hi, ch√†o m·ªçi ng∆∞·ªùi, t·ª•i m√¨nh l·∫°i g·∫∑p nhau r·ªìi üåª.  \n",
        "### Trong ch∆∞∆°ng n√†y t·ª•i m√¨nh s·∫Ω t√¨m hi·ªÉu v·ªÅ c√°ch s·ª≠ d·ª•ng c√°c m√¥ h√¨nh ƒë·ªÉ t·∫°o ra vƒÉn b·∫£n. N√≥i n√¥m na l√† gi·ªëng nh∆∞ c√°ch m√† c√°c chatbot hi·ªán t·∫°i t·∫°o ra c√¢u tr·∫£ l·ªùi cho m·ªçi ng∆∞·ªùi √°. Nh∆∞ng c√°i n√†y ·ªü c·∫•p ƒë·ªô s∆° khai h∆°n. üòä  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrzNuwTVGAiL"
      },
      "source": [
        "![predictext](https://cgupta.tech/images/rnn_representative.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OpPX-_LILXF"
      },
      "source": [
        "Nh∆∞ m·ªçi ng∆∞·ªùi th·∫•y ·ªü b·ª©c h√¨nh tr√™n, ƒë√¢y l√† c∆° ch·∫ø ch√≠nh c·ªßa t√≠nh nƒÉng t·∫°o vƒÉn b·∫£n trong ch∆∞∆°ng n√†y.  \n",
        "\n",
        "Tr∆∞·ªõc khi ƒëi v√†o ph√¢n t√≠ch n√≥ th√¨ ch√∫ng ta c√πng √¥n l·∫°i c√°c ch∆∞∆°ng tr∆∞·ªõc t√≠ nh√°. üòä  \n",
        "\n",
        "---\n",
        "\n",
        "- ·ªû **ch∆∞∆°ng 5**, m·ªçi ng∆∞·ªùi t√¨m hi·ªÉu v·ªÅ c∆° ch·∫ø **Tokenize** ƒë·ªÉ t√°ch vƒÉn b·∫£n, t·∫°o t·ª´ ƒëi·ªÉn, m√£ h√≥a ·ªü c·∫•p t·ª´ hay k√Ω t·ª± r·ªìi th√™m b·ªô ƒë·ªám hay padding v√†o.  \n",
        "\n",
        "- Sang ƒë·∫øn **ch∆∞∆°ng 6**, ch√∫ng ta t√¨m hi·ªÉu v·ªÅ c∆° ch·∫ø **Embedding - Vector bi·ªÉu di·ªÖn** tr√™n c√°c chi·ªÅu kh√¥ng gian cao h∆°n.  \n",
        "\n",
        "- V√† g·∫ßn ƒë√¢y nh·∫•t l√† **ch∆∞∆°ng 7**, t√¨m hi·ªÉu v·ªÅ m·∫°ng h·ªìi quy **RNN** v√† l·ªõp **LSTM** qua ƒë√≥ c·∫£i thi·ªán kh·∫£ nƒÉng hi·ªÉu ng·ªØ nghƒ©a theo tr√¨nh t·ª± c·ªßa chu·ªói.  \n",
        "\n",
        "B√¢y gi·ªù ƒë·∫øn v·ªõi **ch∆∞∆°ng 8**, t·ª•i m√¨nh s·∫Ω t√¨m hi·ªÉu v·ªÅ c∆° ch·∫ø **t·∫°o/sinh vƒÉn b·∫£n** c·ªßa c√°c m√¥ h√¨nh. Th·∫≠t ra n√≥ kh√¥ng kh√≥ nh∆∞ m·ªçi ng∆∞·ªùi nghƒ© m√† r·∫•t l√† ƒë∆°n gi·∫£n. Ch√∫ng ƒë·ªÅu d·ª±a tr√™n nh·ªØng c∆° ch·∫ø tr∆∞·ªõc gi·ªù ch√∫ng ta h·ªçc, tuy nhi√™n kh√°c ·ªü kh√¢u x·ª≠ l√Ω v√† bi·∫øn ƒë·ªïi d·ªØ li·ªáu t√≠.  \n",
        "\n",
        "V·∫≠y s·ª± kh√°c nhau n√†y l√† g√¨, t·ª•i m√¨nh s·∫Ω c√πng t√¨m hi·ªÉu nh√°. üåü  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYXNgZabNqiA"
      },
      "source": [
        "![process_text](https://i.imgur.com/7kOLuRM.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGLvJdjeOSzl"
      },
      "source": [
        "![padding_text](https://i.imgur.com/QOcbcqC.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVQotDeuLNQc"
      },
      "source": [
        "Trong c√°c b√†i to√°n tr∆∞·ªõc ƒë√¢y c·ªßa ch√∫ng ta, t·ª•i m√¨nh s·∫Ω ti·∫øn h√†nh **x·ª≠ l√Ω d·ªØ li·ªáu vƒÉn b·∫£n sang d·∫°ng s·ªë** v√† **ph√¢n lo·∫°i ch√∫ng** d·ª±a tr√™n **nh√£n c√≥ s·∫µn** c·ªßa d·ªØ li·ªáu hu·∫•n luy·ªán tr∆∞·ªõc ƒë√≥. üìä  \n",
        "\n",
        "Th∆∞·ªùng th√¨ **s·ªë l∆∞·ª£ng nh√£n kh√° √≠t**, v√≠ d·ª• nh∆∞ ch·ªâ c√≥ hai nh√£n **\"t√≠ch c·ª±c\"** v√† **\"ti√™u c·ª±c\"** trong b√†i to√°n ph√¢n t√≠ch c·∫£m x√∫c, ho·∫∑c v√†i nh√£n c·ª• th·ªÉ trong b√†i to√°n ph√¢n lo·∫°i vƒÉn b·∫£n. üè∑Ô∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUEX7M8yOlLt"
      },
      "source": [
        "![classify_sentiment](https://media.mlhive.com/i/max/YbAcdK8b2Q4T5Ed4fIf.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vld5yzwhOtHV"
      },
      "source": [
        "Gi·ªù ƒë√¢y ƒë·∫øn v·ªõi **b√†i to√°n t·∫°o/sinh vƒÉn b·∫£n**, thay v√¨ **ph√¢n lo·∫°i v·ªõi c√°c nh√£n c·ªë ƒë·ªãnh**, ch√∫ng ta s·∫Ω **d·ª± ƒëo√°n t·ª´ ti·∫øp theo** c·ªßa vƒÉn b·∫£n ƒë√≥. üìù‚ú®  \n",
        "\n",
        "> **∆†, v·∫≠y nh√£n c·ªßa c√°c vƒÉn b·∫£n s·∫Ω ch√≠nh l√† t·ª´ ti·∫øp theo c·ªßa ch√∫ng √†?**  \n",
        "\n",
        "Yeah, ch√≠nh x√°c r·ªìi ƒë√≥! üåü"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![example](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_8/input_labels.png?raw=true)"
      ],
      "metadata": {
        "id": "AGDKDXvhzsh_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ORPU7SuQe2p"
      },
      "source": [
        "> **√îi, th·∫ø s·ªë l∆∞·ª£ng nh√£n c·ªßa m√¥ h√¨nh ch·∫≥ng ph·∫£i s·∫Ω r·∫•t kh·ªïng l·ªì hay sao Ôºà‚äôÔΩè‚äôÔºâ?**  \n",
        "\n",
        "Yeah, b·∫°n l·∫°i ƒëo√°n ƒë√∫ng r·ªìi. üéâ  \n",
        "\n",
        "S·ªë l∆∞·ª£ng nh√£n ch√≠nh l√† **s·ªë t·ª´ trong b·ªô t·ª´ ƒëi·ªÉn** √° =))  \n",
        "C√≥ khi n√≥ l√™n ƒë·∫øn t·∫≠n **10.000** v√† th∆∞·ªùng s·ªë l∆∞·ª£ng epochs hu·∫•n luy·ªán c≈©ng r·∫•t l·ªõn, c√≥ th·ªÉ t·ª´ **1.000** tr·ªü l√™n.  \n",
        "\n",
        "> **V·∫≠y ch√∫ng t·∫°o ra vƒÉn b·∫£n nh∆∞ th·∫ø n√†o?**  \n",
        "\n",
        "ƒê∆°n gi·∫£n th√¥i, t·ª•i m√¨nh s·∫Ω ti·∫øn h√†nh **l·∫∑p ƒëi l·∫∑p l·∫°i qu√° tr√¨nh d·ª± ƒëo√°n t·ª´ ti·∫øp theo**. üîÑ  \n",
        "Khi m·ªçi ng∆∞·ªùi d·ª± ƒëo√°n ƒë∆∞·ª£c m·ªôt t·ª´ m·ªõi, ch√∫ng ta s·∫Ω l·∫•y t·ª´ ƒë√≥ **k·∫øt h·ª£p l·∫°i v·ªõi vƒÉn b·∫£n ƒë·∫ßu v√†o** v√† ti·∫øp t·ª•c d·ª± ƒëo√°n t·ª´ k·∫ø ti·∫øp.  \n",
        "C·ª© ti·∫øn h√†nh nh∆∞ th·∫ø cho ƒë·∫øn khi h·∫øt s·ªë v√≤ng l·∫∑p ho·∫∑c ƒë·∫°t gi·ªõi h·∫°n s·ªë l∆∞·ª£ng t·ª´ d·ª± ƒëo√°n. üåü  \n",
        "\n",
        "N√≥ gi·ªëng h·ªát v·ªõi h√¨nh m√† t·ª•i m√¨nh ƒë√£ nh√¨n th·∫•y ·ªü ph·∫ßn ƒë·∫ßu ƒë√≥. üìÑ‚ú®  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_dLy8wRTkuX"
      },
      "source": [
        "![predictext](https://cgupta.tech/images/rnn_representative.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_znzuVmHTl3J"
      },
      "source": [
        "·ªû ƒë√¢y, c√¢u vƒÉn ban ƒë·∫ßu c·ªßa ch√∫ng ta l√†: **\"the man is walking\"**, ph·∫ßn m√†u xanh d∆∞∆°ng l√† **c·ª≠a s·ªï tr∆∞·ª£t - window data** ƒë·ªÉ c·∫Øt l·∫•y d·ªØ li·ªáu ƒë·∫ßu v√†o v·ªõi ƒë·ªô d√†i t·ªëi ƒëa hi·ªán t·∫°i l√† **4 t·ª´**. Do ƒë√≥, **d·ªØ li·ªáu ƒë·∫ßu v√†o** c·ªßa ch√∫ng ta s·∫Ω l√† **\"the man is walking\"**.  \n",
        "\n",
        "***M·ªçi ng∆∞·ªùi nh·ªõ kƒ© ph·∫ßn c·ª≠a s·ªï tr∆∞·ª£t c·∫Øt l·∫•y d·ªØ li·ªáu ƒë·∫ßu v√†o gi√∫p m√¨nh nha.*** üìù  \n",
        "\n",
        "Qu√° tr√¨nh t·∫°o c√¢u vƒÉn, ·ªü ƒë√¢y m√¨nh cho s·ªë l∆∞·ª£ng t·ª´ t·∫°o ra l√† **4**, n√™n ch√∫ng ta s·∫Ω c√≥ **4 l·∫ßn l·∫∑p**.  \n",
        "\n",
        "**Step 1**: Nh·∫≠n ƒë·∫ßu v√†o l√† **\"the man is walking\"**, t·ª•i m√¨nh d·ª± ƒëo√°n ra ƒë∆∞·ª£c t·ª´ **\"down\"**.  \n",
        "\n",
        "**Step 2**: T·ª•i m√¨nh k·∫øt h·ª£p t·ª´ **\"down\"** v√†o chung v·ªõi vƒÉn b·∫£n ƒë·∫ßu v√†o l√∫c tr∆∞·ªõc l√† **\"the man is walking\"** th√†nh **\"the man is walking down\"**, sau ƒë√≥ **c·ª≠a s·ªï tr∆∞·ª£t v·ªõi k√≠ch th∆∞·ªõc l√† 4** s·∫Ω c·∫Øt l·∫°i d·ªØ li·ªáu ƒë·∫ßu v√†o th√†nh **\"man is walking down\"** v√† d·ª± ƒëo√°n ra t·ª´ **\"the\"**.  \n",
        "\n",
        "**Step 3**: T∆∞∆°ng t·ª± nh∆∞ tr∆∞·ªõc, d·ªØ li·ªáu ƒë·∫ßu v√†o sau khi c·∫Øt l√† **\"is walking down the\"**, d·ª± ƒëo√°n ra t·ª´ **\"street\"**.  \n",
        "\n",
        "**Step 4**: D·ªØ li·ªáu ƒë·∫ßu v√†o l√† **\"walking down the street\"**, d·ª± ƒëo√°n ra d·∫•u **\".\"**.  \n",
        "\n",
        "V·∫≠y c√¢u cu·ªëi c√πng ch√∫ng ta d·ª± ƒëo√°n ra ƒë∆∞·ª£c s·∫Ω l√†:  \n",
        "> **\"the man is walking down the street .\"** üö∂‚Äç‚ôÇÔ∏è‚ú®  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FzYVdCkaLCd"
      },
      "source": [
        "# L√Ω thuy·∫øt ƒë·ªß nhi·ªÅu r·ªìi, t·ª•i m√¨nh s·∫Ω ti·∫øn h√†nh ƒëi v√†o th·ª±c h√†nh lu√¥n nha. „Éæ(‚Ä¢œâ‚Ä¢`)o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYCnwYGAumKL"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import kagglehub\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from datetime import timedelta\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTHJS8tl-ySf"
      },
      "outputs": [],
      "source": [
        "# T·∫°o m·ªôt MirroredStrategy ƒë·ªÉ s·ª≠ d·ª•ng t·∫•t c·∫£ c√°c GPU c√≥ s·∫µn\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxfK_EC-uep1"
      },
      "source": [
        "# Ti·∫øn h√†nh x·ª≠ l√Ω d·ªØ li·ªáu üõ†Ô∏è‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWxk5JKrbCgu"
      },
      "source": [
        "D·ªØ li·ªáu m√† ch√∫ng ta s·∫Ω d√πng l√† ƒëo·∫°n th∆° d∆∞·ªõi ƒë√¢y nha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUVBfZv1cDnp"
      },
      "outputs": [],
      "source": [
        "data = \"\"\"In the town of Athy one Jeremy Lanigan\n",
        " Battered away til he hadnt a pound.\n",
        " His father died and made him a man again\n",
        " Left him a farm and ten acres of ground.\n",
        " He gave a grand party for friends and relations\n",
        " Who didnt forget him when come to the wall,\n",
        " And if youll but listen Ill make your eyes glisten\n",
        " Of the rows and the ructions of Lanigan‚Äôs Ball.\n",
        " Myself to be sure got free invitation,\n",
        " For all the nice girls and boys I might ask,\n",
        " And just in a minute both friends and relations\n",
        " Were dancing round merry as bees round a cask.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yjydxacbc1Wr"
      },
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra c√∫ ph√°p ƒëo·∫°n th∆°\n",
        "print(repr(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zx47s04-fH_"
      },
      "outputs": [],
      "source": [
        "# T·ª•i m√¨nh s·∫Ω coi m·∫´u d√≤ng trong ƒëo·∫°n th∆° l√† m·ªôt c√¢u v√† t√°ch ch√∫ng ra d·ª±a tr√™n d·∫•u xu·ªëng h√†ng \\n nha\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "# T·∫°o b·ªô t·ª´ ƒëi·ªÉn t·ª´ b·ªô d·ªØ li·ªáu c·ªßa ch√∫ng ta.\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "# Ki·ªÉm tra b·ªô t·ª´ ƒëi·ªÉn\n",
        "print(tokenizer.word_index)\n",
        "print(f\"S·ªë l∆∞·ª£ng t·ª´ trong b·ªô t·ª´ ƒëi·ªÉn l√†: {len(tokenizer.word_index)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "in8T7Aj3fWU6"
      },
      "outputs": [],
      "source": [
        "total_words = len(tokenizer.word_index) + 1 # Th√™m 1 v√¥ ƒë·∫°i di·ªán cho ph·∫ßn token <padding>: 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SExt7Nae8ff"
      },
      "source": [
        "### B√¢y gi·ªù t·ª•i m√¨nh s·∫Ω t·ªõi v·ªõi ph·∫ßn x·ª≠ l√Ω d·ªØ li·ªáu hu·∫•n luy·ªán. üõ†Ô∏è  \n",
        "***M·ªçi ng∆∞·ªùi ch√∫ √Ω kƒ© ph·∫ßn n√†y, tr√°nh nh·∫ßm l·∫´n gi√∫p m√¨nh nha. ·ªû ph·∫ßn th·ª±c h√†nh n√†y, t·∫°m th·ªùi ch√∫ng ta s·∫Ω kh√¥ng s·ª≠ d·ª•ng c·ª≠a s·ªï tr∆∞·ª£t!***  \n",
        "\n",
        "ƒê·∫ßu ti√™n, ta ti·∫øn h√†nh m√£ h√≥a c√¢u t·ª´ d·ªØ li·ªáu vƒÉn b·∫£n sang chu·ªói d·∫°ng s·ªë. üî¢"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![encode](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_8/encode.png?raw=true)"
      ],
      "metadata": {
        "id": "Z7BftXSnyT2w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUtSQXAygX7l"
      },
      "source": [
        "Ti·∫øp ƒë·∫øn, ch√∫ng ta s·∫Ω t√°ch chu·ªói th√†nh nhi·ªÅu chu·ªói con nh·ªè h∆°n v·ªõi ƒë·ªô d√†i chu·ªói tƒÉng d·∫ßn. ‚úÇÔ∏è  \n",
        "\n",
        "***Ch√∫ √Ω: M·ªçi ng∆∞·ªùi c·∫ßn ph·∫£i ph√¢n bi·ªát, hi·ªán t·∫°i ch√∫ng ta kh√¥ng s·ª≠ d·ª•ng c·ª≠a s·ªï tr∆∞·ª£t ·ªü ƒë√¢y nha!*** üßê  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![sub_sequences](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_8/sub_sequences.png?raw=true)"
      ],
      "metadata": {
        "id": "Jl0G0RYSyXgm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5SJT0Nzgtn3"
      },
      "source": [
        "Nh∆∞ m·ªçi ng∆∞·ªùi th·∫•y, t·ª´ m·ªôt chu·ªói ban ƒë·∫ßu ch√∫ng ta ƒë√£ t√°ch ra th√†nh 7 chu·ªói con, nh·ªù ƒë√≥ gia tƒÉng ƒë∆∞·ª£c s·ªë l∆∞·ª£ng d·ªØ li·ªáu. üå± C√°c chu·ªói con ƒë·ªÉ hu·∫•n luy·ªán n√†y c√≤n c√≥ t√™n g·ªçi kh√°c l√† **\"seed text - h·∫°t gi·ªëng vƒÉn b·∫£n\"**, kh·ªüi ngu·ªìn ƒë·ªÉ n·∫£y m·∫ßm, t·∫°o n√™n c√°c vƒÉn b·∫£n d·ª± ƒëo√°n sau n√†y.\n",
        "Vi·ªác chia c√°c chu·ªói con n√†y gi√∫p cho m√¥ h√¨nh c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c ng·ªØ c·∫£nh ·ªü nhi·ªÅu ƒë·ªô d√†i kh√°c nhau.üåü"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAxkotOlhKwP"
      },
      "outputs": [],
      "source": [
        "input_sequences = []\n",
        "for line in corpus:\n",
        "  # M√£ h√≥a c√¢u (line) th√†nh chu·ªói s·ªë\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0] # V√¨ d·ªØ li·ªáu tr·∫£ v·ªÅ c·ªßa h√†m l√† d·∫°ng m·ªôt list n√™n ƒë·ªÉ l·∫•y ƒë∆∞·ª£c chu·ªói s·ªë ph·∫£i l·∫•y ph·∫ßn t·ª≠ 0 c·ªßa list\n",
        "  for i in range(1, len(token_list)):\n",
        "    # Ti·∫øn h√†nh t√°ch ra th√†nh t·ª´ng chu·ªói con\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Ki·ªÉm tra th·ª≠ 5 chu·ªói ƒë·∫ßu ti√™n\n",
        "for i in range(5):\n",
        "  print(input_sequences[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUdDON_DiOjd"
      },
      "source": [
        "B√¢y gi·ªù ch√∫ng ta ti·∫øn h√†nh **padding** cho d·ªØ li·ªáu d·ª±a tr√™n ƒë·ªô d√†i c·ªßa c√¢u d√†i nh·∫•t. üõ†Ô∏è\n",
        "\n",
        "L√Ω do l√† b·ªüi v√¨ kh√¥ng c·ªë ƒë·ªãnh k√≠ch th∆∞·ªõc nh∆∞ **c·ª≠a s·ªï tr∆∞·ª£t**, d·ªØ li·ªáu gi·ªØa c√°c c√¢u c·ªßa ch√∫ng ta c√≥ k√≠ch th∆∞·ªõc ch√™nh l·ªách r·∫•t nhi·ªÅu. Ch√∫ng ta c·∫ßn **ƒë·ªìng d·∫°ng k√≠ch th∆∞·ªõc** ƒë·ªÉ ƒë∆∞a v√†o hu·∫•n luy·ªán m√¥ h√¨nh. üìè\n",
        "\n",
        "*üí° L∆∞u √Ω: Trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng ta s·∫Ω **padding ·ªü ƒë·∫ßu** (pre-padding) nha, ƒë·ªÉ t·ª´ c√≥ th·ªÉ ·∫£nh h∆∞·ªüng m·∫°nh h∆°n ƒë·∫øn t·ª´ d·ª± ƒëo√°n ph√≠a sau, gi·ªëng v·ªõi **kh·∫£ nƒÉng ·∫£nh h∆∞·ªüng c·ªßa tr√¨nh t·ª± c√°c t·ª´** m√† ch√∫ng ta ƒë√£ t√¨m hi·ªÉu ·ªü ch∆∞∆°ng 7.* üåü"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![padding_sequences](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_8/pad_sequences.png?raw=true)\n"
      ],
      "metadata": {
        "id": "v1hq-eENyj3q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEd3Sg6ukep8"
      },
      "outputs": [],
      "source": [
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences_padded = np.array(pad_sequences(input_sequences, maxlen = max_sequence_len, padding=\"pre\"))\n",
        "\n",
        "# Ch·ªânh l·∫°i s·ªë l∆∞·ª£ng token v√¨ ƒë√£ th√™m token padding v√†o <pad>: 0\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Ki·ªÉm tra chu·ªói sau khi ƒë·ªám\n",
        "for i in range(5):\n",
        "  print(input_sequences_padded[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwucCRjQnb2T"
      },
      "source": [
        "V·∫≠y l√† c√°c chu·ªói c·ªßa ch√∫ng ta ƒë√£ ƒë∆∞·ª£c **ƒë·ªám ƒë·∫ßy ƒë·ªß** v·ªõi k√≠ch th∆∞·ªõc l√† **10**. üéâ\n",
        "\n",
        "B√¢y gi·ªù, ch√∫ng ta s·∫Ω ti·∫øn h√†nh **chia d·ªØ li·ªáu** th√†nh hai ph·∫ßn:\n",
        "\n",
        "1. **D·ªØ li·ªáu hu·∫•n luy·ªán**: Ch√≠nh l√† c√°c t·ª´ trong chu·ªói **tr·ª´ t·ª´ cu·ªëi c√πng**.\n",
        "2. **Nh√£n**: L√† **t·ª´ cu·ªëi c√πng** c·ªßa m·ªói chu·ªói.\n",
        "\n",
        "Qu√° tr√¨nh n√†y s·∫Ω gi√∫p ch√∫ng ta c√≥ d·ªØ li·ªáu ph√π h·ª£p ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh d·ª± ƒëo√°n t·ª´ ti·∫øp theo. üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![padding_sequences](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_8/x_labels.png?raw=true)"
      ],
      "metadata": {
        "id": "eLKglTjTyy5K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIKlNHE8qIFj"
      },
      "outputs": [],
      "source": [
        "xs = input_sequences_padded[:,:-1]\n",
        "labels = input_sequences_padded[:,-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RUI2BuFqWqa"
      },
      "source": [
        "·ªû ƒë√¢y t·ª•i m√¨nh s·∫Ω √°p d·ª•ng m·ªôt **th·ªß thu·∫≠t nh·ªè** nh√©! üåü ƒê√¢y c≈©ng l√† c∆° h·ªôi ƒë·ªÉ ti·∫øp c·∫≠n m·ªôt lo·∫°i **loss function** kh√°c.\n",
        "\n",
        "Nh∆∞ tr∆∞·ªõc ƒë√¢y m√¨nh ƒë√£ ƒë·ªÅ c·∫≠p, c√≥ s·ª± kh√°c bi·ªát gi·ªØa **`sparse_categorical_crossentropy`** v√† **`categorical_crossentropy`**:\n",
        "\n",
        "- N·∫øu d·ªØ li·ªáu nh√£n l√† d·∫°ng **s·ªë index** (v√≠ d·ª•: `[0, 1, 2]`), ta s·∫Ω s·ª≠ d·ª•ng **`sparse_categorical_crossentropy`**.\n",
        "- Trong tr∆∞·ªùng h·ª£p n√†y, t·ª•i m√¨nh s·∫Ω ti·∫øn h√†nh **m√£ h√≥a nh√£n theo one-hot-encoding**, n√™n c·∫ßn s·ª≠ d·ª•ng **`categorical_crossentropy`**.\n",
        "\n",
        "### L∆∞u √Ω nh·ªè:\n",
        "- S·ª≠ d·ª•ng **index-based labels** (d·∫°ng s·ªë) c√≥ th·ªÉ **ti·∫øt ki·ªám b·ªô nh·ªõ h∆°n** v√¨ kh√¥ng c·∫ßn l∆∞u tr·ªØ to√†n b·ªô vector one-hot-encoding.\n",
        "- Tuy nhi√™n, v√¨ s√°ch ƒë√£ h∆∞·ªõng d·∫´n nh∆∞ v·∫≠y, t·ª•i m√¨nh c·ª© l√†m theo ƒë·ªÉ **l√†m quen v·ªõi c·∫£ hai c√°ch** nh√©! ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![dataset](https://github.com/Tkag0001/AI_and_Machine_Learning_for_Coders/blob/main/images/Chapter_8/dataset.png?raw=true)"
      ],
      "metadata": {
        "id": "xv9yH-kdy5RK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhDwD98vrgMa"
      },
      "outputs": [],
      "source": [
        "# Ti·∫øn h√†nh m√£ h√≥a one-hot-encoding cho nh√£n\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "ys = to_categorical(labels, num_classes = total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCIMJdgIsKLo"
      },
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra l·∫°i d·ªØ li·ªáu c·ªßa ch√∫ng ta\n",
        "print(f\"Chu·ªói ƒë∆∞·ª£c m√£ h√≥a: {input_sequences[0]}\")\n",
        "print(f\"Chu·ªói x ƒë·ªÉ hu·∫•n luy·ªán: {xs[0]}\")\n",
        "print(f\"Nh√£n c·ªßa chu·ªói x, t·ª©c t·ª´ ti·∫øp theo: {labels[0]}\")\n",
        "print(f\"Nh√£n sau khi one-hot-encoding:\\n {ys[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfHd5Ahluajw"
      },
      "source": [
        "# Ti·∫øn h√†nh t·∫°o v√† hu·∫•n luy·ªán m√¥ h√¨nh üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw23SIbMurtm"
      },
      "source": [
        "·ªû ƒë√¢y t·ª•i m√¨nh s·∫Ω th·ª≠ kh·ªüi t·∫°o m·ªôt m√¥ h√¨nh ƒë∆°n gi·∫£n th√¥i nha.  \n",
        "\n",
        "S·ªë chi·ªÅu **Embedding** t·ª•i m√¨nh t·∫°m cho l√† 8 nha v√¨ k√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn, s·ªë l∆∞·ª£ng t·ª´ c≈©ng kh√° nh·ªè.  \n",
        "\n",
        "V·ªÅ tham s·ªë trong l·ªõp **BLSTM**, s·ªë ƒë∆°n v·ªã ·∫©n nh∆∞ ch∆∞∆°ng tr∆∞·ªõc t·ª•i m√¨nh ƒë·ªÉ theo s·ªë chi·ªÅu Embedding th√¨ ·ªü ƒë√¢y m√¨nh ƒë·ªÉ t∆∞∆°ng ƒë∆∞∆°ng v·ªõi ƒë·ªô d√†i c·ªßa chu·ªói ban ƒë·∫ßu tr·ª´ ƒëi 1 nha (v√¨ ch√∫ng ta ƒë√£ l·∫•y token cu·ªëi l√†m nh√£n n√™n tr·ª´ 1 ƒëi).  \n",
        "\n",
        "ƒê·∫ßu ra l·ªõp tuy·∫øn t√≠nh - **Dense**, th√¨ s·∫Ω b·∫±ng v·ªõi t·ªïng s·ªë l∆∞·ª£ng tokens hay s·ªë l∆∞·ª£ng t·ª´ v·ªõi 1 token padding. ‚ú®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JC_DEmseu36E"
      },
      "outputs": [],
      "source": [
        "# X√¢y d·ª±ng ki·∫øn tr√∫c m√¥ h√¨nh\n",
        "model = Sequential([\n",
        "    Embedding(total_words, 8),\n",
        "    Bidirectional(LSTM(max_sequence_len-1)),\n",
        "    Dense(total_words, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYkUkiIwxEmA"
      },
      "source": [
        "B·ªüi v√¨ m√¥ h√¨nh ·ªü ƒë√¢y kh√° ƒë∆°n gi·∫£n v√† √≠t d·ªØ li·ªáu n√™n ch√∫ng ta ti·∫øn h√†nh hu·∫•n luy·ªán l√¢u h∆°n v·ªõi **s·ªë epoch l√† 15.000**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO0NF3m4w2xN"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "history = model.fit(xs, ys, epochs=1500)\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzLFU3K6Qg57"
      },
      "outputs": [],
      "source": [
        "print(f\"Th·ªùi gian hu·∫•n luy·ªán: {timedelta(seconds=end_time - start_time)} gi√¢y\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsNBEKtuxQAO"
      },
      "outputs": [],
      "source": [
        "# V·∫Ω bi·ªÉu ƒë·ªì hu·∫•n luy·ªán\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_acc = history.history[\"accuracy\"]\n",
        "train_loss = history.history[\"loss\"]\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize = (12, 4))\n",
        "axs[0].plot(train_acc)\n",
        "axs[0].set_title(\"Training Accuracy\")\n",
        "axs[1].plot(train_loss, color=\"orange\")\n",
        "axs[1].set_title(\"Training Loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzWGVLLsy2Bq"
      },
      "outputs": [],
      "source": [
        "eval = model.evaluate(xs, ys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eC4JQ-N6qDQ"
      },
      "outputs": [],
      "source": [
        "model.save(\"model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gY2QpWpFWO8"
      },
      "outputs": [],
      "source": [
        "# T·∫£i l·∫°i model\n",
        "model = load_model(\"model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O082PMny6Ab"
      },
      "source": [
        "K·∫øt qu·∫£ cu·ªëi c√πng t·ª•i m√¨nh ƒëo ƒë∆∞·ª£c s·∫Ω kho·∫£ng c·ª° 95%, ƒëi·ªÅu ƒë√≥ c√≥ nghƒ©a l√† t·ª´ d·ª± ƒëo√°n ƒë∆∞·ª£c ti·∫øp theo c√≥ x√°c su·∫•t 95% gi·ªëng v·ªõi t·ª´ ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán b·ªüi chu·ªói tr∆∞·ªõc ƒë√≥.  \n",
        "\n",
        "***M·ªçi ng∆∞·ªùi l∆∞u √Ω gi√∫p m√¨nh: ·ªû ƒë√¢y s·ª≠ d·ª•ng ƒë·ªô ch√≠nh x√°c - accuracy ƒë·ªÉ ƒëo kh√¥ng th·∫≠t s·ª± hi·ªáu qu·∫£ ƒë·ªëi v·ªõi b√†i to√°n ng·ªØ nghƒ©a t·∫°o/sinh vƒÉn b·∫£n nh∆∞ n√†y. Cho ƒë·∫øn th·ªùi ƒëi·ªÉm hi·ªán t·∫°i t·ª•i m√¨nh h·ªçc, v·∫´n ch∆∞a c√≥ thang ƒëo n√†o ph√π h·ª£p tuy·ªát ƒë·ªëi cho b√†i to√°n.*** üåü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzTqlPpQ3Qq5"
      },
      "source": [
        "### Predicting the Next Word  \n",
        "### B√¢y gi·ªù t·ª•i m√¨nh s·∫Ω ti·∫øn h√†nh d·ª± ƒëo√°n th·ª≠ m·ªôt t·ª´ ti·∫øp theo tr∆∞·ªõc nha.  \n",
        "\n",
        "VƒÉn b·∫£n m√¨nh ch·ªçn l√† **\"in the town of athy\"** n·∫±m ·ªü c√¢u ƒë·∫ßu ti√™n c·ªßa d·ªØ li·ªáu hu·∫•n luy·ªán nha. üåü  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVW2qQCv3rgx"
      },
      "outputs": [],
      "source": [
        "seed_text =  \"in the town of athy\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WebWVHuI3w-_"
      },
      "source": [
        "ƒê·∫ßu ti√™n ta s·∫Ω ti·∫øn h√†nh tokenize cho chu·ªói ƒë√≥, sau ƒë√≥ th√¨ padding v√† ƒë∆∞a v√†o m√¥ h√¨nh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqRF78S_32r3"
      },
      "outputs": [],
      "source": [
        "token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') # -1 v√†o max_sequence_len b·ªüi v√¨ chu·ªói n√†y kh√¥ng c√≥ nh√£n\n",
        "# Ti·∫øn h√†nh d·ª± ƒëo√°n\n",
        "predicted = model.predict(token_list, verbose=1)\n",
        "predicted_index = np.argmax(predicted, axis = -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfUxFC1q4RuN"
      },
      "outputs": [],
      "source": [
        "# T·∫°o index to word ƒë·ªÉ chuy·ªÉn t·ª´ d·ªØ li·ªáu d·∫°ng s·ªë sang vƒÉn b·∫£n l·∫°i\n",
        "index_words ={}\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    index_words[index] = word\n",
        "\n",
        "# Chuy·ªÉn index d·ª± ƒëo√°n th√†nh d·∫°ng ch·ªØ\n",
        "predicted_word = index_words[predicted_index[0]]\n",
        "\n",
        "print(f\"Chu·ªói ban ƒë·∫ßu: {seed_text}\")\n",
        "print(f\"T·ª´ d·ª± ƒëo√°n: '{predicted_word}' v·ªõi s√°c xu·∫•t {predicted[0][predicted_index[0]]*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-gbSdiZ55xL"
      },
      "source": [
        "U·∫ßy ra t·ª´ \"one\" ƒë√∫ng v·ªõi c√¢u ƒë·∫ßu ti√™n trong d·ªØ li·ªáu hu·∫•n luy·ªán lu√¥n n√†y.\n",
        "> In the town of Athy one Jeremy Lanigan<br>\n",
        " Battered away til he hadnt a pound."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O48-oBgP6KdF"
      },
      "source": [
        "### Compounding Predictions to Generate Text  \n",
        "### B√¢y gi·ªù t·ª•i m√¨nh s·∫Ω ti·∫øn h√†nh t·∫°o/sinh vƒÉn b·∫£n d·ª±a tr√™n workflow tr∆∞·ªõc ƒë√≥ t·ª•i m√¨nh ƒë·ªãnh nghƒ©a nha. üåü‚ú®üåà  \n",
        "\n",
        "***L∆∞u √Ω: ·ªû ƒë√¢y ch√∫ng ta ch·ªçn padding chu·ªói theo ƒë·ªô d√†i c√¢u d√†i nh·∫•t v√¨ ban ƒë·∫ßu khi d·ªØ li·ªáu hu·∫•n luy·ªán c≈©ng ƒë∆∞·ª£c l√†m nh∆∞ v·∫≠y ch·ª© ch∆∞a s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p c·ª≠a s·ªï tr∆∞·ª£t nha. ƒê·ªÉ m√† n√≥i m·ªçi ng∆∞·ªùi c≈©ng c√≥ th·ªÉ hi·ªÉu tr∆∞·ªùng h·ª£p n√†y k√≠ch th∆∞·ªõc c·ª≠a s·ªï tr∆∞·ª£t l√† ƒë·ªô d√†i c√¢u d√†i nh·∫•t c≈©ng ƒë∆∞·ª£c. T·ª´ ƒë√≥ c√°c chu·ªói kh√¥ng ƒë·ªß k√≠ch th∆∞·ªõc th√¨ s·∫Ω padding th√™m v√†o. üîÑüìè  ***\n",
        "\n",
        "Th√¥ng qua vi·ªác l·∫∑p ƒëi l·∫∑p l·∫°i qu√° tr√¨nh d·ª± ƒëo√°n t·ª´ ti·∫øp theo, k·∫øt h·ª£p l·∫°i v·ªõi d·ªØ li·ªáu ƒë·∫ßu v√†o r·ªìi d·ª± ƒëo√°n ti·∫øp ta s·∫Ω t·∫°o/sinh ra ƒë∆∞·ª£c ph·∫ßn ti·∫øp theo cho c√¢u vƒÉn. üìùüîÆ  \n",
        "\n",
        "·ªû ƒë√¢y m√¨nh s·ª≠ d·ª•ng d·ªØ li·ªáu ƒë·∫ßu v√†o ban ƒë·∫ßu l√† **\"sweet jeremy saw dublin\"** nha. T·ª•i m√¨nh s·∫Ω ti·∫øn h√†nh d·ª± ƒëo√°n 10 t·ª´ ti·∫øp theo ph√≠a sau. üéØüìñ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugHAzgSL616W"
      },
      "outputs": [],
      "source": [
        "seed_text = \"sweet jeremy saw dublin\"\n",
        "predicted_text = seed_text\n",
        "n_words = 10\n",
        "\n",
        "for i in range(n_words):\n",
        "    token_list = tokenizer.texts_to_sequences([predicted_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict(token_list, verbose=1)\n",
        "    predicted_index = np.argmax(predicted, axis=-1)\n",
        "    output_word = index_words[predicted_index[0]]\n",
        "\n",
        "    print(f\"Step {i}:\")\n",
        "    print(f\"VƒÉn b·∫£n ƒë·∫ßu v√†o: {predicted_text}\")\n",
        "    print(f\"Chu·ªói m√£ h√≥a ƒë·∫ßu v√†o: {token_list}\")\n",
        "    print(f\"T·ª´ d·ª± ƒëo√°n: {output_word} v·ªõi x√°c su·∫•t {predicted[0][predicted_index[0]]*100:2f}\")\n",
        "    print(\"-\"*50)\n",
        "    predicted_text += \" \" + output_word\n",
        "\n",
        "print(predicted_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD6R16wN3LNS"
      },
      "source": [
        "ƒê√¢y l√† k·∫øt qu·∫£ ·ªü l·∫ßn ch·∫°y c·ªßa m√¨nh nha:  \n",
        "\n",
        "> \"sweet jeremy saw dublin all the rows a and of lanigan‚Äôs ball ask ask\"  \n",
        "\n",
        "M·ªçi ng∆∞·ªùi c√≥ th·ªÉ th·∫•y c√†ng v·ªÅ sau th√¨ c√°c t·ª´ t·∫°o ra khi·∫øn c√¢u vƒÉn c√†ng tr·ªü n√™n v√¥ nghƒ©a h∆°n, th·∫≠m ch√≠ sai c·∫£ v·ªÅ m·∫∑t ng·ªØ ph√°p c·ªßa t·ª´. üòÖüìâ  \n",
        "\n",
        "> **V·∫≠y l·ªói n√†y do ƒë√¢u m√† ra?**  \n",
        "\n",
        "C√≥ 2 l√Ω do ch√≠nh ·∫£nh h∆∞·ªüng ƒë·∫øn vi·ªác n√†y l√†:  \n",
        "\n",
        "1. **D·ªØ li·ªáu hu·∫•n luy·ªán c·ªßa ch√∫ng ta qu√° nh·ªè** khi·∫øn m√¥ h√¨nh kh√¥ng th·ªÉ n·∫Øm b·∫Øt ƒë∆∞·ª£c nhi·ªÅu ng·ªØ c·∫£nh. üìö‚ùå  \n",
        "2. **Hi·ªáu ·ª©ng d√¢y chuy·ªÅn, c√°nh b∆∞·ªõm hay hi·ªáu ·ª©ng Domino.** Khi m·ªôt vi·ªác g√¨ ƒë√≥ x·∫£y ra th√¨ c√°c s·ª± vi·ªác sau ph·ª• thu·ªôc v√†o n√≥ s·∫Ω ch·ªãu ·∫£nh h∆∞·ªüng l·ªõn d·∫ßn l√™n. ·ªû ƒë√¢y, khi m·ªôt t·ª´ ƒë∆∞·ª£c d·ª± ƒëo√°n kh√¥ng t·ªët, c√°c t·ª´ ph√≠a sau n√≥ l·∫°i ƒë∆∞·ª£c d·ª± ƒëo√°n d·ª±a tr√™n n√≥ v√† c√°c t·ª´ ph√≠a tr∆∞·ªõc c≈©ng s·∫Ω tr·ªü n√™n t·ªá h∆°n. D·∫ßn d·∫ßn s·ª± t·ªá h·∫°i n√†y ng√†y m·ªôt l·ªõn d·∫ßn khi·∫øn cho c√°c t·ª´ n·ªëi v√†o c√¢u vƒÉn c√†ng v·ªÅ sau tr·ªü n√™n v√¥ nghƒ©a h∆°n r·∫•t l√† nhi·ªÅu. üîóüåÄ  \n",
        "\n",
        "C√≥ m·ªôt fact l√† v√†o nƒÉm 2016 ƒë√£ t·ª´ng c√≥ m·ªôt b·ªô phim vi·ªÖn t∆∞·ªüng [Sunspring](https://en.wikipedia.org/wiki/Sunspring) ƒë∆∞·ª£c t·∫°o ra b·ªüi AI. B·ªô phim n√†y kh√° bu·ªìn c∆∞·ªùi, ban ƒë·∫ßu n·ªôi dung v·∫´n ·ªïn cho ƒë·∫øn c√†ng v·ªÅ sau ch√∫ng l·∫°i tr·ªü n√™n kh√≥ hi·ªÉu ƒë√∫ng nghƒ©a ba ch·∫•m lu√¥n √Ω. üé•ü§ñ‚ú®"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnbyzGdC-riZ"
      },
      "source": [
        "# Extending the dataset  \n",
        "### B√¢y gi·ªù t·ª•i m√¨nh s·∫Ω l√†m vi·ªác v·ªõi m√¥ h√¨nh v·ªõi b·ªô d·ªØ li·ªáu l·ªõn h∆°n nha.  \n",
        "\n",
        "·ªû ƒë√¢y theo t√°c gi·∫£ th√¨ b·ªô d·ªØ li·ªáu d∆∞·ªõi ƒë√¢y bao g·ªìm kho·∫£ng 1.700 v·ªÅ l·ªùi c√°c b√†i h√°t. V√¨ ƒë∆∞·ªùng d·∫´n trong s√°ch kh√¥ng th·ªÉ s·ª≠ d·ª•ng ƒë∆∞·ª£c n·ªØa n√™n t·ª•i m√¨nh s·∫Ω t·∫£i d·ªØ li·ªáu t·ª´ Kaggle nha. üé∂üì•  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS4Qi4i0_CGA"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jamzhu/irishlyricseof\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnMJvtpT_GAc"
      },
      "outputs": [],
      "source": [
        "# # Li·ªát k√™ c√°c file b√™n trong th∆∞ m·ª•c\n",
        "import os\n",
        "for f in os.listdir(path):\n",
        "  print(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exeWRN8l_XlM"
      },
      "outputs": [],
      "source": [
        "# Ti·∫øn h√†nh t·∫£i b·ªô d·ªØ li·ªáu\n",
        "data = open(os.path.join(path,\"irish-lyrics-eof.txt\")).read()\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "# In th·ª≠ 5 h√†ng ƒë·∫ßu ti√™n ra\n",
        "for i in range(5):\n",
        "  print(repr(corpus[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBR3Ohcf_4HD"
      },
      "outputs": [],
      "source": [
        "# Ti·∫øn h√†nh tokenize d·ªØ li·ªáu\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1 # Th√™m m·ªôt cho token padding sau n√†y\n",
        "print(f\"T·ªïng s·ªë l∆∞·ª£ng t·ª´: {total_words}\")\n",
        "print(f\"B·ªô t·ª´ ƒëi·ªÉn t·ª´ v·ª±ng: {tokenizer.word_index}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FARlEG1Anbb"
      },
      "outputs": [],
      "source": [
        "# T·∫°o b·ªô t·ª´ ƒëi·ªÉn chuy·ªÉn d·ªØ li·ªáu s·ªë th√†nh ch·ªØ l·∫°i\n",
        "index_words ={}\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    index_words[index] = word\n",
        "\n",
        "# Ti·∫øn h√†nh t·∫°o c√°c seed text hay chu·ªói con\n",
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)-1):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Ki·ªÉm tra 5 chu·ªói m√£ h√≥a s·ªë ƒë·∫ßu ti√™n\n",
        "for i in range(5):\n",
        "  print(input_sequences[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMe2sgSUBlmN"
      },
      "outputs": [],
      "source": [
        "# Ti·∫øn h√†nh padding cho d·ªØ li·ªáu\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "print(f\"ƒê·ªô d√†i chu·ªói d√†i nh·∫•t: {max_sequence_len}\")\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "# In th·ª≠ 5 chu·ªói ƒë·∫ßu ti√™n\n",
        "for i in range(5):\n",
        "  print(input_sequences[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYB7Zl9vCFph"
      },
      "outputs": [],
      "source": [
        "# Ti·∫øn h√†nh chia d·ªØ li·ªáu hu·∫•n luy·ªán v√† nh√£n\n",
        "xs = input_sequences[:,:-1]\n",
        "labels = input_sequences[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSU79wB_CMV7"
      },
      "outputs": [],
      "source": [
        "# Thi·∫øt l·∫≠p m√¥ h√¨nh\n",
        "model1 = Sequential([\n",
        "    Embedding(total_words, 8),\n",
        "    Bidirectional(LSTM(max_sequence_len-1)),\n",
        "    Dense(total_words, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model1.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZO2GYa3CrXk"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "history1 = model1.fit(xs, labels, epochs=1000)\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZaULiFaibUA"
      },
      "outputs": [],
      "source": [
        "print(f\"Th·ªùi gian hu·∫•n luy·ªán: {timedelta(seconds=end_time-start_time)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeJHizKpCwFH"
      },
      "outputs": [],
      "source": [
        "# V·∫Ω bi·ªÉu ƒë·ªì qu√° tr√¨nh hu·∫•n luy·ªán\n",
        "import matplotlib.pyplot as plt\n",
        "fig, axs = plt.subplots(1, 2, figsize = (12, 4))\n",
        "train_acc = history1.history[\"accuracy\"]\n",
        "train_loss = history1.history[\"loss\"]\n",
        "axs[0].plot(train_acc)\n",
        "axs[0].set_title(\"Training Accuracy\")\n",
        "axs[1].plot(train_loss, color=\"orange\")\n",
        "axs[1].set_title(\"Training Loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffBTpv2Qnz5W"
      },
      "outputs": [],
      "source": [
        "model1.evaluate(xs,labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Kz5jVw561Gr"
      },
      "outputs": [],
      "source": [
        "model1.save(\"model1.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIwiMg7aGh-0"
      },
      "outputs": [],
      "source": [
        "# T·∫£i l·∫°i m√¥ h√¨nh\n",
        "model1 = load_model(\"model1.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdfhGOIflUV3"
      },
      "source": [
        "U·∫ßy, v·∫≠y l√† v·ªõi b·ªô d·ªØ li·ªáu n√†y, ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p hu·∫•n luy·ªán ƒë·∫°t ƒë∆∞·ª£c l√† kho·∫£ng 60%. üéâ\n",
        "\n",
        "B√¢y gi·ªù t·ª•i m√¨nh s·∫Ω ti·∫øn h√†nh d·ª± ƒëo√°n th·ª≠ v·ªõi c√°c v·∫ø tr∆∞·ªõc ƒë√≥ nha. üîç‚ú®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEiQdO-tmmdW"
      },
      "outputs": [],
      "source": [
        "# D·ª± ƒëo√°n m·ªôt t·ª´\n",
        "seed_text = \"in the town of athy\"\n",
        "token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') # -1 v√†o max_sequence_len b·ªüi v√¨ chu·ªói n√†y kh√¥ng c√≥ nh√£n\n",
        "# Ti·∫øn h√†nh d·ª± ƒëo√°n\n",
        "predicted = model1.predict(token_list, verbose=1)\n",
        "predicted_index = np.argmax(predicted, axis = -1)\n",
        "predicted_word = index_words[predicted_index[0]]\n",
        "\n",
        "print(f\"T·ª´ d·ª± ƒëo√°n l√†: '{predicted_word}' v·ªõi x√°c su·∫•t {predicted[0][predicted_index[0]]*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZZxqUJgoD_b"
      },
      "source": [
        "·ªû ƒë√¢y k·∫øt qu·∫£ m√¨nh ra ƒë∆∞·ª£c l√†:  \n",
        "> T·ª´ d·ª± ƒëo√°n l√†: **'light'** v·ªõi x√°c su·∫•t **20.35%** ‚ú®  \n",
        "\n",
        "M·ªçi ng∆∞·ªùi c√≥ th·ªÉ ra k·∫øt qu·∫£ kh√°c nha, ƒëi·ªÅu n√†y ph·ª• thu·ªôc v√†o tr·ªçng s·ªë c·ªßa m√¥ h√¨nh m√† tr·ªçng s·ªë c·ªßa m·ªói m√¥ h√¨nh khi kh·ªüi t·∫°o l√† ng·∫´u nhi√™n, kh√¥ng gi·ªëng nhau n√™n k·∫øt qu·∫£ cu·ªëi c≈©ng v·∫≠y. Mi·ªÖn sao √Ω nghƒ©a c√¢u vƒÉn hay c√°c t·ª´ c√≥ ph·∫ßn li√™n k·∫øt nhau h·ª£p l√Ω l√† ok r·ªìi. üåü  \n",
        "\n",
        "T·ª•i m√¨nh s·∫Ω d·ª± ƒëo√°n ti·∫øp v·ªõi c·ª•m **‚Äúsweet jeremy saw dublin‚Äù**. üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sGNrH4a3rA4"
      },
      "outputs": [],
      "source": [
        "# D·ª± ƒëo√°n m·ªôt chu·ªói\n",
        "seed_text = \"sweet jeremy saw dublin\"\n",
        "num_words = 10\n",
        "predicted_text = seed_text\n",
        "for i in range(num_words):\n",
        "  token_list = tokenizer.texts_to_sequences([predicted_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "  predicted = model1.predict(token_list, verbose=1)\n",
        "  predicted_index = np.argmax(predicted, axis=-1)\n",
        "  output_word = index_words[predicted_index[0]]\n",
        "\n",
        "  print(f\"Step {i}:\")\n",
        "  print(f\"VƒÉn b·∫£n ƒë·∫ßu v√†o: {predicted_text}\")\n",
        "  print(f\"Chu·ªói m√£ h√≥a ƒë·∫ßu v√†o: {token_list[0]}\")\n",
        "  print(f\"T·ª´ d·ª± ƒëo√°n: {output_word} v·ªõi x√°c su·∫•t {predicted[0][predicted_index[0]]*100:2f}%\")\n",
        "  print(\"-\"*50)\n",
        "  predicted_text += \" \" + output_word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n3i2V3Osxlq"
      },
      "outputs": [],
      "source": [
        "print(f\"VƒÉn b·∫£n sinh ra: '{predicted_text}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NcgUOcUs5gb"
      },
      "source": [
        "ƒê√¢y l√† k·∫øt qu·∫£ ·ªü l·∫ßn ch·∫°y c·ªßa m√¨nh:  \n",
        "> VƒÉn b·∫£n sinh ra: 'sweet jeremy saw dublin ever make a begging i neer could you dead and'‚ú®  \n",
        "\n",
        "C√≥ v·∫ª l√† ch√∫ng ƒë√£ ·ªïn h∆°n r·ªìi ha, sai s√≥t th√¨ t·∫•t nhi√™n v·∫´n c√≥ nh∆∞ng ng·ªØ nghƒ©a c·∫•u tr√∫c ƒë√£ ·ªïn h∆°n t√≠ r·ªìi. üåü B√¢y gi·ªù t·ª•i m√¨nh s·∫Ω ƒëi th·ª≠ th√™m nhi·ªÅu c√°ch n·ªØa ƒë·ªÉ tƒÉng ƒë·ªô hi·ªáu qu·∫£ l√™n h∆°n nha. üöÄ  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqKd6g6HuzVR"
      },
      "source": [
        "# Changing the Modle Architecture\n",
        "### Tr∆∞·ªõc m·∫Øt th√¨ t·ª•i m√¨nh s·∫Ω th·ª≠ thay ƒë·ªïi ki·∫øn tr√∫c m√¥ h√¨nh xem sao."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Zj5H1WiuacY"
      },
      "outputs": [],
      "source": [
        "model2= Sequential([\n",
        "    Embedding(total_words, 8),\n",
        "    Bidirectional(LSTM(max_sequence_len-1, return_sequences=True)),\n",
        "    Bidirectional(LSTM(max_sequence_len-1)),\n",
        "    Dense(total_words, activation = 'softmax')\n",
        "])\n",
        "\n",
        "model2.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GntMGZGKvoD7"
      },
      "outputs": [],
      "source": [
        "# Ti·∫øn h√†nh hu·∫•n luy·ªán\n",
        "start_time = time.time()\n",
        "history2 = model2.fit(xs, labels, epochs = 1000)\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YizHpfmv3TX"
      },
      "outputs": [],
      "source": [
        "print(f\"Th·ªùi gian hu·∫•n luy·ªán m√¥ h√¨nh: {timedelta(seconds=end_time - start_time)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN-m48Hlv_iw"
      },
      "outputs": [],
      "source": [
        "# V·∫Ω bi·ªÉu ƒë·ªì qu√° tr√¨nh hu·∫•n luy·ªán\n",
        "train_acc = history2.history[\"accuracy\"]\n",
        "train_loss = history2.history[\"loss\"]\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize = (12, 4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc)\n",
        "axs[0].set_title(\"Training Accuracy\")\n",
        "axs[1].plot(train_loss, color=\"orange\")\n",
        "axs[1].set_title(\"Training Loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C8952SpwUqb"
      },
      "outputs": [],
      "source": [
        "# ƒê√°nh gi√° m√¥ h√¨nh\n",
        "model2.evaluate(xs, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y40Bspoj7Dll"
      },
      "outputs": [],
      "source": [
        "model2.save(\"model2.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_RapNTYGs-P"
      },
      "outputs": [],
      "source": [
        "# T·∫£i l·∫°i m√¥ h√¨nh\n",
        "model2 = load_model(\"model2.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLXe5_SSwiVT"
      },
      "source": [
        "T·ª•i m√¨nh s·∫Ω ti·∫øn h√†nh d·ª± ƒëo√°n l·∫°i v·ªõi c√°c c√¢u tr∆∞·ªõc ƒë√≥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYtBRNkTwZv2"
      },
      "outputs": [],
      "source": [
        "# D·ª± ƒëo√°n m·ªôt t·ª´\n",
        "seed_text = \"in the town of athy\"\n",
        "token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre') # -1 v√†o max_sequence_len b·ªüi v√¨ chu·ªói n√†y kh√¥ng c√≥ nh√£n\n",
        "# Ti·∫øn h√†nh d·ª± ƒëo√°n\n",
        "predicted = model2.predict(token_list, verbose=1)\n",
        "predicted_index = np.argmax(predicted, axis = -1)\n",
        "predicted_word = index_words[predicted_index[0]]\n",
        "\n",
        "print(f\"T·ª´ d·ª± ƒëo√°n l√†: '{predicted_word}' v·ªõi x√°c su·∫•t {predicted[0][predicted_index[0]]*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9nkVt5Kwo2o"
      },
      "outputs": [],
      "source": [
        "# D·ª± ƒëo√°n m·ªôt t·ª´\n",
        "seed_text = \"sweet jeremy saw dublin\"\n",
        "num_words = 10\n",
        "predicted_text = seed_text\n",
        "for i in range(num_words):\n",
        "  token_list = tokenizer.texts_to_sequences([predicted_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "  predicted = model2.predict(token_list, verbose=1)\n",
        "  predicted_index = np.argmax(predicted, axis=-1)\n",
        "  output_word = index_words[predicted_index[0]]\n",
        "\n",
        "  print(f\"Step {i}:\")\n",
        "  print(f\"VƒÉn b·∫£n ƒë·∫ßu v√†o: {predicted_text}\")\n",
        "  print(f\"Chu·ªói m√£ h√≥a ƒë·∫ßu v√†o: {token_list[0]}\")\n",
        "  print(f\"T·ª´ d·ª± ƒëo√°n: {output_word} v·ªõi x√°c su·∫•t {predicted[0][predicted_index[0]]*100:2f}%\")\n",
        "  print(\"-\"*50)\n",
        "  predicted_text += \" \" + output_word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBh1yyAOHlZ6"
      },
      "outputs": [],
      "source": [
        "print(f\"VƒÉn b·∫£n sinh ra: '{predicted_text}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNNKR4SZB9VG"
      },
      "source": [
        "# Improving the Data  \n",
        "### B√¢y gi·ªù t·ª•i m√¨nh ti·∫øn h√†nh th·ª≠ c√°ch c·∫£i thi·ªán d·ªØ li·ªáu ƒë·∫ßu v√†o ƒë·ªÉ c·∫£i ti·∫øn m√¥ h√¨nh nha, tƒÉng ƒë·ªô hi·ªáu qu·∫£ nha. ‚ú®  \n",
        "\n",
        "S·ªü dƒ© m·ªçi ng∆∞·ªùi th·∫•y ƒë∆∞·ªùng **train_accuracy** nhi·ªÖu v·∫≠y l√† v√¨ d·ªØ li·ªáu v·∫´n thi·∫øu ƒëi ph·∫ßn n√†o ƒë√≥ s·ª± li√™n k·∫øt ng·ªØ c·∫£nh m·∫°ch l·∫°c. üåê  \n",
        "\n",
        "> **V·∫≠y nguy√™n nh√¢n do ƒë√¢u m√† ra?**  \n",
        "\n",
        "Ch√≠nh l√† do c√°ch ti·∫øp c·∫≠n, chi·∫øn l∆∞·ª£c chia, x·ª≠ l√Ω d·ªØ li·ªáu c·ªßa ch√∫ng ta tr∆∞·ªõc ƒë√≥. Trong m·ªôt b√†i h√°t th√¨ c√°c c√¢u h√°t ƒë·ªÅu c√≥ s·ª± li√™n k·∫øt ng·ªØ c·∫£nh v·ªõi nhau, ƒë√≥ c≈©ng l√† c√°ch m√† con ng∆∞·ªùi ti·∫øp thu. üé∂ Tuy nhi√™n khi ƒë∆∞a d·ªØ li·ªáu v√†o hu·∫•n luy·ªán cho m√¥ h√¨nh, ch√∫ng ta ƒë√£ coi **m·ªói c√¢u h√°t l√† m·ªôt d√≤ng ƒë·ªôc l·∫≠p** v√† ti·∫øn h√†nh chia chu·ªói con. Ch√≠nh ƒëi·ªÅu n√†y ƒë√£ l√†m cho c√°c c√¢u h√°t tr·ªü n√™n m·∫•t k·∫øt n·ªëi ng·ªØ c·∫£nh v·ªõi nhau. üîó  \n",
        "\n",
        "> **Ok, x√°c ƒë·ªãnh ƒë∆∞·ª£c g·ªëc v·∫•n ƒë·ªÅ r·ªìi v·∫≠y ch√∫ng ta s·∫Ω l√†m g√¨ ti·∫øp theo ƒë√¢y?**  \n",
        "\n",
        "C√¢u tr·∫£ l·ªùi l√† l√†m sao cho c√°c chu·ªói vƒÉn b·∫£n hu·∫•n luy·ªán c√≥ ch·ª©a m·ªôt ph·∫ßn c·ªßa c√¢u tr∆∞·ªõc v√† c√¢u sau, t·ª´ ƒë√≥ gi·ªØa c√°c c√¢u s·∫Ω c√≥ s·ª± li√™n k·∫øt v·ªõi nhau. üß©  \n",
        "\n",
        "B√¢y gi·ªù thay v√¨ ƒë·ªÉ c√°c c√¢u trong b√†i h√°t th√†nh m·ªôt h√†ng ƒë·ªôc l·∫≠p, ch√∫ng ta s·∫Ω ti·∫øn h√†nh **gom t·∫•t c·∫£ c√°c c√¢u ƒë√≥ l·∫°i v·ªÅ chung m·ªôt h√†ng. Ch√∫ng ta s·∫Ω kh√¥ng chia theo k√Ω t·ª± xu·ªëng h√†ng \"\\n\" n·ªØa m√† l√† theo d·∫•u kho·∫£ng c√°ch**  üöÄ  \n",
        "\n",
        "**V·∫≠y workflow c·ªßa ch√∫ng ta s·∫Ω nh∆∞ sau:**\n",
        "\n",
        "**1. Chia c√°c c√¢u d·ª±a tr√™n c·ª≠a s·ªï tr∆∞·ª£t:** Chia ƒëo·∫°n vƒÉn th√†nh danh s√°ch c√°c t·ª´, sau ƒë√≥ d√πng c·ª≠a s·ªï tr∆∞·ª£t ƒë·ªÉ t·∫°o ra c√°c c√¢u hay chu·ªói con. Qua ƒë√≥ ta c√≥ th·ªÉ gi·ªØ ƒë∆∞·ª£c s·ª± li√™n k·∫øt ng·ªØ c·∫£nh c·ªßa c√°c c√¢u v·ªõi nhau.\n",
        "\n",
        "**2. Ti·∫øn h√†nh chia c√°c chu·ªói con t·ª´ c√°c chu·ªói ƒë√£ chia tr∆∞·ªõc ƒë√≥:** l√†m gi·ªëng nh∆∞ c√°ch ban ƒë√¢u tuy nhi√™n thay v√¨ d·ªØ li·ªáu c·ªßa ch√∫ng ta l√† c√°c c√¢u ƒë·ªôc l·∫≠p th√¨ gi·ªù l√† c√°c c√¢u ƒë√£ ƒë∆∞·ª£c t·∫°o ra ·ªü b∆∞·ªõc 1.k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otAJ6Gxc5nlv"
      },
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra l·∫°i d·ªØ li·ªáu\n",
        "print(repr(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BRk1caBJyXg"
      },
      "source": [
        "Ti·∫øn h√†nh chia c√°c c√¢u, chu·ªói d·ª±a tr√™n c·ª≠a s·ªï tr∆∞·ª£t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWF2E1v958Eh"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "# Chia theo kho·∫£ng c√°ch thay v√¨ k√≠ t·ª± xu·ªëng h√†ng \"\\n\"\n",
        "window_size = 6\n",
        "\n",
        "# Ti·∫øn h√†nh l·∫•y chu√¥i c√°c t·ª´ trong vƒÉn b·∫£n\n",
        "corpus = data.lower()\n",
        "words = corpus.split(\" \")\n",
        "\n",
        "# Ti·∫øn h√†nh chia c√°c c√¢u (chu·ªói) d·ª±a tr√™n c·ª≠a s·ªï tr∆∞·ª£t\n",
        "range_size = len(words) - window_size + 1\n",
        "\n",
        "input_sentences = []\n",
        "for i in range(range_size):\n",
        "  input_sentences.append(\" \".join(words[i:i+window_size]))\n",
        "\n",
        "\n",
        "# Ki·ªÉm tra 5 c√¢u (chu·ªói) ƒë·∫ßu ti√™n\n",
        "for i in range(5):\n",
        "  print(repr(input_sentences[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnI46rvLt2Vk"
      },
      "source": [
        "**C√°c c√¢u ·ªü chi·∫øn l∆∞·ª£c chia d·ªØ li·ªáu c≈©:**\n",
        ">come all ye maidens young and fair   \n",
        "and you that are blooming in your prime   \n",
        "always beware and keep your garden fair   \n",
        "let no man steal away your thyme   \n",
        "for thyme it is a precious thing   \n",
        "\n",
        "\n",
        "**C√°c c√¢u chia theo ph∆∞∆°ng ph√°p c·ª≠a s·ªï tr∆∞·ª£t:**\n",
        ">come all ye maidens young and   \n",
        "all ye maidens young and fair\\nand   \n",
        "ye maidens young and fair\\nand you   \n",
        "maidens young and fair\\nand you that    \n",
        "young and fair\\nand you that are"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmh8M-COJrWT"
      },
      "source": [
        "B√¢y gi·ªù t·ª•i m√¨nh ƒë·∫øn v·ªõi b∆∞·ªõc 2, ti·∫øn h√†nh m√£ h√≥a v√† chia chu·ªói con nha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S6aJ6JGEp6N"
      },
      "outputs": [],
      "source": [
        "# T·∫°o tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(input_sentences)\n",
        "\n",
        "sub_sequences = []\n",
        "# Ti·∫øn h√†nh m√£ h√≥a v√† chia chu·ªói con\n",
        "for sentence in input_sentences:\n",
        "  token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
        "  for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    sub_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Ki·ªÉm tra 5 chu·ªói ƒë·∫ßu ti√™n\n",
        "for i in range(5):\n",
        "  print(sub_sequences[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFgORcrIHB10"
      },
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra k√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn\n",
        "print(f\"T·ªïng s·ªë l∆∞·ª£ng t·ª´: {len(tokenizer.word_index)}\")\n",
        "print(f\"B·ªô t·ª´ ƒëi·ªÉn t·ª´ v·ª±ng: {tokenizer.word_index}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_sA4k2iAxvj"
      },
      "outputs": [],
      "source": [
        "# T·∫°o t·ª´ ƒëi·ªÉn chuy·ªÉn s·ªë th√†nh t·ª´\n",
        "index_words ={}\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    index_words[index] = word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l6lBGl4HNUp"
      },
      "source": [
        "·ªû ƒë√¢y c√≥ m·ªôt l∆∞u √Ω nh·ªè: v√¨ b·∫£n th√¢n tokenizer ƒë√£ c√≥ s·∫µn m·ªôt b·ªô l·ªçc ƒë·ªÉ c√°c k√≠ t·ª± ƒë·∫∑c bi·ªát n√™n c√°c t·ª´ n·ªëi li·ªÅn b·ªüi k√≠ t·ª± ƒë·∫∑c bi·ªát (v√≠ d·ª•: \"mother-in-law\" s·∫Ω b·ªã chuy·ªÉn th√†nh \"mother in law\"). Do ƒë√≥ s·∫Ω c√≥ m·ªôt s·ªë chu·ªói c√≥ k√≠ch th∆∞·ªõc l·ªõn h∆°n so v·ªõi c·ª≠a s·ªï tr∆∞·ª£t. Ta s·∫Ω ti·∫øn h√†nh t√≠nh l·∫°i ƒë·ªô d√†i chu·ªói d√†i nh·∫•t."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NXxTxfyHM1w"
      },
      "outputs": [],
      "source": [
        "max_sequence_len = max([len(x) for x in sub_sequences])\n",
        "print(f\"ƒê·ªô d√†i chu·ªói d√†i nh·∫•t: {max_sequence_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ExmeFzg9qvd"
      },
      "source": [
        "K·∫øt qu·∫£ l√†:\n",
        "> ƒê·ªô d√†i chu·ªói d√†i nh·∫•t: 22\n",
        "\n",
        "Nh∆∞ ch√∫ng m√¨nh ƒë√£ d·ª± ƒëo√°n nh∆∞ng kh√¥ng ng·ªù k√≠ch th∆∞·ªõc n√≥ c√≥ th·ªÉ tƒÉng l√™n nhi·ªÅu v·∫≠y."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3zOa44BIbPr"
      },
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra 5 chu·ªói c√≥ ƒë·ªô d√†i l·ªõn h∆°n c·ª≠a s·ªï tr∆∞·ª£t.\n",
        "sequences_higher_than_window = []\n",
        "for i in range(len(sub_sequences)):\n",
        "  if len(sub_sequences[i]) > window_size:\n",
        "    sequences_higher_than_window.append(sub_sequences[i])\n",
        "\n",
        "for i in range(5):\n",
        "  print(sequences_higher_than_window[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGEiY9D1IxOA"
      },
      "source": [
        "B√¢y gi·ªù t·ª•i m√¨nh s·∫Ω ti·∫øn h√†nh padding ƒë·ªÉ t·∫•t c·∫£ chu·ªói c√≥ c√πng k√≠ch th∆∞·ªõc, ƒë·ªô d√†i nha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQ0JH5xpI6bS"
      },
      "outputs": [],
      "source": [
        "input_sequences = np.array(pad_sequences(sub_sequences, maxlen=max_sequence_len, padding=\"pre\"))\n",
        "\n",
        "# Chia d·ªØ li·ªáu x v√† labels ƒë·ªÉ hu·∫•n luy·ªán.\n",
        "xs = input_sequences[:,:-1]\n",
        "labels = input_sequences[:,-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH_4wehACKF3"
      },
      "source": [
        "Ti·∫øn h√†nh t·∫°o dataset ƒë·ªÉ t·ªëi ∆∞u h√≥a d·ªØ li·ªáu ƒë·∫©y v√†o qu√° tr√¨nh hu·∫•n luy·ªán"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9E1MISaHXoem"
      },
      "outputs": [],
      "source": [
        "# T·∫°o datagen ƒë·ªÉ x·ªß l√Ω nhanh, t·ªëi ∆∞u h√≥a qu√° tr√¨nh hu·∫•n luy·ªán h∆°n\n",
        "# T·∫°o ƒë·ªëi t∆∞·ª£ng Dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((xs, labels))\n",
        "\n",
        "# T·ªëi ∆∞u Dataset v·ªõi shuffle, batch v√† prefetch\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "dataset = (\n",
        "    dataset\n",
        "    .cache(\"train_cache.tfdata\")    # Thay v√¨ l∆∞u d·ªØ li·ªáu v√†o ram th√¨ l∆∞u h·∫≥n v√†o disk\n",
        "    .shuffle(buffer_size=1000)      # Tr·ªôn d·ªØ li·ªáu\n",
        "    .batch(32)                      # Chia batch (batch_size=32)\n",
        "    .prefetch(buffer_size=AUTOTUNE) # T·∫£i tr∆∞·ªõc d·ªØ li·ªáu\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPZykQp2nwWX"
      },
      "source": [
        "M√¨nh s·∫Ω ti·∫øn h√†nh ƒë·ªãnh nghƒ©a ki·∫øn tr√∫c m√¥ h√¨nh theo ƒë√∫ng l·∫ßn th·ª≠ c·ªßa t√°c gi·∫£, m·ªçi ng∆∞·ªùi c√≥ th·ªÉ linh ho·∫°t thay ƒë·ªïi c√°c si√™u tham s·ªë li√™n t·ª•c ƒë·ªÉ c·∫£i thi·ªán nha.  \n",
        "\n",
        "T√°c gi·∫£ sau khi th·ª≠ nhi·ªÅu l·∫ßn th√¨ c√≥ ƒë·ªÅ xu·∫•t:  \n",
        "- **window_size**: 6  \n",
        "- **s·ªë chi·ªÅu embedding**: 16  \n",
        "- **s·ªë unit trong LSTMs**: 32  \n",
        "- **tƒÉng t·ªëc ƒë·ªô h·ªçc-learning rate l√™n**: t√°c gi·∫£ kh√¥ng n√≥i r√µ n√™n m√¨nh t·∫°m l·∫•y **0.005** ƒëi. S·ªë m·∫∑c ƒë·ªãnh l√† **0.001**  \n",
        "- **epochs**: 100 b·ªüi s·ªë l∆∞·ª£ng tham s·ªë t√≠nh to√°n n√†y l√† r·∫•t l·ªõn. üöÄ  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JS-L_59IaBRY"
      },
      "outputs": [],
      "source": [
        "total_words = len(tokenizer.word_index) + 1 # v√¨ t·ª´ ƒëi·ªÉn b·∫Øt ƒë·∫ßu t·ª´ 1, s·ªë 0 ƒë∆∞·ª£c th√™m v√†o nh∆∞ padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SBGyebYQuGT"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  model3 = Sequential([\n",
        "      Embedding(total_words, 16),\n",
        "      Bidirectional(LSTM(32, return_sequences=True)),\n",
        "      Bidirectional(LSTM(32)),\n",
        "      Dense(total_words, activation=\"softmax\")\n",
        "  ])\n",
        "  adam = Adam(learning_rate=0.01)\n",
        "  model3.compile(loss = 'sparse_categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4XR-DCiRE17"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "history3 = model3.fit(dataset, epochs = 100)\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17RgwQEFRHft"
      },
      "outputs": [],
      "source": [
        "print(f\"Th·ªùi gian hu·∫•n luy·ªán m√¥ h√¨nh: {timedelta(seconds=end_time - start_time)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAdcyVxpRI8p"
      },
      "outputs": [],
      "source": [
        "# V·∫Ω bi·ªÉu ƒë·ªì hu·∫•n luy·ªán\n",
        "train_acc = history3.history[\"accuracy\"]\n",
        "train_loss = history3.history[\"loss\"]\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize = (12, 4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc)\n",
        "axs[0].set_title(\"Training Accuracy\")\n",
        "axs[1].plot(train_loss, color=\"orange\")\n",
        "axs[1].set_title(\"Training Loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABtvvjgqRMzE"
      },
      "outputs": [],
      "source": [
        "# ƒê√°nh gi√° m√¥ h√¨nh\n",
        "model3.evaluate(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJQQRopD7Qbg"
      },
      "outputs": [],
      "source": [
        "model3.save(\"model3.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT-9aZxQHBSv"
      },
      "outputs": [],
      "source": [
        "# T·∫£i l·∫°i m√¥ h√¨nh\n",
        "model3 = load_model(\"model3.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eA1ZzADROy_"
      },
      "outputs": [],
      "source": [
        "# Ti·∫øn h√†nh d·ª± ƒëo√°n\n",
        "seed_text = \"sweet jeremy saw dublin\"\n",
        "num_words = 100\n",
        "predicted_text = seed_text\n",
        "\n",
        "# V√¨ k√≠ch th∆∞·ªõc ƒë·∫ßu v√†o ƒë√£ thay ƒë·ªïi theo k√≠ch th∆∞·ªõc c·ª≠a s·ªï\n",
        "# n√™n ta c≈©ng thay ƒë·ªïi ph·∫ßn c·∫Øt d·ªØ li·ªáu ƒë∆∞a v√†o t√≠\n",
        "for i in range(num_words):\n",
        "  # K√≠ch th∆∞·ªõc c·ª≠a s·ªï c·∫Øt ra ban ƒë·∫ßu l√† 6 nh∆∞ng ph·∫£i chia t·ª´ cu·ªëi cho nh√£n n√™n ƒë·∫ßu v√†o l√† 5\n",
        "  token_list = tokenizer.texts_to_sequences([predicted_text])[0]\n",
        "  # Ti·∫øn h√†nh ki·ªÉm tra lu√¥n n·∫øu k√≠ch th∆∞·ªõc kh√¥ng ƒë·ªß th√¨ padding v√†o ho·∫∑c d√†i qu√° th√¨ c√≥ th·ªÉ c·∫Øt ƒëi\n",
        "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding=\"pre\")\n",
        "  token_list = np.array(token_list)\n",
        "  predicted_result = model3.predict(token_list)\n",
        "  predicted_index = np.argmax(predicted_result, axis=-1)\n",
        "  output_word = index_words.get(predicted_index[0], \"\") # N·∫øu index kh√¥ng c√≥ trong t·ª´ ƒëi·ªÉn, tr·∫£ v·ªÅ k√≠ t·ª± r·ªóng nh∆∞ padding\n",
        "\n",
        "  print(f\"Step {i}:\")\n",
        "  print(f\"T·ª´ d·ª± ƒëo√°n: {output_word} v·ªõi x√°c su·∫•t {predicted_result[0][predicted_index[0]]*100:2f}%\")\n",
        "  print(\"-\"*50)\n",
        "  predicted_text += \" \" + output_word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PykF8WrdTJ05"
      },
      "outputs": [],
      "source": [
        "print(f\"VƒÉn b·∫£n sinh ra: \\n{predicted_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkZ0nLZ8diuv"
      },
      "source": [
        "# Character-Based Encoding  \n",
        "### T·ª•i m√¨nh s·∫Ω ti·∫øp c·∫≠n v·ªõi vi·ªác m√£ h√≥a ·ªü c·∫•p k√≠ t·ª± nha.  \n",
        "\n",
        "N·∫øu nh∆∞ ·ªü c√°c ch∆∞∆°ng tr∆∞·ªõc ch√∫ng ta th∆∞·ªùng hay s·ª≠ d·ª•ng m√£ h√≥a ·ªü c·∫•p t·ª´ th√¨ ·ªü ch∆∞∆°ng n√†y ch√∫ng ta c·∫ßn ph·∫£i xem x√©t l·∫°i. V·ªõi m·ªôt l∆∞·ª£ng vƒÉn b·∫£n kh·ªïng l·ªì nh∆∞ n√†y, k√≠ch th∆∞·ªõc b·ªô t·ª´ ƒëi·ªÉn t·ª´ c≈©ng s·∫Ω r·∫•t l√† l·ªõn. Khi ƒë·∫øn v·ªõi t√°c v·ª• t·∫°o sinh vƒÉn b·∫£n, ch·∫≥ng ph·∫£i ƒë·∫ßu c·ªßa m√¥ h√¨nh b·∫±ng v·ªõi k√≠ch th∆∞·ªõc b·ªô t·ª´ ƒëi·ªÉn v√† n√≥ th·∫≠t s·ª± l√† m·ªôt con s·ªë cho√°ng ng·ª£p.  \n",
        "\n",
        "Trong khi ƒë√≥ n·∫øu ta s·ª≠ d·ª•ng m√£ h√≥a ·ªü c·∫•p k√≠ t·ª± th√¨ k√≠ch th∆∞·ªõc b·ªô t·ª´ ƒëi·ªÉn s·∫Ω nh·ªè h∆°n r·∫•t nhi·ªÅu, ƒë·∫ßu ra m√† m√¥ h√¨nh c≈©ng √≠t h∆°n. Th·∫≠m ch√≠ th·∫•p h∆°n c·∫£ trƒÉm l·∫ßn. ‚ú®  \n",
        "\n",
        "**VD**: Ch√∫ng ta c√≥ m·ªôt b·ªô d·ªØ li·ªáu vƒÉn b·∫£n:  \n",
        "- Khi m√£ h√≥a ·ªü c·∫•p t·ª´, b·ªô t·ª´ ƒëi·ªÉn c·ªßa ch√∫ng ta c√≥ k√≠ch th∆∞·ªõc l√† **2700** t∆∞∆°ng ·ª©ng v·ªõi 2700 t·ª´ ri√™ng bi·ªát.  \n",
        "- Tuy nhi√™n khi m√£ h√≥a ·ªü c·∫•p k√≠ t·ª± th√¨ k√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn ch·ªâ c√≤n **65**, nh·ªè h∆°n r·∫•t l√† nhi·ªÅu, m√¥ h√¨nh c≈©ng ch·ªâ c·∫ßn d·ª± ƒëo√°n ƒë·∫ßu ra l√† **65 nh√£n** thay v√¨ **2700** nh∆∞ tr∆∞·ªõc.  \n",
        "\n",
        "Qua c√°ch m√£ h√≥a d·ªØ li·ªáu tr√™n th√¨ m√¥ h√¨nh c·ªßa ch√∫ng ta s·∫Ω ƒë∆°n gi·∫£n h∆°n r·∫•t nhi·ªÅu, ngo√†i ra c≈©ng c√≥ th·ªÉ ch·ª©a c√°c k√≠ t·ª± ƒë·∫∑c bi·ªát. üöÄ  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnwJB1vtLunZ"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"kewagbln/shakespeareonline\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdZ1qbPmMRSH"
      },
      "outputs": [],
      "source": [
        "# Li·ªát k√™ danh s√°ch file\n",
        "for f in os.listdir(path):\n",
        "  print(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REwsNOH6MIac"
      },
      "outputs": [],
      "source": [
        "# Load d·ªØ li·ªáu v√†o\n",
        "data = open(os.path.join(path,\"t8.shakespeare.txt\"), \"r\").read()\n",
        "# Ki·ªÉm tra d·ªØ li·ªáu\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iMBBlmNMfQ1"
      },
      "outputs": [],
      "source": [
        "# Ti·∫øn h√†nh chia c√¢u d·ª±a tr√™n c·ª≠a s·ªï tr∆∞·ª£t\n",
        "window_size = 17\n",
        "corpus = data.lower()\n",
        "\n",
        "# L·∫•y danh s√°ch k√≠ t·ª±\n",
        "characters = list(corpus)\n",
        "# print(characters)\n",
        "\n",
        "# Chia c√¢u (chu·ªói) d·ª±a tr√™n c·ª≠a s·ªï tr∆∞·ª£t\n",
        "range_size = len(characters) - window_size + 1\n",
        "input_sentences = []\n",
        "for i in range(range_size):\n",
        "  input_sentences.append(\"\".join(characters[i:i+window_size]))\n",
        "\n",
        "# Ki·ªÉm tra 5 c√¢u (chu·ªói) ƒë·∫ßu ti√™n:\n",
        "for i in range(5):\n",
        "  print(repr(input_sentences[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BunmodZQNtEY"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Ti·∫øn h√†nh t·∫°o b·ªô m√£ h√≥a c·∫•p k√≠ t·ª± v√† chia chu·ªói con\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(input_sentences)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PB_cfGIUNj7"
      },
      "outputs": [],
      "source": [
        "print(f\"T·ªïng s·ªë l∆∞·ª£ng k√≠ t·ª±: {len(tokenizer.word_index)}\")\n",
        "print(f\"B·ªô t·ª´ ƒëi·ªÉn t·ª´ v·ª±ng: {tokenizer.word_index}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BItmUKP7sRqC"
      },
      "outputs": [],
      "source": [
        "# T·∫°o b·ªô t·ª´ ƒëi·ªÉn chuy·ªÉn ƒë·ªïi\n",
        "index_words = {}\n",
        "for word, index in tokenizer.word_index.items():\n",
        "  index_words[index] = word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWMH-WSeYNXk"
      },
      "source": [
        "V√¨ v·∫•n ƒë·ªÅ ram kh√¥ng ƒë·ªß, l∆∞·ª£ng d·ªØ li·ªáu qu√° l·ªõn n√™n m√¨nh ti·∫øn h√†nh l∆∞u t·ª´ng d·ªØ li·ªáu m√£ h√≥a v√†o m·ªôt file pickle nha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKcsonfbUPcw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# T·∫°o file t·∫°m ƒë·ªÉ l∆∞u d·ªØ li·ªáu\n",
        "temp_file = \"sub_sequences_temp.pkl\"\n",
        "max_sequence_len = 0\n",
        "\n",
        "with open(temp_file, \"wb\") as f:\n",
        "  for sentence in input_sentences:\n",
        "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
        "    if len(token_list) > max_sequence_len:\n",
        "      max_sequence_len = len(token_list)\n",
        "    for i in range(1, len(token_list)):\n",
        "      n_gram_sequence = token_list[:i+1]\n",
        "      pickle.dump(n_gram_sequence, f)\n",
        "\n",
        "print(f\"ƒê·ªô d√†i chu·ªói d√†i nh·∫•t: {max_sequence_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVhsqWwaeTGI"
      },
      "source": [
        " Sau khi ti·∫øn h√†nh l∆∞u m·ªôt file pickle xong, ta s·∫Ω ti·∫øn h√†nh x·ª≠ l√Ω theo t·ª´ng batch, tuy l√† vi·ªác n√†y s·∫Ω t·ªën th·ªùi gian h∆°n r·∫•t nhi·ªÅu nh∆∞ng ƒë√¢y l√† c√°ch duy nh·∫•t v√¨ l∆∞·ª£ng d·ªØ li·ªáu qu√° l·ªõn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxHjXDFme2Gw"
      },
      "outputs": [],
      "source": [
        "temp_file = \"sub_sequences_temp.pkl\"\n",
        "def read_sequences_in_batches(file_path, batch_size):\n",
        "    \"\"\"\n",
        "    H√†m generator ƒë·ªçc d·ªØ li·ªáu t·ª´ file Pickle theo t·ª´ng batch.\n",
        "    \"\"\"\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        batch = []\n",
        "        while True:\n",
        "            try:\n",
        "                # ƒê·ªçc t·ª´ng chu·ªói\n",
        "                n_gram_sequence = pickle.load(f)\n",
        "                batch.append(n_gram_sequence)\n",
        "                # N·∫øu ƒë·ªß batch size, yield batch v√† kh·ªüi t·∫°o l·∫°i\n",
        "                if len(batch) == batch_size:\n",
        "                    yield batch\n",
        "                    batch = []\n",
        "            except EOFError:\n",
        "                # Tr·∫£ v·ªÅ batch cu·ªëi c√πng n·∫øu c√≤n d·ªØ li·ªáu\n",
        "                if batch:\n",
        "                    yield batch\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkcQWUNPe9MR"
      },
      "outputs": [],
      "source": [
        "def process_batch(batch, max_seq_length):\n",
        "    \"\"\"\n",
        "    Pad c√°c chu·ªói v√† t√°ch X (input), labels (output).\n",
        "    \"\"\"\n",
        "    # Pad c√°c chu·ªói ƒë·∫øn chi·ªÅu d√†i t·ªëi ƒëa\n",
        "    padded_batch = pad_sequences(batch, maxlen=max_seq_length, padding='pre')\n",
        "\n",
        "    # T√°ch X v√† labels\n",
        "    X = padded_batch[:, :-1]  # T·∫•t c·∫£ tr·ª´ ph·∫ßn t·ª≠ cu·ªëi\n",
        "    labels = padded_batch[:, -1]  # Ph·∫ßn t·ª≠ cu·ªëi c√πng l√†m nh√£n\n",
        "    return X, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NzkFZtefsmT"
      },
      "outputs": [],
      "source": [
        "def data_generator(file_path, batch_size, max_seq_length):\n",
        "    for batch in read_sequences_in_batches(file_path, batch_size):\n",
        "        X, labels = process_batch(batch, max_seq_length)\n",
        "        yield X, labels\n",
        "\n",
        "batch_size = 16\n",
        "dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(temp_file, batch_size, max_sequence_len),\n",
        "    output_signature=(\n",
        "        tf.TensorSpec(shape=(None, max_sequence_len - 1), dtype=tf.int32),  # X\n",
        "        tf.TensorSpec(shape=(None,), dtype=tf.int32)  # labels\n",
        "    )\n",
        ")\n",
        "\n",
        "# Prefetch ƒë·ªÉ tƒÉng hi·ªáu su·∫•t\n",
        "dataset = dataset.cache(\"train_cache.tfdata\").prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Z6XxkOd7NCM"
      },
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra s·ªë l∆∞·ª£ng k√≠ t·ª± t·ªëi ƒëa\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(f\"S·ªë l∆∞·ª£ng k√≠ t·ª±: {total_words}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgB-oFNmiFwK"
      },
      "source": [
        "U·∫ßy v·∫≠y l√† s·ªë l∆∞·ª£ng nh√£n ƒë·∫ßu ra m√† m√¥ h√¨nh ph·∫£i d·ª± ƒëo√°n ƒë√£ th·∫•p h∆°n r·∫•t nhi·ªÅu r·ªìi, ch·ªâ 66 k√≠ t·ª±. B√¢y gi·ªù t·ª•i m√¨nh ti·∫øn h√†nh ƒë·ªãnh nghƒ©a v√† hu·∫•n luy·ªán m√¥ h√¨nh th·ª≠ nha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQfaeQaz72L-"
      },
      "outputs": [],
      "source": [
        "# ƒê·ªãnh nghƒ©a m√¥ h√¨nh\n",
        "with strategy.scope():\n",
        "  model4 = Sequential([\n",
        "      Embedding(total_words, 16),\n",
        "      Bidirectional(LSTM(32, return_sequences=True)),\n",
        "      Bidirectional(LSTM(32)),\n",
        "      Dense(total_words, activation=\"softmax\")\n",
        "  ])\n",
        "  adam = Adam(learning_rate=0.01)\n",
        "  model4.compile(loss = 'sparse_categorical_crossentropy', optimizer = adam, metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0egK2V0mA5ts"
      },
      "outputs": [],
      "source": [
        "model4.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I7MPVbZS8AMA"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "history4 = model4.fit(dataset, epochs = 100)\n",
        "end_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al72VLFa8SME"
      },
      "outputs": [],
      "source": [
        "print(f\"Th·ªùi gian hu·∫•n luy·ªán m√¥ h√¨nh: {timedelta(seconds=end_time - start_time)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5xK6oNW8UUS"
      },
      "outputs": [],
      "source": [
        "# V·∫Ω bi·ªÉu ƒë·ªì qu√° tr√¨nh hu·∫•n luy·ªán\n",
        "train_acc = history4.history[\"accuracy\"]\n",
        "train_loss = history4.history[\"loss\"]\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize = (12, 4))\n",
        "axs = axs.flatten()\n",
        "axs[0].plot(train_acc)\n",
        "axs[0].set_title(\"Training Accuracy\")\n",
        "axs[1].plot(train_loss, color=\"orange\")\n",
        "axs[1].set_title(\"Training Loss\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSSahWnHA4yt"
      },
      "outputs": [],
      "source": [
        "model4.eval(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkqHVuCl7Wgt"
      },
      "outputs": [],
      "source": [
        "model4.save(\"model4.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y6lTIs2AQ2U"
      },
      "outputs": [],
      "source": [
        "# C·∫≠p nh·∫≠t tr·ªçng s·ªë t·ª´ m√¥ h√¨nh ƒë√£ l∆∞u\n",
        "model4 = tf.keras.models.load_model('model4.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcJsdx5WE1JU"
      },
      "outputs": [],
      "source": [
        "# Ti·∫øn h√†nh d·ª± ƒëo√°n\n",
        "seed_text = \"sweet jeremy saw dublin\"\n",
        "num_words = 500\n",
        "predicted_text = seed_text\n",
        "\n",
        "# V√¨ k√≠ch th∆∞·ªõc ƒë·∫ßu v√†o ƒë√£ thay ƒë·ªïi theo k√≠ch th∆∞·ªõc c·ª≠a s·ªï\n",
        "# n√™n ta c≈©ng thay ƒë·ªïi ph·∫ßn c·∫Øt d·ªØ li·ªáu ƒë∆∞a v√†o t√≠\n",
        "for i in range(num_words):\n",
        "  # K√≠ch th∆∞·ªõc c·ª≠a s·ªï c·∫Øt ra ban ƒë·∫ßu l√† 6 nh∆∞ng ph·∫£i chia t·ª´ cu·ªëi cho nh√£n n√™n ƒë·∫ßu v√†o l√† 5\n",
        "  token_list = tokenizer.texts_to_sequences([predicted_text])[0]\n",
        "  # Ti·∫øn h√†nh ki·ªÉm tra lu√¥n n·∫øu k√≠ch th∆∞·ªõc kh√¥ng ƒë·ªß th√¨ padding v√†o ho·∫∑c d√†i qu√° th√¨ c√≥ th·ªÉ c·∫Øt ƒëi\n",
        "  token_list = pad_sequences([token_list], maxlen=window_size-1, padding=\"pre\")\n",
        "  token_list = np.array(token_list)\n",
        "  predicted_result = model4.predict(token_list)\n",
        "  predicted_index = np.argmax(predicted_result, axis=-1)\n",
        "  output_char = index_words[predicted_index[0]]\n",
        "\n",
        "  print(f\"Step {i}:\")\n",
        "  print(f\"T·ª´ d·ª± ƒëo√°n: {output_char} v·ªõi x√°c su·∫•t {predicted_result[0][predicted_index[0]]*100:2f}%\")\n",
        "  print(\"-\"*50)\n",
        "  predicted_text += \" \" + output_char\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7UaPsBUc5vo"
      },
      "outputs": [],
      "source": [
        "print(\"VƒÉn b·∫£n sinh ra:\")\n",
        "print(predicted_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 115588,
          "sourceId": 276428,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6378565,
          "sourceId": 10304562,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 2489101,
          "sourceId": 4222990,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30822,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}